{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b76a8c0d",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a234a155",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df341691",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U dspy datasets tabulate duckdb pandas numpy ipywidgets \"sqlglot[rs]\" wandb --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4329c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from datasets import load_dataset\n",
    "import tabulate\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288f15f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env.local\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "if not wandb_api_key:\n",
    "    raise ValueError(\"WANDB_API_KEY not found in environment variables\")\n",
    "\n",
    "lm = dspy.LM(\"openai/gpt-4o-mini\", api_key=openai_api_key, temperature=1, max_tokens=16000)\n",
    "reflection_lm = dspy.LM(\"openai/gpt-5-mini\", api_key=openai_api_key, temperature=1, max_tokens=16000)\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e655b0",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef6f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"gretelai/synthetic_text_to_sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13420ef6",
   "metadata": {},
   "source": [
    "# Set up DSPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cab5f8",
   "metadata": {},
   "source": [
    "## Set up Signature and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4380853",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProblemDef(dspy.Signature):\n",
    "    \"\"\"You are a database expert. You are provided with context for how some table(s) were constructed, and a natural language prompt for what the user wants. Your job is to write a SQL query to provide them with the required data.\"\"\"\n",
    "    \n",
    "    sql_context: str = dspy.InputField(description=\"SQL queries for creating the table(s) and loading some data\")\n",
    "    sql_prompt: str = dspy.InputField(description=\"User's natural language prompt\")\n",
    "    sql: str = dspy.OutputField(description=\"SQL query that delivers on the user's request. Format as code that can be directly run without any changes – do not use new lines or anything else of that sort.\")\n",
    "\n",
    "program = dspy.ChainOfThought(ProblemDef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e6cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install duckdb pandas numpy sqlglot --quiet\n",
    "import duckdb, pandas as pd, numpy as np, re\n",
    "import sqlglot\n",
    "from sqlglot import parse_one\n",
    "\n",
    "_ORDER_BY = re.compile(r\"\\border\\s+by\\b\", re.IGNORECASE)\n",
    "\n",
    "def _split_sql_statements(script: str):\n",
    "    out, buf, q = [], [], None\n",
    "    i, n = 0, len(script)\n",
    "    while i < n:\n",
    "        ch = script[i]\n",
    "        if q:\n",
    "            buf.append(ch)\n",
    "            if ch == q:\n",
    "                if i + 1 < n and script[i+1] == q:\n",
    "                    buf.append(script[i+1]); i += 1\n",
    "                else:\n",
    "                    q = None\n",
    "        else:\n",
    "            if ch in (\"'\", '\"', \"`\"):\n",
    "                q = ch; buf.append(ch)\n",
    "            elif ch == ';':\n",
    "                s = \"\".join(buf).strip()\n",
    "                if s: out.append(s)\n",
    "                buf = []\n",
    "            else:\n",
    "                buf.append(ch)\n",
    "        i += 1\n",
    "    tail = \"\".join(buf).strip()\n",
    "    if tail: out.append(tail)\n",
    "    return out\n",
    "\n",
    "import re\n",
    "from sqlglot import parse_one\n",
    "\n",
    "_SQLITE_DATE_RE = re.compile(\n",
    "    r\"\"\"\\bdate\\s*\\(\\s*'now'\\s*(?:,\\s*'([+-])\\s*(\\d+)\\s*(year|month|day)s?'\\s*)?\\)\"\"\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "_SQLITE_DATETIME_RE = re.compile(\n",
    "    r\"\"\"\\bdatetime\\s*\\(\\s*'now'\\s*(?:,\\s*'([+-])\\s*(\\d+)\\s*(year|month|day|hour|minute|second)s?'\\s*)?\\)\"\"\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "def _normalize_sqlite_dates(sql: str) -> str:\n",
    "    # date('now') or date('now','-1 year') -> CURRENT_DATE +/- INTERVAL 'N unit'\n",
    "    def _date_subst(m):\n",
    "        sign, num, unit = m.group(1), m.group(2), m.group(3)\n",
    "        if not sign:  # just date('now')\n",
    "            return \"CURRENT_DATE\"\n",
    "        op = \"-\" if sign == \"-\" else \"+\"\n",
    "        return f\"CURRENT_DATE {op} INTERVAL '{num} {unit.lower()}'\"\n",
    "    sql = _SQLITE_DATE_RE.sub(_date_subst, sql)\n",
    "\n",
    "    # datetime('now') / datetime('now','+/-N unit') -> CURRENT_TIMESTAMP +/- INTERVAL 'N unit'\n",
    "    def _dt_subst(m):\n",
    "        sign, num, unit = m.group(1), m.group(2), m.group(3)\n",
    "        if not sign:\n",
    "            return \"CURRENT_TIMESTAMP\"\n",
    "        op = \"-\" if sign == \"-\" else \"+\"\n",
    "        return f\"CURRENT_TIMESTAMP {op} INTERVAL '{num} {unit.lower()}'\"\n",
    "    sql = _SQLITE_DATETIME_RE.sub(_dt_subst, sql)\n",
    "\n",
    "    return sql\n",
    "\n",
    "def _mysql_to_duckdb(stmt: str) -> str:\n",
    "    s = _normalize_sqlite_dates(stmt)  # <-- NEW: normalize SQLite first\n",
    "    try:\n",
    "        return parse_one(s, read=\"mysql\").sql(dialect=\"duckdb\")\n",
    "    except Exception:\n",
    "        # minimal fallbacks for MySQLisms if parse fails\n",
    "        s = re.sub(r\"`([^`]+)`\", r'\"\\1\"', s)\n",
    "        s = re.sub(\n",
    "            r\"DATE_SUB\\s*\\(\\s*(CURRENT_DATE|NOW\\(\\))\\s*,\\s*INTERVAL\\s+(\\d+)\\s+(YEAR|MONTH|DAY)\\s*\\)\",\n",
    "            lambda m: f\"{'CURRENT_DATE' if m.group(1).startswith('CURRENT') else 'CURRENT_DATE'} - INTERVAL '{m.group(2)} {m.group(3).lower()}'\",\n",
    "            s, flags=re.IGNORECASE,\n",
    "        )\n",
    "        s = re.sub(\n",
    "            r\"DATE_ADD\\s*\\(\\s*(CURRENT_DATE|NOW\\(\\))\\s*,\\s*INTERVAL\\s+(\\d+)\\s+(YEAR|MONTH|DAY)\\s*\\)\",\n",
    "            lambda m: f\"{'CURRENT_DATE' if m.group(1).startswith('CURRENT') else 'CURRENT_DATE'} + INTERVAL '{m.group(2)} {m.group(3).lower()}'\",\n",
    "            s, flags=re.IGNORECASE,\n",
    "        )\n",
    "        s = re.sub(r\"\\bIFNULL\\s*\\(\", \"COALESCE(\", s, flags=re.IGNORECASE)\n",
    "        s = re.sub(r\"\\bLOCATE\\s*\\(\\s*([^,]+)\\s*,\\s*([^)]+)\\)\", r\"STRPOS(\\2, \\1)\", s, flags=re.IGNORECASE)\n",
    "        return s\n",
    "\n",
    "def _normalize_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == \"O\":\n",
    "            try:\n",
    "                df[c] = pd.to_numeric(df[c])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return df.replace({np.nan: None})\n",
    "\n",
    "def _exec_script_capture_last_select(con, script: str):\n",
    "    last_df, last_sel_sql = None, None\n",
    "    for raw in _split_sql_statements(script):\n",
    "        stmt = _mysql_to_duckdb(raw)\n",
    "        # detect SELECT after minimal comment strip\n",
    "        s = re.sub(r\"^\\s*(--[^\\n]*\\n|/\\*.*?\\*/\\s*)*\", \"\", stmt, flags=re.DOTALL)\n",
    "        if re.match(r\"(?is)^\\s*(with\\b.*?select|select)\\b\", s):\n",
    "            last_df = con.execute(stmt).fetchdf()\n",
    "            last_sel_sql = stmt\n",
    "        else:\n",
    "            con.execute(stmt)\n",
    "    if last_df is not None:\n",
    "        last_df = _normalize_df(last_df)\n",
    "    return last_df, last_sel_sql\n",
    "\n",
    "def evaluate_sql(sql_context: str, golden_sql: str, predicted_sql: str):\n",
    "    con = duckdb.connect(\":memory:\")\n",
    "\n",
    "    # context\n",
    "    try:\n",
    "        for raw in _split_sql_statements(sql_context):\n",
    "            con.execute(_mysql_to_duckdb(raw))\n",
    "    except Exception as e:\n",
    "        return 0, {\"reason\": \"context_error\", \"detail\": str(e)}\n",
    "\n",
    "    # golden\n",
    "    try:\n",
    "        gold_df, gold_last_select = _exec_script_capture_last_select(con, golden_sql)\n",
    "    except Exception as e:\n",
    "        return 0, {\"reason\": \"gold_error\", \"detail\": str(e)}\n",
    "    if gold_df is None:\n",
    "        return 0, {\"reason\": \"gold_no_select\", \"detail\": \"No SELECT in golden_sql.\"}\n",
    "\n",
    "    # predicted\n",
    "    try:\n",
    "        pred_df, pred_last_select = _exec_script_capture_last_select(con, predicted_sql)\n",
    "    except Exception as e:\n",
    "        return 0, {\"reason\": \"pred_error\", \"detail\": str(e)}\n",
    "    if pred_df is None:\n",
    "        return 0, {\"reason\": \"pred_no_select\", \"detail\": \"No SELECT in predicted_sql.\"}\n",
    "\n",
    "    # column alignment (allow pred supersets; else try set/positional)\n",
    "    gold_cols, pred_cols = list(gold_df.columns), list(pred_df.columns)\n",
    "    if gold_cols == pred_cols:\n",
    "        pass\n",
    "    elif set(gold_cols).issubset(pred_cols):\n",
    "        pred_df = pred_df[gold_cols]\n",
    "    elif set(gold_cols) == set(pred_cols):\n",
    "        pred_df = pred_df[gold_cols]\n",
    "    elif gold_df.shape[1] == pred_df.shape[1]:\n",
    "        new_names = [f\"c{i}\" for i in range(gold_df.shape[1])]\n",
    "        gold_df = gold_df.copy(); pred_df = pred_df.copy()\n",
    "        gold_df.columns = new_names; pred_df.columns = new_names\n",
    "    else:\n",
    "        return 0, {\"reason\": \"column_mismatch\",\n",
    "                   \"detail\": f\"Different number of columns: expected {gold_df.shape[1]}, got {pred_df.shape[1]}\"}\n",
    "\n",
    "    # ordering rule from gold's last SELECT\n",
    "    gold_has_order = bool(_ORDER_BY.search(gold_last_select or \"\"))\n",
    "    if not gold_has_order:\n",
    "        try:\n",
    "            g = gold_df.sort_values(by=list(gold_df.columns), kind=\"mergesort\").reset_index(drop=True)\n",
    "            p = pred_df.sort_values(by=list(gold_df.columns), kind=\"mergesort\").reset_index(drop=True)\n",
    "        except Exception:\n",
    "            g = gold_df.reset_index(drop=True); p = pred_df.reset_index(drop=True)\n",
    "    else:\n",
    "        g = gold_df.reset_index(drop=True); p = pred_df.reset_index(drop=True)\n",
    "\n",
    "    # value compare\n",
    "    if g.shape != p.shape:\n",
    "        return 0, {\"reason\": \"shape_mismatch\", \"detail\": f\"gold {g.shape} vs pred {p.shape}\"}\n",
    "\n",
    "    for c in g.columns:\n",
    "        if pd.api.types.is_numeric_dtype(g[c]) and pd.api.types.is_numeric_dtype(p[c]):\n",
    "            if not np.allclose(g[c].values, p[c].values, rtol=1e-6, atol=1e-8, equal_nan=True):\n",
    "                return 0, {\"reason\": \"value_mismatch\", \"detail\": f\"Numeric mismatch in '{c}'\",\n",
    "                           \"gold_head\": g.head(10).to_dict(\"records\"),\n",
    "                           \"pred_head\": p.head(10).to_dict(\"records\")}\n",
    "        else:\n",
    "            eq = [(x == y) or (x is None and y is None) for x, y in zip(g[c].values, p[c].values)]\n",
    "            if not all(eq):\n",
    "                return 0, {\"reason\": \"value_mismatch\", \"detail\": f\"Mismatch in '{c}'\",\n",
    "                           \"gold_head\": g.head(10).to_dict(\"records\"),\n",
    "                           \"pred_head\": p.head(10).to_dict(\"records\")}\n",
    "    return 1, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb24b156",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936c6363",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_index = 4\n",
    "context = ds['train'][demo_index]['sql_context']\n",
    "prompt = ds['train'][demo_index]['sql_prompt']\n",
    "golden_sql = ds['train'][demo_index]['sql']\n",
    "\n",
    "print(f\"Context: {context}\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Golden sql: {golden_sql}\")\n",
    "result = program(sql_context=context, sql_prompt=prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b7fd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "score, info = evaluate_sql(context, golden_sql, result.sql)\n",
    "print(score, info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58142861",
   "metadata": {},
   "source": [
    "## Environment didn't work, let's use LLM as Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c72a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Judge(dspy.Signature):\n",
    "    \"\"\"You are required to judge two SQL queries for functional similarity. You will be given a context of how the table(s) and data were created, and the natural language prompt from the user\"\"\"\n",
    "\n",
    "    sql_context: str = dspy.InputField(description=\"SQL statement(s) creating the table(s) and the input data\")\n",
    "    sql_prompt: str = dspy.InputField(description=\"Natural language prompt from the user\")\n",
    "    golden_sql: str = dspy.InputField(description=\"The golden SQL query from our dataset\")\n",
    "    candidate_sql: str = dspy.InputField(description=\"A SQL query generated by a model for the same prompt\")\n",
    "    similar: bool = dspy.OutputField(description=\"True if the candidate SQL query is functionally similar to the golden SQL query\")\n",
    "\n",
    "judge = dspy.ChainOfThought(Judge)\n",
    "judge.lm = reflection_lm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1677bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_response = judge(sql_context=context, sql_prompt=prompt, golden_sql=golden_sql, candidate_sql=result.sql)\n",
    "print(f\"Context: {context}\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Golden SQL: {golden_sql}\")\n",
    "print(f\"Candidate SQL: {result.sql}\")\n",
    "print(f\"Judge Response: {judge_response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7746087",
   "metadata": {},
   "source": [
    "# Get ready to GEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b7379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets dspy-ai\n",
    "import math, random\n",
    "from typing import Callable, List, Tuple, Optional\n",
    "from datasets import Dataset, DatasetDict\n",
    "from dspy import GEPA\n",
    "\n",
    "def split_for_gepa(\n",
    "    ds: Dataset,\n",
    "    to_example: Callable[[dict], \"dspy.Example\"],\n",
    "    val_size: float = 0.15,\n",
    "    seed: int = 42,\n",
    "    group_col: Optional[str] = None,\n",
    "    stratify_col: Optional[str] = None,\n",
    ") -> Tuple[List[\"dspy.Example\"], List[\"dspy.Example\"]]:\n",
    "    \"\"\"\n",
    "    Return (train_set, val_set) as lists of dspy.Example.\n",
    "    - If group_col is set: group-aware split (no group leakage).\n",
    "    - Else if stratify_col is set: use HF stratified split.\n",
    "    - Else: random split.\n",
    "    \"\"\"\n",
    "    assert 0.0 < val_size < 1.0, \"val_size must be in (0,1)\"\n",
    "    rng = random.Random(seed)\n",
    "\n",
    "    # --- Group-aware split (preferred for text2sql) ---\n",
    "    if group_col:\n",
    "        groups = ds[group_col]\n",
    "        # Build group -> indices\n",
    "        g2idx = {}\n",
    "        for i, g in enumerate(groups):\n",
    "            g2idx.setdefault(g, []).append(i)\n",
    "        uniq_groups = list(g2idx.keys())\n",
    "        rng.shuffle(uniq_groups)\n",
    "        n_val_groups = max(1, math.floor(val_size * len(uniq_groups)))\n",
    "        val_groups = set(uniq_groups[:n_val_groups])\n",
    "\n",
    "        val_idx = [i for g in val_groups for i in g2idx[g]]\n",
    "        train_idx = [i for g in uniq_groups[n_val_groups:] for i in g2idx[g]]\n",
    "\n",
    "        # Edge case: if a group is gigantic, ensure both splits non-empty\n",
    "        if not train_idx or not val_idx:\n",
    "            # fallback: plain random split\n",
    "            perm = list(range(len(ds)))\n",
    "            rng.shuffle(perm)\n",
    "            cut = max(1, math.floor(val_size * len(ds)))\n",
    "            val_idx, train_idx = perm[:cut], perm[cut:]\n",
    "\n",
    "        ds_train = ds.select(train_idx)\n",
    "        ds_val = ds.select(val_idx)\n",
    "\n",
    "    # --- Stratified split (when you have a label/cluster column) ---\n",
    "    elif stratify_col:\n",
    "        # HF does stratify on categorical-like columns\n",
    "        parts: DatasetDict = ds.train_test_split(\n",
    "            test_size=val_size,\n",
    "            seed=seed,\n",
    "            stratify_by_column=stratify_col,\n",
    "        )\n",
    "        ds_train, ds_val = parts[\"train\"], parts[\"test\"]\n",
    "\n",
    "    # --- Simple random split ---\n",
    "    else:\n",
    "        parts: DatasetDict = ds.train_test_split(test_size=val_size, seed=seed)\n",
    "        ds_train, ds_val = parts[\"train\"], parts[\"test\"]\n",
    "\n",
    "    # Map to dspy.Example lists\n",
    "    train_set = [to_example(r) for r in ds_train]\n",
    "    val_set = [to_example(r) for r in ds_val]\n",
    "    return train_set, val_set\n",
    "\n",
    "def to_dspy_example(row):\n",
    "    # mark inputs; leave gold 'sql' as label\n",
    "    return dspy.Example(\n",
    "        sql_prompt=row[\"sql_prompt\"],\n",
    "        sql_context=row[\"sql_context\"],\n",
    "        sql=row[\"sql\"],          # gold label\n",
    "    ).with_inputs(\"sql_prompt\", \"sql_context\")\n",
    "\n",
    "\n",
    "# call function that splits ds['train'] into train_set and val_set as needed\n",
    "# ds is your loaded HF dataset dict; we split ds[\"train\"]\n",
    "train_set, val_set = split_for_gepa(\n",
    "    ds[\"train\"],\n",
    "    to_dspy_example,          # your to_dspy_example(row)\n",
    "    val_size=0.5,\n",
    "    seed=42,\n",
    "    group_col=None,      # e.g., \"db_id\" if available\n",
    "    stratify_col=None,   # or a column like \"op_class\" if you want stratification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d14596",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_variants_to_try = 20 # number of variants to test\n",
    "mini_batch_size = 20 # mini-batch size\n",
    "val_set_size = 200 # val-set size\n",
    "train_set_size = 200 # train-set size\n",
    "\n",
    "def budget_for_variants(N, V, k, slack=2):\n",
    "    # slack handles occasional extra probes/promotions\n",
    "    return V + N * (k + slack)\n",
    "\n",
    "def metric_with_feedback(example, pred, trace=None, pred_name=None, pred_trace=None):\n",
    "    judge_response = judge(sql_context=example.sql_context, sql_prompt=example.sql_prompt, golden_sql=example.sql, candidate_sql=pred.sql)\n",
    "    score = 0\n",
    "    if (judge_response.similar):\n",
    "        score = 1\n",
    "    return dspy.Prediction(score=score, feedback=judge_response.reasoning)\n",
    "\n",
    "val_for_tracking = val_set[:val_set_size]   # 128–512 is a good range\n",
    "train_set_for_optimization = train_set[:train_set_size]\n",
    "optimizer = GEPA(\n",
    "    metric=metric_with_feedback,\n",
    "    num_threads=32,\n",
    "    track_stats=True,\n",
    "    reflection_minibatch_size=mini_batch_size,\n",
    "    reflection_lm=reflection_lm,\n",
    "    use_wandb=True,\n",
    "    wandb_api_key=wandb_api_key,\n",
    "    log_dir=\"logs\",\n",
    "    auto=\"light\"   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77daa2b",
   "metadata": {},
   "source": [
    "# Run GEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940c1f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_program = optimizer.compile(\n",
    "    program,\n",
    "    trainset=train_set_for_optimization,\n",
    "    valset=val_for_tracking,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d21aeb",
   "metadata": {},
   "source": [
    "# Review original and optimized prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f8d446",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(program.predict.signature.instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6cf895",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimized_program.predict.signature.instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f2a893",
   "metadata": {},
   "source": [
    "# Store Programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e93a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "program.save(\"./dspy_program/program.json\", save_program=False)\n",
    "optimized_program.save(\"./optimized_program/program.json\", save_program=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e85afb4",
   "metadata": {},
   "source": [
    "# Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64245ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datasets import Dataset\n",
    "from time import perf_counter\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "def evaluate_program(\n",
    "    program,\n",
    "    ds_test: Dataset,\n",
    "    limit: int = 100,\n",
    "    max_workers: int = 8,\n",
    "    field_map: Optional[Dict[str, str]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate a DSPy program on the first `limit` rows of a HF Dataset split.\n",
    "\n",
    "    Args:\n",
    "        program: a DSPy Module with signature program(sql_prompt=..., sql_context=...)\n",
    "        ds_test: Hugging Face Dataset (e.g., ds[\"test\"])\n",
    "        limit: number of rows to evaluate (default 100)\n",
    "        max_workers: parallel threads for I/O-bound LM + judge\n",
    "        field_map: optional mapping if your column names differ:\n",
    "                   {\"sql_prompt\": \"...\", \"sql_context\": \"...\", \"sql\": \"...\"}\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"accuracy\": float,\n",
    "          \"correct\": int,\n",
    "          \"total\": int,\n",
    "          \"avg_latency_s\": float,\n",
    "          \"failures\": [ {idx, reason, pred_sql, feedback} ... up to 20 ],\n",
    "        }\n",
    "    \"\"\"\n",
    "    if field_map is None:\n",
    "        field_map = {\"sql_prompt\": \"sql_prompt\", \"sql_context\": \"sql_context\", \"sql\": \"sql\"}\n",
    "\n",
    "    ds_test = ds_test.shuffle()\n",
    "    n = min(limit, len(ds_test))\n",
    "    subset = ds_test.select(range(n))\n",
    "    start = perf_counter()\n",
    "\n",
    "    def _eval_one(i_row):\n",
    "        i, row = i_row\n",
    "        try:\n",
    "            pred = program(\n",
    "                sql_prompt=row[field_map[\"sql_prompt\"]],\n",
    "                sql_context=row[field_map[\"sql_context\"]],\n",
    "            )\n",
    "            pred_sql = getattr(pred, \"sql\", None) or (pred.get(\"sql\") if isinstance(pred, dict) else None) or \"\"\n",
    "            jr = judge(\n",
    "                sql_context=row[field_map[\"sql_context\"]],\n",
    "                sql_prompt=row[field_map[\"sql_prompt\"]],\n",
    "                golden_sql=row[field_map[\"sql\"]],\n",
    "                candidate_sql=pred_sql,\n",
    "            )\n",
    "            ok = bool(getattr(jr, \"similar\", False))\n",
    "            feedback = getattr(jr, \"reasoning\", \"\") or \"\"\n",
    "            return (i, ok, pred_sql, feedback, None)\n",
    "        except Exception as e:\n",
    "            return (i, False, \"\", \"\", f\"{type(e).__name__}: {e}\")\n",
    "\n",
    "    results = []\n",
    "    # Threaded evaluation (I/O bound: LM + judge). Tune max_workers to your provider limits.\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futures = [ex.submit(_eval_one, (i, subset[i])) for i in range(n)]\n",
    "        for f in as_completed(futures):\n",
    "            results.append(f.result())\n",
    "\n",
    "    # Sort back to input order\n",
    "    results.sort(key=lambda x: x[0])\n",
    "\n",
    "    correct = sum(1 for _, ok, *_ in results if ok)\n",
    "    total = n\n",
    "    acc = correct / total if total else 0.0\n",
    "    elapsed = perf_counter() - start\n",
    "    avg_lat = elapsed / total if total else 0.0\n",
    "\n",
    "    failures = []\n",
    "    for i, ok, pred_sql, feedback, err in results:\n",
    "        if not ok and len(failures) < 20:\n",
    "            failures.append({\n",
    "                \"idx\": i,\n",
    "                \"reason\": (\"error: \" + err) if err else \"mismatch\",\n",
    "                \"pred_sql\": pred_sql,\n",
    "                \"feedback\": feedback,\n",
    "            })\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"correct\": correct,\n",
    "        \"total\": total,\n",
    "        \"avg_latency_s\": avg_lat,\n",
    "        \"failures\": failures,\n",
    "    }\n",
    "    \n",
    "test_split = ds[\"test\"]\n",
    "test_split = test_split.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca6e0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_program = dspy.ChainOfThought(ProblemDef)\n",
    "og_program.load(\"./dspy_program/program.json\")\n",
    "og_metrics = evaluate_program(og_program, test_split, limit=500, max_workers=32)\n",
    "print(f\"Original Program: {og_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fdea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_program = dspy.ChainOfThought(ProblemDef)\n",
    "opt_program.load(\"./optimized_program/program.json\")\n",
    "opt_metrics = evaluate_program(opt_program, test_split, limit=500, max_workers=32)\n",
    "print(f\"Original Program: {opt_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9937eb78",
   "metadata": {},
   "source": [
    "# Store Eval Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcc020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_metrics(metrics, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4, sort_keys=True) \n",
    "\n",
    "save_metrics(og_metrics, \"./dspy_program/4o-mini.json\")\n",
    "save_metrics(opt_metrics,\"./optimized_program/4o-mini.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
