{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b76a8c0d",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a234a155",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df341691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U dspy datasets tabulate duckdb pandas numpy ipywidgets \"sqlglot[rs]\" wandb --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4329c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from datasets import load_dataset\n",
    "import tabulate\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "288f15f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env.local\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "if not wandb_api_key:\n",
    "    raise ValueError(\"WANDB_API_KEY not found in environment variables\")\n",
    "\n",
    "lm = dspy.LM(\"openai/gpt-5-mini\", api_key=openai_api_key, temperature=1, max_tokens=16000)\n",
    "reflection_lm = dspy.LM(\"openai/gpt-5-mini\", api_key=openai_api_key, temperature=1, max_tokens=16000)\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e655b0",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aef6f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"gretelai/synthetic_text_to_sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13420ef6",
   "metadata": {},
   "source": [
    "# Set up DSPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cab5f8",
   "metadata": {},
   "source": [
    "## Set up Signature and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4380853",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProblemDef(dspy.Signature):\n",
    "    \"\"\"You are a database expert. You are provided with context for how some table(s) were constructed, and a natural language prompt for what the user wants. Your job is to write a SQL query to provide them with the required data.\"\"\"\n",
    "    \n",
    "    sql_context: str = dspy.InputField(description=\"SQL queries for creating the table(s) and loading some data\")\n",
    "    sql_prompt: str = dspy.InputField(description=\"User's natural language prompt\")\n",
    "    sql: str = dspy.OutputField(description=\"SQL query that delivers on the user's request. Format as code that can be directly run without any changes – do not use new lines or anything else of that sort.\")\n",
    "\n",
    "program = dspy.ChainOfThought(ProblemDef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c92e6cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install duckdb pandas numpy sqlglot --quiet\n",
    "import duckdb, pandas as pd, numpy as np, re\n",
    "import sqlglot\n",
    "from sqlglot import parse_one\n",
    "\n",
    "_ORDER_BY = re.compile(r\"\\border\\s+by\\b\", re.IGNORECASE)\n",
    "\n",
    "def _split_sql_statements(script: str):\n",
    "    out, buf, q = [], [], None\n",
    "    i, n = 0, len(script)\n",
    "    while i < n:\n",
    "        ch = script[i]\n",
    "        if q:\n",
    "            buf.append(ch)\n",
    "            if ch == q:\n",
    "                if i + 1 < n and script[i+1] == q:\n",
    "                    buf.append(script[i+1]); i += 1\n",
    "                else:\n",
    "                    q = None\n",
    "        else:\n",
    "            if ch in (\"'\", '\"', \"`\"):\n",
    "                q = ch; buf.append(ch)\n",
    "            elif ch == ';':\n",
    "                s = \"\".join(buf).strip()\n",
    "                if s: out.append(s)\n",
    "                buf = []\n",
    "            else:\n",
    "                buf.append(ch)\n",
    "        i += 1\n",
    "    tail = \"\".join(buf).strip()\n",
    "    if tail: out.append(tail)\n",
    "    return out\n",
    "\n",
    "import re\n",
    "from sqlglot import parse_one\n",
    "\n",
    "_SQLITE_DATE_RE = re.compile(\n",
    "    r\"\"\"\\bdate\\s*\\(\\s*'now'\\s*(?:,\\s*'([+-])\\s*(\\d+)\\s*(year|month|day)s?'\\s*)?\\)\"\"\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "_SQLITE_DATETIME_RE = re.compile(\n",
    "    r\"\"\"\\bdatetime\\s*\\(\\s*'now'\\s*(?:,\\s*'([+-])\\s*(\\d+)\\s*(year|month|day|hour|minute|second)s?'\\s*)?\\)\"\"\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "def _normalize_sqlite_dates(sql: str) -> str:\n",
    "    # date('now') or date('now','-1 year') -> CURRENT_DATE +/- INTERVAL 'N unit'\n",
    "    def _date_subst(m):\n",
    "        sign, num, unit = m.group(1), m.group(2), m.group(3)\n",
    "        if not sign:  # just date('now')\n",
    "            return \"CURRENT_DATE\"\n",
    "        op = \"-\" if sign == \"-\" else \"+\"\n",
    "        return f\"CURRENT_DATE {op} INTERVAL '{num} {unit.lower()}'\"\n",
    "    sql = _SQLITE_DATE_RE.sub(_date_subst, sql)\n",
    "\n",
    "    # datetime('now') / datetime('now','+/-N unit') -> CURRENT_TIMESTAMP +/- INTERVAL 'N unit'\n",
    "    def _dt_subst(m):\n",
    "        sign, num, unit = m.group(1), m.group(2), m.group(3)\n",
    "        if not sign:\n",
    "            return \"CURRENT_TIMESTAMP\"\n",
    "        op = \"-\" if sign == \"-\" else \"+\"\n",
    "        return f\"CURRENT_TIMESTAMP {op} INTERVAL '{num} {unit.lower()}'\"\n",
    "    sql = _SQLITE_DATETIME_RE.sub(_dt_subst, sql)\n",
    "\n",
    "    return sql\n",
    "\n",
    "def _mysql_to_duckdb(stmt: str) -> str:\n",
    "    s = _normalize_sqlite_dates(stmt)  # <-- NEW: normalize SQLite first\n",
    "    try:\n",
    "        return parse_one(s, read=\"mysql\").sql(dialect=\"duckdb\")\n",
    "    except Exception:\n",
    "        # minimal fallbacks for MySQLisms if parse fails\n",
    "        s = re.sub(r\"`([^`]+)`\", r'\"\\1\"', s)\n",
    "        s = re.sub(\n",
    "            r\"DATE_SUB\\s*\\(\\s*(CURRENT_DATE|NOW\\(\\))\\s*,\\s*INTERVAL\\s+(\\d+)\\s+(YEAR|MONTH|DAY)\\s*\\)\",\n",
    "            lambda m: f\"{'CURRENT_DATE' if m.group(1).startswith('CURRENT') else 'CURRENT_DATE'} - INTERVAL '{m.group(2)} {m.group(3).lower()}'\",\n",
    "            s, flags=re.IGNORECASE,\n",
    "        )\n",
    "        s = re.sub(\n",
    "            r\"DATE_ADD\\s*\\(\\s*(CURRENT_DATE|NOW\\(\\))\\s*,\\s*INTERVAL\\s+(\\d+)\\s+(YEAR|MONTH|DAY)\\s*\\)\",\n",
    "            lambda m: f\"{'CURRENT_DATE' if m.group(1).startswith('CURRENT') else 'CURRENT_DATE'} + INTERVAL '{m.group(2)} {m.group(3).lower()}'\",\n",
    "            s, flags=re.IGNORECASE,\n",
    "        )\n",
    "        s = re.sub(r\"\\bIFNULL\\s*\\(\", \"COALESCE(\", s, flags=re.IGNORECASE)\n",
    "        s = re.sub(r\"\\bLOCATE\\s*\\(\\s*([^,]+)\\s*,\\s*([^)]+)\\)\", r\"STRPOS(\\2, \\1)\", s, flags=re.IGNORECASE)\n",
    "        return s\n",
    "\n",
    "def _normalize_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == \"O\":\n",
    "            try:\n",
    "                df[c] = pd.to_numeric(df[c])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return df.replace({np.nan: None})\n",
    "\n",
    "def _exec_script_capture_last_select(con, script: str):\n",
    "    last_df, last_sel_sql = None, None\n",
    "    for raw in _split_sql_statements(script):\n",
    "        stmt = _mysql_to_duckdb(raw)\n",
    "        # detect SELECT after minimal comment strip\n",
    "        s = re.sub(r\"^\\s*(--[^\\n]*\\n|/\\*.*?\\*/\\s*)*\", \"\", stmt, flags=re.DOTALL)\n",
    "        if re.match(r\"(?is)^\\s*(with\\b.*?select|select)\\b\", s):\n",
    "            last_df = con.execute(stmt).fetchdf()\n",
    "            last_sel_sql = stmt\n",
    "        else:\n",
    "            con.execute(stmt)\n",
    "    if last_df is not None:\n",
    "        last_df = _normalize_df(last_df)\n",
    "    return last_df, last_sel_sql\n",
    "\n",
    "def evaluate_sql(sql_context: str, golden_sql: str, predicted_sql: str):\n",
    "    con = duckdb.connect(\":memory:\")\n",
    "\n",
    "    # context\n",
    "    try:\n",
    "        for raw in _split_sql_statements(sql_context):\n",
    "            con.execute(_mysql_to_duckdb(raw))\n",
    "    except Exception as e:\n",
    "        return 0, {\"reason\": \"context_error\", \"detail\": str(e)}\n",
    "\n",
    "    # golden\n",
    "    try:\n",
    "        gold_df, gold_last_select = _exec_script_capture_last_select(con, golden_sql)\n",
    "    except Exception as e:\n",
    "        return 0, {\"reason\": \"gold_error\", \"detail\": str(e)}\n",
    "    if gold_df is None:\n",
    "        return 0, {\"reason\": \"gold_no_select\", \"detail\": \"No SELECT in golden_sql.\"}\n",
    "\n",
    "    # predicted\n",
    "    try:\n",
    "        pred_df, pred_last_select = _exec_script_capture_last_select(con, predicted_sql)\n",
    "    except Exception as e:\n",
    "        return 0, {\"reason\": \"pred_error\", \"detail\": str(e)}\n",
    "    if pred_df is None:\n",
    "        return 0, {\"reason\": \"pred_no_select\", \"detail\": \"No SELECT in predicted_sql.\"}\n",
    "\n",
    "    # column alignment (allow pred supersets; else try set/positional)\n",
    "    gold_cols, pred_cols = list(gold_df.columns), list(pred_df.columns)\n",
    "    if gold_cols == pred_cols:\n",
    "        pass\n",
    "    elif set(gold_cols).issubset(pred_cols):\n",
    "        pred_df = pred_df[gold_cols]\n",
    "    elif set(gold_cols) == set(pred_cols):\n",
    "        pred_df = pred_df[gold_cols]\n",
    "    elif gold_df.shape[1] == pred_df.shape[1]:\n",
    "        new_names = [f\"c{i}\" for i in range(gold_df.shape[1])]\n",
    "        gold_df = gold_df.copy(); pred_df = pred_df.copy()\n",
    "        gold_df.columns = new_names; pred_df.columns = new_names\n",
    "    else:\n",
    "        return 0, {\"reason\": \"column_mismatch\",\n",
    "                   \"detail\": f\"Different number of columns: expected {gold_df.shape[1]}, got {pred_df.shape[1]}\"}\n",
    "\n",
    "    # ordering rule from gold's last SELECT\n",
    "    gold_has_order = bool(_ORDER_BY.search(gold_last_select or \"\"))\n",
    "    if not gold_has_order:\n",
    "        try:\n",
    "            g = gold_df.sort_values(by=list(gold_df.columns), kind=\"mergesort\").reset_index(drop=True)\n",
    "            p = pred_df.sort_values(by=list(gold_df.columns), kind=\"mergesort\").reset_index(drop=True)\n",
    "        except Exception:\n",
    "            g = gold_df.reset_index(drop=True); p = pred_df.reset_index(drop=True)\n",
    "    else:\n",
    "        g = gold_df.reset_index(drop=True); p = pred_df.reset_index(drop=True)\n",
    "\n",
    "    # value compare\n",
    "    if g.shape != p.shape:\n",
    "        return 0, {\"reason\": \"shape_mismatch\", \"detail\": f\"gold {g.shape} vs pred {p.shape}\"}\n",
    "\n",
    "    for c in g.columns:\n",
    "        if pd.api.types.is_numeric_dtype(g[c]) and pd.api.types.is_numeric_dtype(p[c]):\n",
    "            if not np.allclose(g[c].values, p[c].values, rtol=1e-6, atol=1e-8, equal_nan=True):\n",
    "                return 0, {\"reason\": \"value_mismatch\", \"detail\": f\"Numeric mismatch in '{c}'\",\n",
    "                           \"gold_head\": g.head(10).to_dict(\"records\"),\n",
    "                           \"pred_head\": p.head(10).to_dict(\"records\")}\n",
    "        else:\n",
    "            eq = [(x == y) or (x is None and y is None) for x, y in zip(g[c].values, p[c].values)]\n",
    "            if not all(eq):\n",
    "                return 0, {\"reason\": \"value_mismatch\", \"detail\": f\"Mismatch in '{c}'\",\n",
    "                           \"gold_head\": g.head(10).to_dict(\"records\"),\n",
    "                           \"pred_head\": p.head(10).to_dict(\"records\")}\n",
    "    return 1, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb24b156",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "936c6363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: CREATE TABLE upgrades (id INT, cost FLOAT, type TEXT); INSERT INTO upgrades (id, cost, type) VALUES (1, 500, 'Insulation'), (2, 1000, 'HVAC'), (3, 1500, 'Lighting');\n",
      "Prompt: Find the energy efficiency upgrades with the highest cost and their types.\n",
      "Golden sql: SELECT type, cost FROM (SELECT type, cost, ROW_NUMBER() OVER (ORDER BY cost DESC) as rn FROM upgrades) sub WHERE rn = 1;\n",
      "Prediction(\n",
      "    reasoning='We need the upgrade(s) that have the maximum cost. Use a subquery to get MAX(cost) and return rows matching that value (including id, type, and cost).',\n",
      "    sql='SELECT id, type, cost FROM upgrades WHERE cost = (SELECT MAX(cost) FROM upgrades);'\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "demo_index = 4\n",
    "context = ds['train'][demo_index]['sql_context']\n",
    "prompt = ds['train'][demo_index]['sql_prompt']\n",
    "golden_sql = ds['train'][demo_index]['sql']\n",
    "\n",
    "print(f\"Context: {context}\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Golden sql: {golden_sql}\")\n",
    "result = program(sql_context=context, sql_prompt=prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42b7fd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 None\n"
     ]
    }
   ],
   "source": [
    "score, info = evaluate_sql(context, golden_sql, result.sql)\n",
    "print(score, info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58142861",
   "metadata": {},
   "source": [
    "## Environment didn't work, let's use LLM as Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c72a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Judge(dspy.Signature):\n",
    "    \"\"\"You are required to judge two SQL queries for functional similarity. You will be given a context of how the table(s) and data were created, and the natural language prompt from the user\"\"\"\n",
    "\n",
    "    sql_context: str = dspy.InputField(description=\"SQL statement(s) creating the table(s) and the input data\")\n",
    "    sql_prompt: str = dspy.InputField(description=\"Natural language prompt from the user\")\n",
    "    golden_sql: str = dspy.InputField(description=\"The golden SQL query from our dataset\")\n",
    "    candidate_sql: str = dspy.InputField(description=\"A SQL query generated by a model for the same prompt\")\n",
    "    similar: bool = dspy.OutputField(description=\"True if the candidate SQL query is functionally similar to the golden SQL query\")\n",
    "\n",
    "judge = dspy.ChainOfThought(Judge)\n",
    "judge.lm = reflection_lm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1677bf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: CREATE TABLE upgrades (id INT, cost FLOAT, type TEXT); INSERT INTO upgrades (id, cost, type) VALUES (1, 500, 'Insulation'), (2, 1000, 'HVAC'), (3, 1500, 'Lighting');\n",
      "Prompt: Find the energy efficiency upgrades with the highest cost and their types.\n",
      "Golden SQL: SELECT type, cost FROM (SELECT type, cost, ROW_NUMBER() OVER (ORDER BY cost DESC) as rn FROM upgrades) sub WHERE rn = 1;\n",
      "Candidate SQL: SELECT id, type, cost FROM upgrades WHERE cost = (SELECT MAX(cost) FROM upgrades);\n",
      "Judge Response: Prediction(\n",
      "    reasoning='Both queries return the upgrade(s) that have the maximum cost and include the type and cost information. Differences:\\n- The candidate also returns the id column (extra column not present in the golden query).\\n- The golden query uses ROW_NUMBER() and will return a single row (even if there are ties), whereas the candidate uses cost = MAX(cost) and will return all rows that tie for the maximum cost.\\n\\nDespite these differences in returned columns and tie-handling, the candidate still retrieves the highest-cost upgrade(s) and their types, so it is functionally similar to the golden query for the user intent.',\n",
      "    similar=True\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "judge_response = judge(sql_context=context, sql_prompt=prompt, golden_sql=golden_sql, candidate_sql=result.sql)\n",
    "print(f\"Context: {context}\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Golden SQL: {golden_sql}\")\n",
    "print(f\"Candidate SQL: {result.sql}\")\n",
    "print(f\"Judge Response: {judge_response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7746087",
   "metadata": {},
   "source": [
    "# Get ready to GEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c35b7379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets dspy-ai\n",
    "import math, random\n",
    "from typing import Callable, List, Tuple, Optional\n",
    "from datasets import Dataset, DatasetDict\n",
    "from dspy import GEPA\n",
    "\n",
    "def split_for_gepa(\n",
    "    ds: Dataset,\n",
    "    to_example: Callable[[dict], \"dspy.Example\"],\n",
    "    val_size: float = 0.15,\n",
    "    seed: int = 42,\n",
    "    group_col: Optional[str] = None,\n",
    "    stratify_col: Optional[str] = None,\n",
    ") -> Tuple[List[\"dspy.Example\"], List[\"dspy.Example\"]]:\n",
    "    \"\"\"\n",
    "    Return (train_set, val_set) as lists of dspy.Example.\n",
    "    - If group_col is set: group-aware split (no group leakage).\n",
    "    - Else if stratify_col is set: use HF stratified split.\n",
    "    - Else: random split.\n",
    "    \"\"\"\n",
    "    assert 0.0 < val_size < 1.0, \"val_size must be in (0,1)\"\n",
    "    rng = random.Random(seed)\n",
    "\n",
    "    # --- Group-aware split (preferred for text2sql) ---\n",
    "    if group_col:\n",
    "        groups = ds[group_col]\n",
    "        # Build group -> indices\n",
    "        g2idx = {}\n",
    "        for i, g in enumerate(groups):\n",
    "            g2idx.setdefault(g, []).append(i)\n",
    "        uniq_groups = list(g2idx.keys())\n",
    "        rng.shuffle(uniq_groups)\n",
    "        n_val_groups = max(1, math.floor(val_size * len(uniq_groups)))\n",
    "        val_groups = set(uniq_groups[:n_val_groups])\n",
    "\n",
    "        val_idx = [i for g in val_groups for i in g2idx[g]]\n",
    "        train_idx = [i for g in uniq_groups[n_val_groups:] for i in g2idx[g]]\n",
    "\n",
    "        # Edge case: if a group is gigantic, ensure both splits non-empty\n",
    "        if not train_idx or not val_idx:\n",
    "            # fallback: plain random split\n",
    "            perm = list(range(len(ds)))\n",
    "            rng.shuffle(perm)\n",
    "            cut = max(1, math.floor(val_size * len(ds)))\n",
    "            val_idx, train_idx = perm[:cut], perm[cut:]\n",
    "\n",
    "        ds_train = ds.select(train_idx)\n",
    "        ds_val = ds.select(val_idx)\n",
    "\n",
    "    # --- Stratified split (when you have a label/cluster column) ---\n",
    "    elif stratify_col:\n",
    "        # HF does stratify on categorical-like columns\n",
    "        parts: DatasetDict = ds.train_test_split(\n",
    "            test_size=val_size,\n",
    "            seed=seed,\n",
    "            stratify_by_column=stratify_col,\n",
    "        )\n",
    "        ds_train, ds_val = parts[\"train\"], parts[\"test\"]\n",
    "\n",
    "    # --- Simple random split ---\n",
    "    else:\n",
    "        parts: DatasetDict = ds.train_test_split(test_size=val_size, seed=seed)\n",
    "        ds_train, ds_val = parts[\"train\"], parts[\"test\"]\n",
    "\n",
    "    # Map to dspy.Example lists\n",
    "    train_set = [to_example(r) for r in ds_train]\n",
    "    val_set = [to_example(r) for r in ds_val]\n",
    "    return train_set, val_set\n",
    "\n",
    "def to_dspy_example(row):\n",
    "    # mark inputs; leave gold 'sql' as label\n",
    "    return dspy.Example(\n",
    "        sql_prompt=row[\"sql_prompt\"],\n",
    "        sql_context=row[\"sql_context\"],\n",
    "        sql=row[\"sql\"],          # gold label\n",
    "    ).with_inputs(\"sql_prompt\", \"sql_context\")\n",
    "\n",
    "\n",
    "# call function that splits ds['train'] into train_set and val_set as needed\n",
    "# ds is your loaded HF dataset dict; we split ds[\"train\"]\n",
    "train_set, val_set = split_for_gepa(\n",
    "    ds[\"train\"],\n",
    "    to_dspy_example,          # your to_dspy_example(row)\n",
    "    val_size=0.5,\n",
    "    seed=42,\n",
    "    group_col=None,      # e.g., \"db_id\" if available\n",
    "    stratify_col=None,   # or a column like \"op_class\" if you want stratification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88d14596",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_variants_to_try = 20 # number of variants to test\n",
    "mini_batch_size = 20 # mini-batch size\n",
    "val_set_size = 200 # val-set size\n",
    "train_set_size = 200 # train-set size\n",
    "\n",
    "def budget_for_variants(N, V, k, slack=2):\n",
    "    # slack handles occasional extra probes/promotions\n",
    "    return V + N * (k + slack)\n",
    "\n",
    "def metric_with_feedback(example, pred, trace=None, pred_name=None, pred_trace=None):\n",
    "    judge_response = judge(sql_context=example.sql_context, sql_prompt=example.sql_prompt, golden_sql=example.sql, candidate_sql=pred.sql)\n",
    "    score = 0\n",
    "    if (judge_response.similar):\n",
    "        score = 1\n",
    "    return dspy.Prediction(score=score, feedback=judge_response.reasoning)\n",
    "\n",
    "val_for_tracking = val_set[:val_set_size]   # 128–512 is a good range\n",
    "train_set_for_optimization = train_set[:train_set_size]\n",
    "optimizer = GEPA(\n",
    "    metric=metric_with_feedback,\n",
    "    num_threads=32,\n",
    "    track_stats=True,\n",
    "    reflection_minibatch_size=mini_batch_size,\n",
    "    reflection_lm=reflection_lm,\n",
    "    use_wandb=True,\n",
    "    wandb_api_key=wandb_api_key,\n",
    "    log_dir=\"logs\",\n",
    "    auto=\"light\"   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77daa2b",
   "metadata": {},
   "source": [
    "# Run GEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "940c1f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 17:23:32 INFO dspy.teleprompt.gepa.gepa: Running GEPA for approx 1180 metric calls of the program. This amounts to 2.95 full evals on the train+val set.\n",
      "2025/10/14 17:23:32 INFO dspy.teleprompt.gepa.gepa: Using 200 examples for tracking Pareto scores. You can consider using a smaller sample of the valset to allow GEPA to explore more diverse solutions within the same budget.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>base_program_full_valset_score</td><td>▁</td></tr><tr><td>best_program_as_per_agg_score</td><td>▁███</td></tr><tr><td>best_program_as_per_agg_score_valset</td><td>▁███</td></tr><tr><td>best_score_on_train_val</td><td>▁███</td></tr><tr><td>best_score_on_valset</td><td>▁███</td></tr><tr><td>best_valset_agg_score</td><td>▁███</td></tr><tr><td>iteration</td><td>▁▂▃▅▆▇█</td></tr><tr><td>linear_pareto_front_program_idx</td><td>▁███</td></tr><tr><td>new_program_idx</td><td>▁▃▆█</td></tr><tr><td>new_subsample_score</td><td>█▆▁▅▅▆</td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>base_program_full_valset_score</td><td>0.615</td></tr><tr><td>best_program_as_per_agg_score</td><td>2</td></tr><tr><td>best_program_as_per_agg_score_valset</td><td>2</td></tr><tr><td>best_score_on_train_val</td><td>0.65</td></tr><tr><td>best_score_on_valset</td><td>0.65</td></tr><tr><td>best_valset_agg_score</td><td>0.65</td></tr><tr><td>iteration</td><td>6</td></tr><tr><td>linear_pareto_front_program_idx</td><td>2</td></tr><tr><td>new_instruction_predict</td><td>You are a SQL-writin...</td></tr><tr><td>new_program_idx</td><td>4</td></tr><tr><td>+6</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">floral-morning-17</strong> at: <a href='https://wandb.ai/raveeshbhalla90-personal/text2sql/runs/tx1h8cjc' target=\"_blank\">https://wandb.ai/raveeshbhalla90-personal/text2sql/runs/tx1h8cjc</a><br> View project at: <a href='https://wandb.ai/raveeshbhalla90-personal/text2sql' target=\"_blank\">https://wandb.ai/raveeshbhalla90-personal/text2sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251014_165529-tx1h8cjc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/raveesh/dev/text2sql/wandb/run-20251014_172332-yvdh3d07</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/raveeshbhalla90-personal/text2sql/runs/yvdh3d07' target=\"_blank\">denim-violet-18</a></strong> to <a href='https://wandb.ai/raveeshbhalla90-personal/text2sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/raveeshbhalla90-personal/text2sql' target=\"_blank\">https://wandb.ai/raveeshbhalla90-personal/text2sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/raveeshbhalla90-personal/text2sql/runs/yvdh3d07' target=\"_blank\">https://wandb.ai/raveeshbhalla90-personal/text2sql/runs/yvdh3d07</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GEPA Optimization:   0%|          | 0/1180 [00:00<?, ?rollouts/s]2025/10/14 17:23:35 INFO dspy.evaluate.evaluate: Average Metric: 111.0 / 200 (55.5%)\n",
      "2025/10/14 17:23:35 INFO dspy.teleprompt.gepa.gepa: Iteration 0: Base program full valset score: 0.555\n",
      "GEPA Optimization:  17%|█▋        | 200/1180 [00:01<00:07, 125.92rollouts/s]2025/10/14 17:23:35 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Selected program 0 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.00 / 20 (70.0%): 100%|██████████| 20/20 [00:00<00:00, 175.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 17:23:35 INFO dspy.evaluate.evaluate: Average Metric: 14.0 / 20 (70.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 17:23:36 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Proposed new text for predict: You are a SQL expert assistant whose job is: given (1) a short natural-language request (sql_prompt) and (2) a small SQL schema and seed data snippet (sql_context), produce a single correct SQL statement that answers the prompt plus a brief reasoning explanation of how you formed the query.\n",
      "\n",
      "Output format (always):\n",
      "- A short \"reasoning\" paragraph describing the approach, assumptions, and any edge-cases handled.\n",
      "- The SQL statement (runnable against the provided schema). Do not produce extraneous SQL or multiple alternative queries unless the prompt explicitly asks for options.\n",
      "\n",
      "General rules and intent:\n",
      "1. Preserve the semantics of the user's request exactly. Do not add, remove, or weaken required filters/constraints unless:\n",
      "   - The schema does not contain needed columns/tables, in which case explicitly state the assumption you must make.\n",
      "   - The prompt is ambiguous and you state the chosen interpretation as an assumption in the reasoning.\n",
      "2. Prefer SQL that is correct and portable across common RDBMSs. Use standard constructs (SELECT, INSERT INTO (collist) VALUES, UPDATE ... WHERE, DELETE ... WHERE, GROUP BY, HAVING, JOIN, LEFT JOIN, NOT EXISTS, window functions) rather than esoteric vendor-only syntax. If you use a dialect-specific feature, mention it in reasoning.\n",
      "3. When producing DML (INSERT/UPDATE/DELETE), always:\n",
      "   - Use an explicit column list in INSERT INTO (col1, col2, ...) VALUES (...) unless the schema context shows the exact column order and the prompt implies omitting it.\n",
      "   - Ensure string/date values are quoted correctly (single quotes).\n",
      "   - For destructive operations (DELETE/UPDATE), include the requested WHERE clause exactly. Do not add additional WHERE filters that change semantics.\n",
      "4. Aggregation and grouping:\n",
      "   - Use GROUP BY for per-group aggregates. Use HAVING to filter groups (e.g., \"groups with more than 5 records\").\n",
      "   - Be mindful of result shape differences: returning one aggregated scalar (SELECT MAX(...)) vs returning multiple rows (SELECT col, MAX(...) GROUP BY col) are different. Match the shape the prompt implies.\n",
      "   - If the prompt expects counts for all categories including zero, prefer LEFT JOIN or conditional aggregation (SUM(CASE WHEN ... THEN 1 ELSE 0 END)) and COALESCE to present zeros. If the prompt expects only existing rows, use an inner join or filter on the table with data.\n",
      "5. Joins vs filtering on same-named columns:\n",
      "   - If the logical filter should be applied to an associated table (e.g., count companies by companies.sector), join to that table and apply the filter on the correct table. Do not filter only on an identically-named column in the other table unless the prompt or schema implies they are authoritative.\n",
      "6. NULLs and empty-result behavior:\n",
      "   - For aggregate results where user likely expects 0 instead of NULL (e.g., total pallets, sum counts), wrap with COALESCE(..., 0) if appropriate, and state that decision in reasoning.\n",
      "   - For case-insensitive string matches, use LOWER(column) LIKE '%term%' and mention that this is for case-insensitivity.\n",
      "7. Conditional aggregation and including rows with no matches:\n",
      "   - Use LEFT JOIN + SUM(CASE WHEN ...) or COUNT(CASE WHEN ... THEN 1 END) when you need to include rows that lack related records (e.g., list every member and their steps on a date, including zero).\n",
      "   - Use inner JOIN or filter on the activity table when you want only members with activity on that date.\n",
      "   - Always explain this difference in reasoning if either approach could be chosen.\n",
      "8. Filtering by date/time, booleans, and enumerations:\n",
      "   - Use exact literal formats matching the context (e.g., 'YYYY-MM-DD' for dates) and match the values exactly as shown in the schema data (case-sensitive where appropriate).\n",
      "   - If country or region strings may have variants (e.g., 'UK' vs 'United Kingdom'), do not invent extra matches unless the prompt says to accept synonyms; if you choose to accept synonyms, mention it.\n",
      "9. Top-N queries:\n",
      "   - Use ORDER BY ... DESC and LIMIT n or FETCH FIRST n ROWS ONLY. Either is acceptable; if you use LIMIT, it's widely supported but note if you prefer standard SQL you can use FETCH.\n",
      "10. Anti-joins and \"none\" semantics:\n",
      "   - To count rows that have no related rows in another table, prefer NOT EXISTS with a correlated subquery (correct for all SQL dialects), or LEFT JOIN ... WHERE joined_key IS NULL — but be explicit which interpretation of \"have not completed any training\" you used (any training vs training in a specific department).\n",
      "11. Do not invent tables, columns, or relationships that are not present in sql_context. If the prompt requires data from a missing table (e.g., Donations in context-less prompt), either:\n",
      "   - state the assumption (e.g., \"assuming a Donations table with columns DonorID, Amount\"), or\n",
      "   - ask for clarification (short statement in reasoning).\n",
      "12. Output style:\n",
      "   - Keep reasoning concise (1–4 sentences) and clearly state any assumptions or non-obvious decisions (e.g., using COALESCE to return 0).\n",
      "   - Provide a single SQL statement only (no extra statements like COMMIT unless the prompt asked about transaction behavior; you may mention commit as advice in reasoning).\n",
      "   - Use aliases where they improve readability but do not change semantics.\n",
      "\n",
      "Common SQL patterns you should use when relevant (describe briefly in reasoning when used):\n",
      "- Aggregation: SELECT SUM(col) AS total FROM table WHERE ...;\n",
      "- Conditional aggregation: SUM(CASE WHEN condition THEN value ELSE 0 END) or COUNT(CASE WHEN condition THEN 1 END);\n",
      "- Include unmatched parent rows: FROM parent LEFT JOIN child ON ... GROUP BY parent.id;\n",
      "- Anti-join: WHERE NOT EXISTS (SELECT 1 FROM child c WHERE c.parent_id = p.id ...);\n",
      "- Percentage/share using window functions: ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) AS percentage;\n",
      "- Case-insensitive search: LOWER(name) LIKE '%term%';\n",
      "- Defensive null-to-zero for aggregates: COALESCE(SUM(...), 0);\n",
      "- Top N: ORDER BY aggregate_col DESC LIMIT N (or FETCH FIRST N ROWS ONLY).\n",
      "\n",
      "Error-prone pitfalls to avoid (and mention if you considered them):\n",
      "- Omitting a WHERE clause the prompt required, or adding extra WHERE conditions that change the result set.\n",
      "- Using LEFT JOIN + conditional aggregation when the prompt wanted only rows with matches (and vice versa).\n",
      "- Filtering on the wrong table when the schema indicates the authoritative value lives in a joined table.\n",
      "- Changing the result shape unexpectedly (one-row scalar vs multiple rows grouped) without explaining.\n",
      "- Returning NULL from aggregate when user likely expects 0 (explicitly handle or explain).\n",
      "\n",
      "If the schema is small and seed data is provided, tailor the SQL to the schema; but always write the query to be correct for general data, not only the provided seed rows.\n",
      "\n",
      "Example response structure you must follow:\n",
      "reasoning\n",
      "<one or two brief sentences explaining approach and assumptions>\n",
      "\n",
      "sql\n",
      "<single SQL statement>\n",
      "\n",
      "Follow these rules for every request.\n",
      "2025/10/14 17:23:36 INFO dspy.evaluate.evaluate: Average Metric: 16.0 / 20 (80.0%)\n",
      "2025/10/14 17:23:37 INFO dspy.evaluate.evaluate: Average Metric: 107.0 / 200 (53.5%)\n",
      "2025/10/14 17:23:37 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Full valset score for new program: 0.535\n",
      "2025/10/14 17:23:37 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Full train_val score for new program: 0.535\n",
      "2025/10/14 17:23:37 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Individual valset scores for new program: [0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0]\n",
      "2025/10/14 17:23:37 INFO dspy.teleprompt.gepa.gepa: Iteration 1: New valset pareto front scores: [0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0]\n",
      "2025/10/14 17:23:37 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Full valset pareto front score: 0.6\n",
      "2025/10/14 17:23:37 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Updated valset pareto front programs: [{0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}, {0, 1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}, {0, 1}]\n",
      "2025/10/14 17:23:37 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best valset aggregate score so far: 0.555\n",
      "2025/10/14 17:23:37 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best program as per aggregate score on train_val: 0\n",
      "2025/10/14 17:23:37 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best program as per aggregate score on valset: 0\n",
      "2025/10/14 17:23:37 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best score on valset: 0.555\n",
      "2025/10/14 17:23:37 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best score on train_val: 0.555\n",
      "2025/10/14 17:23:37 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Linear pareto front program index: 0\n",
      "2025/10/14 17:23:37 INFO dspy.teleprompt.gepa.gepa: Iteration 1: New program candidate index: 1\n",
      "GEPA Optimization:  37%|███▋      | 440/1180 [00:03<00:06, 120.87rollouts/s]2025/10/14 17:23:37 INFO dspy.teleprompt.gepa.gepa: Iteration 2: No merge candidates found\n",
      "2025/10/14 17:23:37 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Selected program 1 score: 0.535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.00 / 20 (70.0%): 100%|██████████| 20/20 [00:00<00:00, 151.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 17:23:37 INFO dspy.evaluate.evaluate: Average Metric: 14.0 / 20 (70.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 17:23:38 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Proposed new text for predict: You are a SQL expert assistant. You receive two inputs:\n",
      "- sql_prompt: a short natural-language request describing exactly what SQL result or change the user wants.\n",
      "- sql_context: a small SQL schema and (optional) seed data snippet, using CREATE TABLE / INSERT statements that define available tables, columns, and example values.\n",
      "\n",
      "Your job: produce exactly two labeled sections in this order:\n",
      "1) reasoning — a concise 1–4 sentence paragraph describing your approach, any assumptions, and any non-obvious decisions or edge-cases handled.\n",
      "2) sql — a single SQL statement (runnable against the provided schema) that implements the request.\n",
      "\n",
      "Formatting and content rules (follow strictly):\n",
      "\n",
      "Overall output format\n",
      "- Always output exactly two sections prefixed by the words \"reasoning\" and \"sql\" (lowercase), each followed by a newline and then the content. Do not include additional sections, explanations, or unrelated text.\n",
      "- The \"reasoning\" content must be brief (1–4 sentences) and must explicitly state any assumptions, ambiguous-interpretation choices, or use of dialect-specific features.\n",
      "- The \"sql\" content must contain exactly one SQL statement and nothing else. Do not output multiple statements, DDL plus DML, COMMIT/ROLLBACK, or commentary inside the sql section.\n",
      "\n",
      "Preserve semantics and filters\n",
      "- Preserve the semantics of the user's request exactly. Do not add, remove, or weaken required filters/constraints unless:\n",
      "  - the schema lacks required tables/columns (in which case state the assumption you make in reasoning), or\n",
      "  - the prompt is ambiguous (state chosen interpretation in reasoning).\n",
      "- Never add extra WHERE filters that change the requested result. For destructive operations (UPDATE / DELETE), include exactly the WHERE clause the prompt requested; do not append extra predicates.\n",
      "\n",
      "Single statement rules\n",
      "- Return one SQL statement only. No additional helper queries, temporary table creation, or multiple DML statements.\n",
      "- Use aliases where they improve readability but do not change semantics.\n",
      "\n",
      "Portability and syntax\n",
      "- Prefer standard SQL constructs: SELECT, INSERT INTO (col list) VALUES (...), UPDATE ... WHERE, DELETE ... WHERE, GROUP BY, HAVING, JOIN, LEFT JOIN, NOT EXISTS, window functions, LIMIT or FETCH FIRST for top-N.\n",
      "- If you use a dialect-specific feature (e.g., INTERVAL syntax, TIMESTAMPDIFF, RETURNING, UPSERT, BOOLEAN literals that are not standard), state it in the reasoning and justify why it was used.\n",
      "- For top-N use ORDER BY ... LIMIT n or ORDER BY ... FETCH FIRST n ROWS ONLY; either is acceptable, but mention if it's dialect-specific.\n",
      "\n",
      "DML specifics\n",
      "- For INSERT, always use an explicit column list: INSERT INTO table (col1, col2, ...) VALUES (...), unless the schema context proves the exact column order and the prompt explicitly implies omitting it.\n",
      "- For UPDATE and DELETE, include the exact WHERE clause as requested by the prompt. Do not add additional WHERE filters.\n",
      "- If generating a surrogate primary key (id) because the schema lacks auto-increment, explicitly state the approach (e.g., SELECT COALESCE(MAX(id),0)+1 ...) in the reasoning.\n",
      "\n",
      "Aggregation and grouping\n",
      "- Use GROUP BY for per-group aggregates. Use HAVING to filter groups.\n",
      "- Match the result shape implied by the prompt: one scalar (e.g., a single MAX(...)) vs multi-row grouped result.\n",
      "- If the user likely expects zero for missing aggregates, wrap in COALESCE(..., 0) and state that decision in reasoning. (Example: COALESCE(SUM(...), 0).)\n",
      "- If the prompt expects counts for all parent rows including zero, use LEFT JOIN or conditional aggregation (SUM(CASE WHEN ... THEN 1 ELSE 0 END)) and COALESCE to show zeros. If prompt expects only existing rows, use INNER JOIN or filter on the child table.\n",
      "- When computing percentages or shares, using window functions is acceptable (e.g., ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2)) — mention it in reasoning.\n",
      "\n",
      "Joins and foreign data\n",
      "- Apply filters to the authoritative table. If a value logically belongs to a joined table, filter there rather than filtering an identically named column in another table.\n",
      "- Use LEFT JOIN to include parent rows with no children; use INNER JOIN when only matching rows should be returned. Explicitly mention in reasoning why you chose LEFT vs INNER if it affects whether zero/missing children are included.\n",
      "- To count rows that have no related rows in another table, prefer NOT EXISTS correlated subqueries (correct across SQL dialects) or LEFT JOIN ... WHERE joined_key IS NULL — and state which interpretation of \"none\" you used.\n",
      "\n",
      "NULL and empty-result behavior\n",
      "- For aggregates where users likely expect 0 (e.g., totals, sums), use COALESCE(..., 0) and note that choice in reasoning.\n",
      "- COUNT(*) returns 0 when no rows match; AVG()/SUM()/MAX() return NULL when no rows match — handle or explain as appropriate.\n",
      "- Be explicit about case-insensitive string matching (use LOWER(column) LIKE '%term%') and explain that it implements case-insensitivity.\n",
      "\n",
      "Dates, booleans, and enumerations\n",
      "- Use literal date format 'YYYY-MM-DD' when filtering by dates.\n",
      "- Follow exact values as shown in sql_context (case-sensitive where appropriate). Do not invent synonyms (e.g., 'UK' vs 'United Kingdom') unless you state that you are accepting synonyms explicitly.\n",
      "- When computing date ranges like \"last week\" or \"past 7 days\", explain your inclusive/exclusive endpoints in reasoning (e.g., BETWEEN CURRENT_DATE - INTERVAL '7 days' AND CURRENT_DATE is inclusive of both ends in many dialects; DATE >= 'YYYY-MM-DD' excludes the end if you choose <).\n",
      "\n",
      "Conditional aggregation and duplicates\n",
      "- If likes/shares/other metrics might have multiple rows per grouped key, aggregate them with SUM(...) and GROUP BY to avoid row multiplication. If the prompt asks for counts per post, ensure you aggregate on post_id.\n",
      "- When you need to include posts with no likes/shares, LEFT JOIN and COALESCE the aggregated values to 0.\n",
      "- Avoid returning raw non-aggregated columns from tables that have multiple child rows unless you GROUP BY all non-aggregated columns.\n",
      "\n",
      "Top-N and ties\n",
      "- For Top-N, use ORDER BY ... LIMIT n. If ties should be handled (e.g., return all tied maxima), either:\n",
      "  - use ORDER BY ... LIMIT 1 to return one arbitrary top row (state that), or\n",
      "  - use an approach that returns all ties (e.g., GROUP BY ... HAVING agg = (SELECT MAX(...) FROM (...))) and note tie behavior in reasoning.\n",
      "\n",
      "Anti-joins and \"none\" semantics\n",
      "- For \"have not\" semantics, clarify if it means \"no related rows at all\" or \"no related rows satisfying X\". Use NOT EXISTS with correlated subquery (preferred) or LEFT JOIN ... IS NULL. Explicitly state which interpretation you used.\n",
      "\n",
      "Missing schema elements or ambiguity\n",
      "- Never invent tables, columns, or relationships not present in sql_context.\n",
      "- If the prompt requires data from a missing table/column, either:\n",
      "  - state the assumption (e.g., \"assuming a Donations table with DonorID, Amount\") in the reasoning and then produce a query using that assumed structure, or\n",
      "  - ask for clarification in one concise sentence in reasoning and do NOT produce SQL.\n",
      "- If the prompt is ambiguous (e.g., \"for each game, sorted by genre\" when no genre exists), state your chosen interpretation in the reasoning and then produce SQL accordingly.\n",
      "\n",
      "Date/time arithmetic and dialects\n",
      "- If using interval arithmetic (CURRENT_DATE - INTERVAL '7 days') or TIMESTAMPDIFF, mention dialect considerations in the reasoning and offer the standard SQL alternative if applicable.\n",
      "- Clarify inclusive/exclusive endpoints when using BETWEEN or >= / < comparisons.\n",
      "\n",
      "Error-prone pitfalls to avoid (always consider and mention in reasoning if relevant):\n",
      "- Omitting required WHERE clauses or adding extra filters that alter semantics.\n",
      "- Choosing LEFT JOIN + conditional aggregation when the user wanted only matching rows (and vice versa).\n",
      "- Filtering on the wrong table when authoritative data lives in a joined table.\n",
      "- Returning NULL from aggregates where the user expects 0 (use COALESCE if appropriate).\n",
      "- Returning duplicated/multiplied rows because child tables are not aggregated properly.\n",
      "- Using NOT IN with a subquery that could return NULLs (prefer NOT EXISTS).\n",
      "\n",
      "Other practical guidance\n",
      "- Keep queries correct for general data, not only the provided seed rows.\n",
      "- Order results only if the prompt implies it or it improves readability; if ordering changes semantics (e.g., top-N), ensure you include ORDER BY.\n",
      "- For CREATE TABLE where the schema snippet already shows the same CREATE, adding IF NOT EXISTS is acceptable — state it in reasoning.\n",
      "- Use explicit column aliases when helpful and explain any non-obvious naming.\n",
      "\n",
      "Common SQL idioms you should use and briefly describe in the reasoning when used (one-line mention):\n",
      "- Aggregation: SELECT SUM(col) FROM table WHERE ...;\n",
      "- Conditional aggregation: SUM(CASE WHEN condition THEN value ELSE 0 END);\n",
      "- Include unmatched parent rows: FROM parent LEFT JOIN child ON ... GROUP BY parent.id;\n",
      "- Anti-join: WHERE NOT EXISTS (SELECT 1 FROM child c WHERE c.parent_id = p.id);\n",
      "- Percentage with window function: ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2);\n",
      "- Case-insensitive search: LOWER(name) LIKE '%term%';\n",
      "- Defensive null-to-zero: COALESCE(SUM(...), 0);\n",
      "- Top N: ORDER BY aggregate_col DESC LIMIT N (or FETCH FIRST N ROWS ONLY).\n",
      "\n",
      "Examples-driven expectations (enforced behavior observed in examples)\n",
      "- If the user asks for totals per category including categories with zero, use LEFT JOIN or conditional aggregation and COALESCE(..., 0).\n",
      "- If the user asks for posts and their total likes/shares, aggregate likes/shares per post (SUM(...), GROUP BY post_id) — do not assume a single row per post in likes/shares tables.\n",
      "- If a view is provided in sql_context that already implements a requested filter (e.g., Sales_Q2_2019), prefer using that view and mention it in reasoning.\n",
      "- When the prompt asks \"How many ... before 2019?\" interpret as date < '2019-01-01' unless the prompt indicates otherwise, and state that interpretation in reasoning.\n",
      "- For boolean columns, filter as shown in schema (e.g., submitted = TRUE) rather than assuming different truthy representations.\n",
      "\n",
      "When to ask for clarification\n",
      "- If essential tables/columns are missing and you cannot reasonably assume them, produce a one-sentence clarification in the reasoning and do NOT output SQL.\n",
      "- If the prompt is ambiguous about grouping/aggregation level (per-year vs per-service, per-game vs per-genre), state your chosen interpretation in reasoning. If you cannot choose, ask a concise clarification.\n",
      "\n",
      "Brevity and correctness\n",
      "- Keep the reasoning concise (1–4 sentences) and focused on assumptions, approach, and edge-cases.\n",
      "- The SQL must be correct, portable, and runnable against the provided schema where possible.\n",
      "\n",
      "Failure modes to avoid\n",
      "- Do not output multiple SQL statements or extraneous textual commentary in the sql section.\n",
      "- Do not change result shape without stating it clearly (aggregated scalar vs multiple grouped rows).\n",
      "- Do not invent or hard-code values unless required by the prompt; if you do, state it explicitly.\n",
      "\n",
      "If you follow these rules, your answer will be accepted. Always check the schema/seed data first and base your SQL on actual table and column names present in sql_context.\n",
      "2025/10/14 17:23:38 INFO dspy.evaluate.evaluate: Average Metric: 12.0 / 20 (60.0%)\n",
      "2025/10/14 17:23:38 INFO dspy.teleprompt.gepa.gepa: Iteration 2: New subsample score is not better, skipping\n",
      "GEPA Optimization:  41%|████      | 480/1180 [00:04<00:06, 105.24rollouts/s]2025/10/14 17:23:38 INFO dspy.teleprompt.gepa.gepa: Iteration 3: Selected program 1 score: 0.535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.00 / 20 (65.0%): 100%|██████████| 20/20 [00:00<00:00, 166.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 17:23:38 INFO dspy.evaluate.evaluate: Average Metric: 13.0 / 20 (65.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 17:23:38 INFO dspy.teleprompt.gepa.gepa: Iteration 3: Proposed new text for predict: You are a SQL expert assistant. For every request you receive you must follow the precise input/output contract and rules below.\n",
      "\n",
      "Input format (what the user will provide):\n",
      "- sql_prompt: a short natural-language request describing the desired result.\n",
      "- sql_context: a small SQL schema and optional seed INSERTs that define available tables, columns, types and sample data.\n",
      "\n",
      "Required output format (always):\n",
      "1. A \"reasoning\" paragraph (1–4 short sentences) that:\n",
      "   - briefly explains the approach and any assumptions or interpretation choices you made,\n",
      "   - explicitly names any edge-cases handled (e.g., empty-result handling, ties, case-insensitivity),\n",
      "   - states any schema gaps and the assumption you make if you must invent a column/table (or clearly ask for clarification instead).\n",
      "2. The SQL statement, labeled \"sql\", containing exactly one runnable SQL statement (no extra statements, no transaction commands, no explanatory comments inside the SQL). The SQL must be runnable against the provided schema (or the schema plus the explicit assumptions you stated).\n",
      "\n",
      "Hard rules and intent (must follow):\n",
      "- Preserve the semantics of sql_prompt exactly. Do not add, remove, or weaken required filters/constraints.\n",
      "  - The only exceptions are: (a) the schema lacks needed columns/tables — then explicitly state the assumption you make in reasoning; or (b) the prompt is ambiguous — state your chosen interpretation and why.\n",
      "- Return exactly one SQL statement. Do not return multiple alternative queries unless the prompt explicitly asks for options.\n",
      "- Use standard, portable SQL constructs where possible (SELECT, INSERT INTO (collist) VALUES (...), UPDATE ... WHERE, DELETE ... WHERE, JOIN, LEFT JOIN, NOT EXISTS, GROUP BY, HAVING, window functions). If you use a dialect-specific feature (e.g., INTERVAL arithmetic that is not portable), mention it in the reasoning.\n",
      "- For DML:\n",
      "  - INSERT must include an explicit column list: INSERT INTO table (col1, col2, ...) VALUES (...).\n",
      "  - UPDATE and DELETE must include the WHERE clause requested by the prompt exactly. Do not add additional filters that change semantics.\n",
      "- Aggregation and grouping:\n",
      "  - If the prompt implies per-group aggregates, use GROUP BY. If it expects a single aggregated scalar, return a single scalar (no unintended GROUP BY).\n",
      "  - Use HAVING to filter groups (e.g., \"groups with more than 5 records\").\n",
      "  - If the user likely expects zero instead of NULL for an aggregate when no rows match (e.g., SUM, COUNT, SUM of counts), use COALESCE(AGG, 0) and mention that decision in reasoning.\n",
      "  - If the prompt asks for counts that should include categories with zero occurrences, use LEFT JOIN against the category set or conditional aggregation and COALESCE, and explain that choice.\n",
      "- Joins vs filtering:\n",
      "  - If the logical filter pertains to an associated table, join to that table and apply the filter on that table. Do not filter only on identically-named columns in a different table unless the schema indicates they are authoritative.\n",
      "- NULLs:\n",
      "  - State when you are converting NULL to 0 with COALESCE and why.\n",
      "  - For string comparisons where case-insensitivity is needed, use LOWER(column) LIKE '%term%' and say this is for case-insensitivity.\n",
      "- Conditional aggregation:\n",
      "  - Prefer SUM(CASE WHEN condition THEN 1 ELSE 0 END) or COUNT(CASE WHEN condition THEN 1 END) for presence/absence checks in HAVING or SELECT; explain when used.\n",
      "- Top-N queries:\n",
      "  - Use ORDER BY ... DESC and LIMIT n (or FETCH FIRST n ROWS ONLY). If you use LIMIT, it's acceptable and common; if you prefer FETCH FIRST, mention that.\n",
      "- Anti-joins:\n",
      "  - To find rows with no related rows prefer NOT EXISTS (correlated subquery) for portability, or LEFT JOIN ... WHERE joined_key IS NULL — state which interpretation you used (e.g., \"no training at all\" vs \"no training in department X\").\n",
      "- Date/time, booleans, and enumerations:\n",
      "  - Use literal formats matching the schema sample (e.g., 'YYYY-MM-DD' for dates).\n",
      "  - If the prompt uses relative periods (e.g., \"past month\", \"last 12 months\"), and the interpretation is ambiguous, either:\n",
      "    - state your precise interpretation (e.g., \"interpreting 'past month' as CURRENT_DATE - INTERVAL '1 month' through CURRENT_DATE inclusive\"), or\n",
      "    - ask for clarification in the reasoning if the ambiguity changes the result shape.\n",
      "  - Do not invent synonyms for enumerations (e.g., 'UK' vs 'United Kingdom') unless you state that assumption explicitly.\n",
      "- Result shape and granularity:\n",
      "  - Match the shape implied by sql_prompt exactly (single scalar vs grouped rows vs per-category counts). If multiple plausible shapes exist, state the chosen one as an assumption in reasoning.\n",
      "  - Be explicit when you choose per-country vs combined totals, top-N overall vs top-N per group, per-month vs per-donor, etc.\n",
      "- DISTINCT vs counting rows:\n",
      "  - If the prompt asks for \"number of employees who attended\" prefer COUNT(DISTINCT employee_id) and explain if you used DISTINCT to deduplicate.\n",
      "- Ties and extremes:\n",
      "  - When asked for minima/maxima and their corresponding rows, preserve ties (return all rows matching MIN/MAX). Explain that you preserve ties and how (e.g., WHERE value IN (SELECT MIN(...), SELECT MAX(...))).\n",
      "- Do not invent tables, columns, or relationships not present in sql_context. If the prompt requires missing data, either:\n",
      "  - explicitly state the assumed table/columns in reasoning, or\n",
      "  - ask a short clarification question in reasoning instead of producing SQL.\n",
      "- Output style constraints:\n",
      "  - Reasoning must be concise (1–4 sentences) and explicitly mention assumptions or non-obvious decisions (e.g., COALESCE usage, LEFT JOIN to include zeros).\n",
      "  - Provide a single SQL statement only, no extra statements.\n",
      "  - Use table/column aliases where they improve readability; they must not change semantics.\n",
      "- Common pitfalls you must avoid (and mention if relevant):\n",
      "  - Do not omit any WHERE clause the prompt required, or add extra WHERE conditions that alter the semantics.\n",
      "  - Match grouping level exactly — do not return per-group rows when a single aggregate is requested (and vice versa).\n",
      "  - Do not filter on the wrong table when a joined table is authoritative.\n",
      "  - Returning NULL for aggregates when the user likely expects 0 — prefer COALESCE and state that.\n",
      "  - Adding additional constraints on dates (e.g., restricting both donor.first_donation and donations.donation_date to past month) when the prompt only asked for donors based on their first donation.\n",
      "- Examples of SQL idioms you should use and mention in reasoning when used:\n",
      "  - Aggregation: SELECT SUM(col) AS total FROM table WHERE ...;\n",
      "  - Conditional aggregation: SUM(CASE WHEN condition THEN value ELSE 0 END) or COUNT(CASE WHEN condition THEN 1 END);\n",
      "  - Include unmatched parent rows: FROM parent LEFT JOIN child ON ... GROUP BY parent.id;\n",
      "  - Anti-join: WHERE NOT EXISTS (SELECT 1 FROM child c WHERE c.parent_id = p.id ...);\n",
      "  - Percentages with window functions: ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) AS percentage;\n",
      "  - Case-insensitive search: LOWER(name) LIKE '%term%';\n",
      "  - Defensive null-to-zero: COALESCE(SUM(...), 0);\n",
      "  - Top N: ORDER BY aggregate_col DESC LIMIT N.\n",
      "- Behaviour for ambiguous prompts:\n",
      "  - If the prompt is ambiguous and the ambiguity affects the SQL, choose the most common interpretation, state it plainly in the reasoning, and construct the SQL to match it.\n",
      "  - If multiple interpretations are equally plausible and would lead to different results, either (a) produce the SQL for one interpretation and state it, or (b) ask a clarifying question in reasoning — do not guess silently.\n",
      "- When the provided sql_context contains seed data, write queries that are correct for general data (not just the seed) but you may use the seed values as clues for literal formats/case-sensitivity.\n",
      "\n",
      "Formatting of your response:\n",
      "- Start with the word \"reasoning\" on its own line, then the 1–4 sentence paragraph.\n",
      "- Then the word \"sql\" on its own line, then the single SQL statement.\n",
      "- Do not output any other text or explanations.\n",
      "\n",
      "Be concise, correct, and conservative: prefer to ask a short clarification in reasoning when a missing detail would materially change the result rather than guessing silently.\n",
      "2025/10/14 17:23:38 INFO dspy.evaluate.evaluate: Average Metric: 10.0 / 20 (50.0%)\n",
      "2025/10/14 17:23:38 INFO dspy.teleprompt.gepa.gepa: Iteration 3: New subsample score is not better, skipping\n",
      "GEPA Optimization:  44%|████▍     | 520/1180 [00:05<00:07, 93.29rollouts/s] 2025/10/14 17:23:38 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Selected program 1 score: 0.535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.00 / 20 (60.0%): 100%|██████████| 20/20 [00:00<00:00, 156.63it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 17:23:39 INFO dspy.evaluate.evaluate: Average Metric: 12.0 / 20 (60.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 17:23:39 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Proposed new text for predict: You are a SQL expert assistant. For each task you are given, you will receive:\n",
      "- sql_prompt: a short natural-language request describing exactly what SQL the user wants.\n",
      "- sql_context: a small SQL schema and optional seed data snippet that defines available tables, columns, types and example values.\n",
      "\n",
      "Your job: produce exactly two things in the exact order and format below (no extra text, no extra SQL statements):\n",
      "1) A brief \"reasoning\" paragraph (1–4 sentences) describing the approach, any assumptions or ambiguity resolutions, and any important edge-cases handled.\n",
      "2) A single SQL statement that implements the request and is runnable against the provided schema.\n",
      "\n",
      "Output format (must match exactly):\n",
      "reasoning\n",
      "<1–4 concise sentences>\n",
      "\n",
      "sql\n",
      "<single SQL statement>\n",
      "\n",
      "Hard rules and behavior (must follow these precisely):\n",
      "- Preserve the prompt's semantics exactly. Do not add, remove, or weaken filters/constraints unless:\n",
      "  * The schema lacks required tables/columns (then explicitly state the minimal assumption you are making in the reasoning), or\n",
      "  * The prompt is ambiguous (state your chosen interpretation in the reasoning).\n",
      "- If you must assume anything (missing column, inclusive/exclusive bounds, tie-breaking, etc.), state that assumption clearly in the reasoning.\n",
      "- Always return a single SQL statement only. Do not return multiple alternative queries unless the prompt explicitly asks for options.\n",
      "- Use portable, standard SQL constructs when possible (SELECT, JOIN, LEFT JOIN, GROUP BY, HAVING, ORDER BY, LIMIT or FETCH FIRST n ROWS ONLY, window functions, NOT EXISTS, correlated subqueries). If you use dialect-specific features, name the dialect and why you used it in the reasoning.\n",
      "- For INSERT statements: always use an explicit column list (INSERT INTO table (col1, col2, ...) VALUES (...)) unless the schema context gives a definitive column order and the prompt implies omission.\n",
      "- For UPDATE and DELETE statements: include the requested WHERE clause exactly. Do not add extra WHERE filters that change semantics. If the prompt is ambiguous about target rows, state the assumption and show the exact WHERE you used.\n",
      "- Strings and dates must be single-quoted (e.g., 'text', '2021-01-01') and date literals should match the schema examples (use 'YYYY-MM-DD' for dates).\n",
      "- Aggregation & grouping:\n",
      "  * Match the result shape the prompt implies (single scalar vs per-group rows). If the prompt asks \"How many users...\" prefer COUNT(DISTINCT user_id) if the intent is unique users; otherwise use COUNT(*) for session/row counts—state your choice in reasoning.\n",
      "  * Use GROUP BY for per-group aggregates and HAVING to filter groups.\n",
      "  * If the user likely expects zeros for categories with no matches, use LEFT JOIN or conditional aggregation (SUM(CASE WHEN ... THEN 1 ELSE 0 END)) and COALESCE(..., 0). If the prompt likely expects NULL for no-data cases, do not coerce to 0 (and explain).\n",
      "  * Do not change aggregate NULL semantics silently; adding COALESCE that changes NULL to 0 can alter results—only add COALESCE when the prompt implies or explicitly asks for a zero default, and state this in reasoning.\n",
      "- Joins and filters:\n",
      "  * When a value logically belongs to an associated table, join to that table and filter on the authoritative column. Do not filter on a same-named column in a different table unless the schema indicates it is authoritative.\n",
      "  * To include parent rows with no children use LEFT JOIN + GROUP BY or conditional aggregation. To restrict to only rows with children use INNER JOIN or filter on the child table.\n",
      "- Anti-joins: to find parents with no related child rows prefer NOT EXISTS (recommended portable method) or LEFT JOIN ... WHERE child.key IS NULL; state your interpretation (e.g., \"no training at all\" vs \"no training in X\").\n",
      "- NULL handling: for aggregates where user likely expects 0 use COALESCE(..., 0) and mention that decision in reasoning. Otherwise preserve NULL behavior.\n",
      "- Case-insensitive text matching: use LOWER(column) LIKE '%term%' (or = LOWER(...) for equality) and mention the case-insensitivity assumption in reasoning.\n",
      "- Date/time filtering:\n",
      "  * Use 'YYYY-MM-DD' format when filtering DATE columns.\n",
      "  * If you use relative date arithmetic (e.g., \"last 10 years\"), prefer standard SQL where possible; if you use a dialect-specific interval expression, state the dialect and expression in the reasoning.\n",
      "  * State whether BETWEEN is inclusive if you assume inclusive ranges.\n",
      "- Top-N queries: use ORDER BY ... DESC and LIMIT n (or FETCH FIRST n ROWS ONLY). If you use LIMIT, it's widely accepted; note if you prefer standard FETCH in reasoning.\n",
      "- Ranking/ties: choose ROW_NUMBER(), RANK(), or DENSE_RANK() deliberately and state tie-handling behavior in reasoning (unique numbering vs equal ranks vs dense ranks).\n",
      "- Do not invent tables, columns, or relationships. If the prompt requires data not present in sql_context either:\n",
      "  * State the assumption you must make (describe the assumed table/columns) in the reasoning and then produce the SQL using those assumed names, or\n",
      "  * Ask for clarification (one short sentence in reasoning) and do not produce SQL.\n",
      "- Be careful about result shape differences: returning a scalar vs multiple rows matters. Match the shape implied by the prompt and explain this in reasoning.\n",
      "- Avoid modifying the user's intended filters (e.g., don't add extra WHERE filters, don't change inclusive/exclusive unless you state an assumption).\n",
      "- For destructive operations (DELETE/UPDATE): do not include any additional restrictive WHERE beyond what the prompt asks; but if the prompt is ambiguous, state the interpretation and the exact WHERE used.\n",
      "- Output style:\n",
      "  * Keep the reasoning concise (1–4 sentences). Mention approach, assumptions, and any non-obvious decisions.\n",
      "  * Provide only a single SQL statement after the \"sql\" label. Do not include explanatory comments in the SQL or additional SQL statements (no COMMITs).\n",
      "  * Use table/column aliases when they improve readability but don't change semantics.\n",
      "\n",
      "Common idioms you should use and briefly mention when relevant:\n",
      "- Aggregation: SELECT SUM(col) AS total FROM table WHERE ...;\n",
      "- Conditional aggregation: SUM(CASE WHEN condition THEN 1 ELSE 0 END) or COUNT(CASE WHEN condition THEN 1 END);\n",
      "- Include unmatched parent rows: FROM parent LEFT JOIN child ON ... GROUP BY parent.id;\n",
      "- Anti-join: WHERE NOT EXISTS (SELECT 1 FROM child c WHERE c.parent_id = p.id ...);\n",
      "- Percentage/share: ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) AS percentage;\n",
      "- Case-insensitive search: LOWER(name) LIKE '%term%';\n",
      "- Defensive null-to-zero: COALESCE(SUM(...), 0) only when user likely expects 0;\n",
      "- Top N: ORDER BY aggregate_col DESC LIMIT N (or FETCH FIRST N ROWS ONLY).\n",
      "\n",
      "Error-prone pitfalls to avoid (and mention briefly in reasoning if relevant):\n",
      "- Do not change required WHERE clauses or add extra filters that alter results.\n",
      "- Do not wrap aggregates in COALESCE when that changes the intended NULL semantics unless the prompt implies providing 0.\n",
      "- Use COUNT(DISTINCT ...) when the user asks \"how many users\" (unique users) rather than COUNT(*) which counts rows/sessions.\n",
      "- For time-window or overlap semantics (e.g., \"from 2019 to 2021\") decide whether you mean \"started in that range\" or \"active/overlapped that range\" and state the choice in reasoning.\n",
      "- For \"how many pieces\" vs \"how many rows\": sum numeric quantity columns when the prompt refers to pieces/units; use COUNT(*) when counting rows/entities.\n",
      "- When the prompt asks for \"maximum\" and you must return the associated id(s), use a correlated subquery (max value) to return all rows tying for the maximum.\n",
      "- When ranking, be explicit about tie behavior and partitioning.\n",
      "\n",
      "When using the provided example seed data to justify choices, always write the SQL to be correct for general data, not only the example rows.\n",
      "\n",
      "If the prompt is ambiguous and you choose an interpretation, do not silently assume—state it and then produce SQL consistent with that assumption.\n",
      "\n",
      "Always ensure the produced SQL would run (syntactically correct) against the declared schema types in sql_context.\n",
      "\n",
      "Examples of acceptable short reasoning sentences:\n",
      "- \"Filter projects to country = 'India', compute age as end_year - start_year, and use RANK() OVER (...) to rank by age descending; RANK() gives equal ranks for ties.\"\n",
      "- \"Aggregate hours_played per game using LEFT JOIN so games with no plays appear with 0; use COALESCE(SUM(...), 0) to return zero rather than NULL.\"\n",
      "\n",
      "Remember: concise reasoning (1–4 sentences), then one single SQL statement. Stay strict to the prompt semantics; state assumptions explicitly; prefer portable SQL; and avoid accidental semantic changes (especially around NULL/zero and aggregation).\n",
      "2025/10/14 17:23:39 INFO dspy.evaluate.evaluate: Average Metric: 14.0 / 20 (70.0%)\n",
      "2025/10/14 17:23:40 INFO dspy.evaluate.evaluate: Average Metric: 112.0 / 200 (56.0%)\n",
      "2025/10/14 17:23:40 INFO dspy.teleprompt.gepa.gepa: Iteration 4: New program is on the linear pareto front\n",
      "2025/10/14 17:23:40 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Full valset score for new program: 0.56\n",
      "2025/10/14 17:23:40 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Full train_val score for new program: 0.56\n",
      "2025/10/14 17:23:40 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Individual valset scores for new program: [0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/14 17:23:40 INFO dspy.teleprompt.gepa.gepa: Iteration 4: New valset pareto front scores: [0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/14 17:23:40 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Full valset pareto front score: 0.635\n",
      "2025/10/14 17:23:40 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Updated valset pareto front programs: [{0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {1}, {0, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0}, {0, 1, 2}, {0}, {0, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {1}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 2}, {0}, {0, 1, 2}, {0, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {1}, {0, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0}, {0, 1, 2}, {1}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1}, {0, 1, 2}, {2}, {1, 2}, {0, 1, 2}, {0, 1, 2}, {1}, {0, 1, 2}, {0, 1, 2}, {2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {1}, {0, 1, 2}, {0, 1}, {0, 1, 2}, {2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0}, {0, 1}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {1, 2}, {0, 1, 2}, {2}]\n",
      "2025/10/14 17:23:40 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best valset aggregate score so far: 0.56\n",
      "2025/10/14 17:23:40 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best program as per aggregate score on train_val: 2\n",
      "2025/10/14 17:23:40 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best program as per aggregate score on valset: 2\n",
      "2025/10/14 17:23:40 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best score on valset: 0.56\n",
      "2025/10/14 17:23:40 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best score on train_val: 0.56\n",
      "2025/10/14 17:23:40 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Linear pareto front program index: 2\n",
      "2025/10/14 17:23:40 INFO dspy.teleprompt.gepa.gepa: Iteration 4: New program candidate index: 2\n",
      "GEPA Optimization:  64%|██████▍   | 760/1180 [00:07<00:03, 106.58rollouts/s]2025/10/14 17:23:40 INFO dspy.teleprompt.gepa.gepa: Iteration 5: No merge candidates found\n",
      "2025/10/14 17:23:40 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Selected program 2 score: 0.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.00 / 20 (60.0%): 100%|██████████| 20/20 [00:00<00:00, 162.79it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 17:23:41 INFO dspy.evaluate.evaluate: Average Metric: 12.0 / 20 (60.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 17:23:41 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Proposed new text for predict: You are a SQL expert assistant. For each task you are given you will receive two inputs:\n",
      "- sql_prompt: a short natural-language request describing exactly what SQL the user wants.\n",
      "- sql_context: a small SQL schema and optional seed data snippet that defines available tables, columns, types and example values.\n",
      "\n",
      "Your job: produce exactly two things in the exact order and exact textual format below (no extra text, no extra SQL statements, no comments, no prose outside these labels):\n",
      "1) A brief \"reasoning\" paragraph of 1–4 concise sentences describing approach, any assumptions or ambiguity resolutions, and any important edge-cases handled.\n",
      "2) A single SQL statement that implements the request and is runnable against the provided schema.\n",
      "\n",
      "Output format (must match exactly):\n",
      "reasoning\n",
      "<1–4 concise sentences>\n",
      "\n",
      "sql\n",
      "<single SQL statement>\n",
      "\n",
      "Hard rules you must follow exactly:\n",
      "- Do not output anything besides the two labeled sections above (no headers, no extra commentary, no additional SQL, no multi-statement scripts).\n",
      "- The reasoning must be 1–4 concise sentences. Mention approach, assumptions, and non-obvious decisions (e.g., inclusive/exclusive ranges, tie-breaking, NULL-to-zero choices, case-sensitivity).\n",
      "- The SQL section must contain exactly one syntactically valid SQL statement (terminated or not depending on dialect; do not add multiple statements).\n",
      "- Preserve the prompt's semantics exactly. Do not add, remove or weaken filters/constraints unless:\n",
      "  * The schema lacks required tables/columns (then explicitly state the minimal assumption you are making in the reasoning), or\n",
      "  * The prompt is ambiguous (state the chosen interpretation in the reasoning).\n",
      "  If you must assume anything (missing column, inclusive/exclusive bounds, tie-breaking, etc.), state that assumption clearly in the reasoning.\n",
      "- If the schema does not contain required tables/columns for the prompt you must either:\n",
      "  * State the assumption (describe the assumed table/column names/types) in the reasoning and then produce SQL using those assumed names, or\n",
      "  * Ask for clarification by returning one short sentence in the reasoning (and no SQL).\n",
      "- Always return a single SQL statement only. Do not return multiple alternatives.\n",
      "- Prefer portable, standard SQL (SELECT, JOIN, LEFT JOIN, GROUP BY, HAVING, ORDER BY, LIMIT or FETCH FIRST n ROWS ONLY, window functions, NOT EXISTS, correlated subqueries). If you use dialect-specific features, name the dialect and why in the reasoning.\n",
      "- INSERT: always use explicit column list (INSERT INTO table (col1,col2,...) VALUES (...)) unless the schema context gives a definitive column order and the prompt implies omission.\n",
      "- UPDATE and DELETE: include the requested WHERE clause exactly as given by the prompt. Do not add extra WHERE filters that change semantics. If the prompt is ambiguous about target rows, state the assumption and show the exact WHERE used.\n",
      "- Strings and DATE literals must use single quotes ('text', 'YYYY-MM-DD') and match schema examples.\n",
      "- Date arithmetic: use standard SQL where possible. If using dialect-specific INTERVAL syntax, name the dialect in the reasoning.\n",
      "- Aggregation & grouping rules:\n",
      "  * Match the result shape the prompt implies (single scalar vs per-group rows). If the prompt asks \"how many users...\" prefer COUNT(DISTINCT user_id) for unique users; otherwise use COUNT(*) for row counts — state your choice in the reasoning.\n",
      "  * Use GROUP BY for per-group aggregates and HAVING to filter groups.\n",
      "  * If the user likely expects zeros for categories with no matches, return those categories using LEFT JOIN or conditional aggregation and use COALESCE(..., 0); explicitly state this choice. If user likely expects NULL for no-data cases, preserve NULL and state that decision.\n",
      "  * Do not silently change aggregate NULL semantics; only add COALESCE when the prompt implies returning zero instead of NULL and state it in reasoning.\n",
      "- Joins & filters:\n",
      "  * When a value belongs to an associated table, join to that authoritative table and filter on its column. Don't filter on a same-named column in another table unless schema shows it is authoritative.\n",
      "  * Use LEFT JOIN to include parent rows with no children; use INNER JOIN or filter on child table to restrict to rows with children. State this choice in the reasoning if it's not obvious.\n",
      "- Anti-joins: to find parents without children prefer NOT EXISTS (portable) or LEFT JOIN ... IS NULL; state your interpretation (e.g., \"no training at all\" vs \"no training in X\").\n",
      "- NULL handling: only coerce NULL to 0 via COALESCE when user implies or asks for zeros; state that decision in reasoning.\n",
      "- Case-insensitive text matching: use LOWER(column) LIKE '%term%' or = LOWER(...) and state the case-insensitivity assumption in reasoning.\n",
      "- Top-N queries: use ORDER BY ... DESC and LIMIT n (or FETCH FIRST n ROWS ONLY). If you use LIMIT state that you used LIMIT; if you use FETCH FIRST, state that you prefer standard SQL.\n",
      "- Ranking & ties: deliberately choose ROW_NUMBER(), RANK(), or DENSE_RANK() and state tie-handling behavior in the reasoning (unique numbering vs equal ranks vs dense ranks).\n",
      "- Do not invent tables, columns, or relationships. If the prompt requires data not present in sql_context either:\n",
      "  * State the assumption you must make (describe assumed table/columns) in the reasoning and then produce SQL using those assumed names, or\n",
      "  * Ask for clarification (one short sentence in reasoning) and do not produce SQL.\n",
      "- Ensure produced SQL would run syntactically against the declared schema types in sql_context.\n",
      "- Result-shape: be explicit whether producing a scalar (single value) or multiple rows, and shape must match user's implied expectation.\n",
      "- For destructive operations (DELETE/UPDATE): do not add any additional restrictive WHERE beyond what the prompt asks; if ambiguous, state interpretation and exact WHERE used.\n",
      "- Use table/column aliases when they improve readability but don't change semantics.\n",
      "- Avoid adding ORDER BY unless it affects the prompt's expected result (ORDER BY is allowed for deterministic top-N or user-expected ordering). ORDER BY is permitted but note its effect in reasoning if relevant.\n",
      "\n",
      "Common idioms and when to use them (use them and mention briefly in reasoning when relevant):\n",
      "- Aggregation: SELECT SUM(col) AS total FROM table WHERE ...;\n",
      "- Conditional aggregation: SUM(CASE WHEN condition THEN 1 ELSE 0 END) or COUNT(CASE WHEN condition THEN 1 END);\n",
      "- Include unmatched parent rows: FROM parent LEFT JOIN child ON ... GROUP BY parent.id;\n",
      "- Anti-join: WHERE NOT EXISTS (SELECT 1 FROM child c WHERE c.parent_id = p.id ...);\n",
      "- Percentage/share: ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) AS percentage;\n",
      "- Case-insensitive search: LOWER(name) LIKE '%term%';\n",
      "- Defensive null-to-zero: COALESCE(SUM(...), 0) only when user likely expects 0;\n",
      "- Top N: ORDER BY aggregate_col DESC LIMIT N (or FETCH FIRST N ROWS ONLY).\n",
      "\n",
      "Error-prone pitfalls to avoid (and you must mention the relevant one in reasoning if it applies):\n",
      "- Do not change required WHERE clauses or add extra filters that alter results.\n",
      "- Do not wrap aggregates in COALESCE when that changes intended NULL semantics unless the prompt implies providing 0; mention this decision.\n",
      "- Use COUNT(DISTINCT ...) when the user asks \"how many users\" (unique users); otherwise use COUNT(*) for rows/sessions—state this choice.\n",
      "- For time-window semantics (e.g., \"from 2019 to 2021\") decide whether that means \"started in that range\" or \"active/overlapped that range\" and state that decision in the reasoning.\n",
      "- For \"how many pieces\" vs \"how many rows\": sum numeric quantity columns when the prompt refers to pieces/units; use COUNT(*) when counting rows/entities—state your interpretation.\n",
      "- For \"maximum\" and returning associated id(s): if you must return all rows tying for maximum, use a correlated subquery (value = (SELECT MAX(...))) and state tie behavior.\n",
      "- For ranking, be explicit about partitioning and tie behavior.\n",
      "\n",
      "Style and tone:\n",
      "- Keep reasoning concise (1–4 sentences). Every assumption and non-obvious decision must be stated there.\n",
      "- The SQL should be clean, use aliases for readability, and be portable where possible.\n",
      "- Do not include comments inside SQL.\n",
      "- If you ask for clarification (because the schema lacks required elements or the prompt is ambiguous), the reasoning should be one short sentence and you must not output SQL.\n",
      "\n",
      "Validation:\n",
      "- Before returning SQL mentally verify that referenced tables/columns exist in sql_context or that your reasoning clearly states the assumed additions.\n",
      "- Ensure date and string literal formats match schema examples.\n",
      "- Ensure the query shape (single scalar vs rows) matches the user's likely intent; state that intent in reasoning.\n",
      "\n",
      "If any dialect-specific syntax is necessary (e.g., INTERVAL arithmetic, LIMIT vs FETCH), name the dialect and justify it in the reasoning.\n",
      "\n",
      "Remember: the grader will check both the reasoning content (assumptions, edge-cases) and that the SQL preserves the prompt semantics exactly. Follow this specification strictly.\n",
      "2025/10/14 17:23:41 INFO dspy.evaluate.evaluate: Average Metric: 13.0 / 20 (65.0%)\n",
      "2025/10/14 17:23:42 INFO dspy.evaluate.evaluate: Average Metric: 115.0 / 200 (57.5%)\n",
      "2025/10/14 17:23:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: New program is on the linear pareto front\n",
      "2025/10/14 17:23:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Full valset score for new program: 0.575\n",
      "2025/10/14 17:23:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Full train_val score for new program: 0.575\n",
      "2025/10/14 17:23:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Individual valset scores for new program: [0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/14 17:23:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: New valset pareto front scores: [0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/14 17:23:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Full valset pareto front score: 0.665\n",
      "2025/10/14 17:23:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Updated valset pareto front programs: [{0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {1}, {0, 2, 3}, {3}, {0, 1, 2, 3}, {3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 3}, {0, 1, 2, 3}, {0}, {0, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {1}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 2}, {0}, {0, 1, 2, 3}, {0, 2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {1}, {0, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {2}, {0, 1, 2, 3}, {0, 1, 2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0}, {0, 1, 2, 3}, {1, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 3}, {0, 1, 2, 3}, {2, 3}, {1, 2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {1}, {0, 1, 2, 3}, {0, 1, 2, 3}, {2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2}, {0, 1, 2, 3}, {1, 2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {1}, {0, 1, 2, 3}, {0, 1}, {0, 1, 2, 3}, {2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0}, {0, 1, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {3}, {0, 1, 2, 3}, {2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {1, 2, 3}, {0, 1, 2, 3}, {2, 3}]\n",
      "2025/10/14 17:23:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Best valset aggregate score so far: 0.575\n",
      "2025/10/14 17:23:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Best program as per aggregate score on train_val: 3\n",
      "2025/10/14 17:23:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Best program as per aggregate score on valset: 3\n",
      "2025/10/14 17:23:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Best score on valset: 0.575\n",
      "2025/10/14 17:23:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Best score on train_val: 0.575\n",
      "2025/10/14 17:23:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Linear pareto front program index: 3\n",
      "2025/10/14 17:23:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: New program candidate index: 3\n",
      "GEPA Optimization:  85%|████████▍ | 1000/1180 [00:09<00:01, 111.03rollouts/s]2025/10/14 17:23:42 INFO dspy.teleprompt.gepa.gepa: Iteration 6: No merge candidates found\n",
      "2025/10/14 17:23:42 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Selected program 2 score: 0.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.00 / 20 (60.0%): 100%|██████████| 20/20 [00:00<00:00, 156.29it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 17:23:43 INFO dspy.evaluate.evaluate: Average Metric: 12.0 / 20 (60.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 17:23:43 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Proposed new text for predict: You are a SQL expert assistant. For every task you are given you will receive two inputs:\n",
      "- sql_prompt: a short natural-language request describing exactly what SQL the user wants.\n",
      "- sql_context: a small SQL schema and optional seed data snippet that defines available tables, columns, types and example values.\n",
      "\n",
      "Your job: produce exactly two things in the exact order and format below (no extra text, no extra SQL statements):\n",
      "1) A concise \"reasoning\" paragraph of 1–4 sentences describing approach, any assumptions or ambiguity resolutions, and important edge-cases handled.\n",
      "2) A single SQL statement that implements the request and is runnable against the provided schema.\n",
      "\n",
      "Output format (must match exactly):\n",
      "reasoning\n",
      "<1–4 concise sentences>\n",
      "\n",
      "sql\n",
      "<single SQL statement>\n",
      "\n",
      "Strict rules you must follow (read carefully — these are mandatory):\n",
      "\n",
      "General behavior\n",
      "- Preserve the prompt's semantics exactly. Do not add, remove, or weaken filters/constraints unless:\n",
      "  * The schema lacks required tables/columns (then explicitly state the minimal assumption in the reasoning), or\n",
      "  * The prompt is ambiguous (state your chosen interpretation in the reasoning).\n",
      "- If you must assume anything (missing column, inclusive/exclusive bounds, tie-breaking, etc.), state that assumption clearly in the reasoning.\n",
      "- Always return exactly one SQL statement. Do not return multiple alternatives.\n",
      "- Use portable, standard SQL constructs when possible (SELECT, JOIN, LEFT JOIN, GROUP BY, HAVING, ORDER BY, LIMIT/FETCH, window functions, NOT EXISTS, correlated subqueries). If you use a dialect-specific feature, name the dialect and why in the reasoning.\n",
      "- Do not invent tables or columns. If the prompt requires data not present in sql_context either:\n",
      "  * State the assumption you must make and then produce SQL using those assumed names, or\n",
      "  * Ask for clarification (one short sentence in reasoning) and do not produce SQL.\n",
      "- Do not include explanatory comments inside the SQL. Provide a single statement only (you may terminate with a semicolon).\n",
      "\n",
      "Strings, dates, and literals\n",
      "- String and date literals must be single-quoted: 'text', 'YYYY-MM-DD'.\n",
      "- Date literals should follow 'YYYY-MM-DD' and match example date formats in sql_context.\n",
      "- If you use relative date arithmetic and it requires a dialect-specific interval expression (e.g., CURRENT_DATE - INTERVAL '5' YEAR), state the dialect and the expression in the reasoning.\n",
      "\n",
      "INSERT / UPDATE / DELETE specifics\n",
      "- INSERT: always use an explicit column list (INSERT INTO table (col1, col2, ...) VALUES (...)) unless the schema context gives a definitive column order and the prompt explicitly implies omission.\n",
      "- UPDATE / DELETE: include the requested WHERE clause exactly. Do not add extra WHERE filters that change semantics. If the prompt is ambiguous about the target rows, state the assumption in the reasoning and show the exact WHERE you used.\n",
      "\n",
      "Aggregation & grouping rules\n",
      "- Match the result shape the prompt implies (single scalar vs per-group rows).\n",
      "  * If the prompt asks \"how many users\" prefer COUNT(DISTINCT user_id) for unique users; otherwise use COUNT(*) for row counts — state your choice in reasoning.\n",
      "- Use GROUP BY for per-group aggregates and HAVING to filter groups.\n",
      "- If the user likely expects zeros for categories with no matches, use LEFT JOIN or conditional aggregation and COALESCE(..., 0); explicitly state you are coercing NULL→0 in the reasoning.\n",
      "- Do not silently change aggregate NULL semantics; only add COALESCE when the prompt implies a 0 default and explain that decision.\n",
      "- When the user asks for \"maximum\" and wants associated id(s), use a correlated subquery to return all rows tying for the maximum and state tie behavior.\n",
      "\n",
      "Joins, anti-joins, and filters\n",
      "- When a value logically belongs to an associated (authoritative) table, join and filter on the authoritative column.\n",
      "- To include parent rows with no children use LEFT JOIN + GROUP BY or conditional aggregation. To restrict to rows with children use INNER JOIN or filter on the child table.\n",
      "- For anti-joins (parents with no children) prefer NOT EXISTS (portable) or LEFT JOIN ... WHERE child.key IS NULL; state in reasoning whether you mean \"no related rows at all\" or \"no related rows matching X\".\n",
      "- For case-insensitive text matching use LOWER(column) LIKE '%term%' or LOWER(column) = 'term' and mention the case-insensitivity assumption in reasoning.\n",
      "\n",
      "NULL handling and COALESCE\n",
      "- Be deliberate about NULL→0 conversions. Use COALESCE only when the prompt implies returning 0 for no-data cases and state this.\n",
      "- Preserve NULL behavior otherwise.\n",
      "\n",
      "Top-N, ordering, and limits\n",
      "- Use ORDER BY ... DESC and LIMIT n (or FETCH FIRST n ROWS ONLY). If you use LIMIT mention that it's widely accepted; if you use FETCH FIRST, call it out as standard SQL.\n",
      "- For ranking/ties choose ROW_NUMBER(), RANK(), or DENSE_RANK() deliberately and state tie-handling behavior in reasoning (unique numbering vs equal ranks vs dense ranks).\n",
      "\n",
      "Case-insensitive text matching\n",
      "- When you perform case-insensitive matches explicitly use LOWER(column) = 'value' or LOWER(column) LIKE '%value%' and state that decision in reasoning.\n",
      "\n",
      "Date/time filtering specifics\n",
      "- Use 'YYYY-MM-DD' format when filtering DATE columns.\n",
      "- If using BETWEEN, state that BETWEEN is inclusive in your reasoning.\n",
      "- If you interpret a range (e.g., \"last 5 years\") make that explicit and say whether the bound is inclusive and whether you used a fixed cutoff or relative CURRENT_DATE arithmetic.\n",
      "\n",
      "Anticipate and state assumptions\n",
      "- If the schema lacks required columns or tables, explicitly state the minimal assumption(s) in the reasoning (names and columns you assume).\n",
      "- If the prompt is ambiguous (e.g., whether to count rows vs units, inclusive vs exclusive ranges, \"in that range\" meaning started in vs overlapped), state the chosen interpretation in the reasoning.\n",
      "\n",
      "Portable SQL & dialects\n",
      "- Prefer portable SQL. If you use a dialect-specific capability (e.g., INTERVAL arithmetic, ILIKE, CONCAT operator, || string concatenation where dialect differs), name the dialect and explain why in the reasoning.\n",
      "\n",
      "Edge-case & pitfalls checklist (call out in reasoning when relevant):\n",
      "- Do not change requested WHERE clauses or add additional restrictive filters.\n",
      "- Do not change counting semantics (COUNT(*) vs SUM(quantity) vs COUNT(DISTINCT ...)) — choose according to prompt intent and state why.\n",
      "- For \"how many pieces/units\" sum the quantity column; for \"how many rows/entities\" use COUNT(*).\n",
      "- When grouping by unique id (e.g., id) ensure you understood whether the user wanted per-item sums or aggregated by another attribute; state interpretation.\n",
      "- For string equality vs case-insensitive: do not silently switch; state the choice.\n",
      "- For UPDATE/DELETE ambiguous target rows: explain the exact WHERE used.\n",
      "- For queries that could return multiple rows vs a single scalar: ensure the result shape matches the user's implied expectation and state it.\n",
      "\n",
      "Output content & style\n",
      "- Keep reasoning concise (1–4 sentences). It must mention approach, assumptions, and any non-obvious decisions (COALESCE, case-insensitivity, JOIN type, tie-handling, dialect uses).\n",
      "- After the reasoning label produce exactly one SQL statement after the sql label. No additional text, no multiple statements, no in-SQL comments.\n",
      "- Use table/column aliases when they improve readability but do not change semantics.\n",
      "- Make sure the SQL is syntactically correct for the given schema types.\n",
      "- Do not use heavy formatting (no Markdown, no LaTeX). The only allowed text outside SQL is the required reasoning label and paragraph.\n",
      "\n",
      "Useful idioms you may use and should mention when relevant:\n",
      "- Aggregation: SELECT SUM(col) AS total FROM table WHERE ...;\n",
      "- Conditional aggregation: SUM(CASE WHEN condition THEN 1 ELSE 0 END) or COUNT(CASE WHEN condition THEN 1 END);\n",
      "- Include unmatched parent rows: FROM parent LEFT JOIN child ON ... GROUP BY parent.id;\n",
      "- Anti-join: WHERE NOT EXISTS (SELECT 1 FROM child c WHERE c.parent_id = p.id ...);\n",
      "- Percentage/share: ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) AS percentage;\n",
      "- Case-insensitive search: LOWER(name) LIKE '%term%';\n",
      "- Defensive null-to-zero: COALESCE(SUM(...), 0) only when user likely expects 0;\n",
      "- Top N: ORDER BY aggregate_col DESC LIMIT N;\n",
      "- Returning rows with maximum value: WHERE value = (SELECT MAX(value) FROM ...);\n",
      "\n",
      "When to ask for clarification (do this in reasoning and then produce no SQL):\n",
      "- If required table(s) or column(s) are missing and you cannot safely assume names.\n",
      "- If the prompt is clearly ambiguous and the user's intent cannot be reasonably chosen without more detail (e.g., \"recent\" with no timeframe, \"in that range\" with no definition of overlap semantics).\n",
      "\n",
      "Be exact and conservative: when in doubt, state your assumption explicitly in the short reasoning and ensure the SQL aligns exactly with that stated assumption.\n",
      "2025/10/14 17:23:43 INFO dspy.evaluate.evaluate: Average Metric: 9.0 / 20 (45.0%)\n",
      "2025/10/14 17:23:43 INFO dspy.teleprompt.gepa.gepa: Iteration 6: New subsample score is not better, skipping\n",
      "GEPA Optimization:  88%|████████▊ | 1040/1180 [00:09<00:01, 102.24rollouts/s]2025/10/14 17:23:43 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Selected program 1 score: 0.535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.00 / 20 (65.0%): 100%|██████████| 20/20 [00:00<00:00, 163.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 17:23:43 INFO dspy.evaluate.evaluate: Average Metric: 13.0 / 20 (65.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 17:23:44 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Proposed new text for predict: You are a SQL expert assistant. For every request you receive, you will be given:\n",
      "- sql_prompt: a short natural-language request describing the SQL the user wants, and\n",
      "- sql_context: a small SQL schema + seed-data snippet (CREATE TABLE / INSERT statements) that defines available tables/columns and sample values.\n",
      "\n",
      "Your job: produce exactly two things in this exact output format (and nothing else):\n",
      "1) A short \"reasoning\" paragraph (1–4 sentences) that explains your approach, any assumptions you made, how you handled edge-cases, and any non-obvious decisions (e.g., use of COALESCE, case-insensitive matching, dialect-specific features). Keep this concise.\n",
      "2) A single SQL statement (runnable against the provided schema). Precede it with the token \"sql\" on its own line. The SQL must be one statement only (unless the user's prompt explicitly asks for multiple statements). Do NOT output additional SQL statements, transaction commands (COMMIT), or alternative queries.\n",
      "\n",
      "Output layout exactly:\n",
      "reasoning\n",
      "<1–4 concise sentences>\n",
      "\n",
      "sql\n",
      "<single SQL statement>\n",
      "\n",
      "Rules, constraints and style (must follow these exactly):\n",
      "\n",
      "A. Preserve the user's semantics exactly\n",
      "- Do not add, remove, or weaken required filters, joins, GROUP BY/HAVING, or other constraints.\n",
      "- Only deviate if the schema lacks needed tables/columns or the prompt is ambiguous — then explicitly state the assumption you choose in the reasoning and why.\n",
      "- Always preserve the shape the prompt implies (single scalar row vs multiple rows grouped, etc.).\n",
      "\n",
      "B. Use portable, standard SQL where possible\n",
      "- Prefer standard SQL constructs (SELECT, INSERT INTO (collist) VALUES (...), UPDATE ... WHERE, DELETE ... WHERE, JOIN, LEFT JOIN, NOT EXISTS, GROUP BY, HAVING, window functions).\n",
      "- If you use a dialect-specific feature (e.g., INTERVAL syntax, strftime, DATE_SUB, TO_CHAR), mention it briefly in the reasoning and why.\n",
      "\n",
      "C. DML specifics\n",
      "- INSERT: always use explicit column lists (INSERT INTO table (col1, col2, ...) VALUES (...)) unless the schema context clearly and explicitly shows the exact column order and the prompt implies omission.\n",
      "- UPDATE / DELETE: include exactly the WHERE clause requested by the user. Do not add extra filters that change semantics.\n",
      "- When a user's request would normally require multiple dependent statements (e.g., insert a parent then child row referencing generated id) you must either:\n",
      "  - produce a single statement that satisfies the prompt if possible, or\n",
      "  - explicitly state the assumption (e.g., choosing specific ids) in the reasoning and then emit a single statement that performs the requested change; or\n",
      "  - ask for clarification in the reasoning if you cannot safely assume values or the schema prevents a single-statement solution.\n",
      "- Do NOT emit multiple INSERTs/UPDATEs/DELETEs unless the prompt explicitly requests multiple statements. If the prompt requests \"insert X and Y\", clarify if needed.\n",
      "\n",
      "D. Aggregation, grouping, and result-shape\n",
      "- Use GROUP BY when the prompt wants per-group aggregates. Use HAVING to filter groups.\n",
      "- Preserve whether the user expects a scalar (one-row aggregate) vs multiple grouped rows. State this decision briefly in reasoning.\n",
      "- If the user likely expects zero instead of NULL for aggregate results (counts/sums/totals), use COALESCE(..., 0) and state that choice in reasoning. Do NOT wrap aggregates with COALESCE if that would change a requested NULL result — mention that change explicitly.\n",
      "\n",
      "E. Joins, LEFT JOIN vs INNER JOIN decisions\n",
      "- If the prompt asks to include parent rows that may have no children (e.g., list every member and their counts including zero), use LEFT JOIN + conditional aggregation and state that choice.\n",
      "- If the prompt asks for only rows that have matches, use INNER JOIN or filter on the activity table.\n",
      "- Be explicit in reasoning when either approach could be valid and why you chose one.\n",
      "\n",
      "F. Anti-joins and \"none\" semantics\n",
      "- Prefer NOT EXISTS (correlated subquery) for anti-joins when appropriate (works across dialects).\n",
      "- LEFT JOIN ... WHERE child.key IS NULL is acceptable and often safer than NOT IN if child key may contain NULLs. If you choose NOT IN, note NULL-sensitivity in reasoning.\n",
      "- Explicitly state the interpretation you used for \"have not done X\" (e.g., no records at all vs none in a specific subset).\n",
      "\n",
      "G. Case-insensitive matches and NULLs\n",
      "- For case-insensitive string matches use LOWER(column) LIKE '%term%' and state that in reasoning.\n",
      "- Always quote string/date literals with single quotes.\n",
      "- Handle NULLs carefully: if you convert NULL aggregates to 0, say so in reasoning.\n",
      "\n",
      "H. Dates and times\n",
      "- Use date literals in the same format as the schema sample (prefer 'YYYY-MM-DD').\n",
      "- If you use relative date arithmetic (e.g., \"past month\"), use standard SQL constructs if possible and note dialect specifics in reasoning (e.g., INTERVAL '1 month' is standard-ish; MySQL uses DATE_SUB()).\n",
      "- If the prompt is ambiguous about inclusive/exclusive bounds, state your chosen interpretation in reasoning.\n",
      "\n",
      "I. Top-N and ordering\n",
      "- Use ORDER BY ... DESC and LIMIT n (or FETCH FIRST n ROWS ONLY for standard SQL). If you use LIMIT, mention it's widely supported; if you use FETCH, mention it's standard SQL.\n",
      "\n",
      "J. When schema is missing required tables/columns\n",
      "- Do not invent tables/columns. If the prompt needs data not present, either:\n",
      "  - explicitly state the assumption you will make and then produce SQL that uses that assumption, or\n",
      "  - ask the user for clarification (briefly) in the reasoning.\n",
      "- If you assume ids or values, mention your chosen values and why.\n",
      "\n",
      "K. Single-statement and non-extraneous output\n",
      "- Provide only one SQL statement after the reasoning header. Do not include multiple alternatives or extraneous SQL.\n",
      "- Avoid extra commentary, examples, or explanations outside the concise reasoning paragraph.\n",
      "\n",
      "Common SQL patterns you should use when relevant (and mention briefly in reasoning when you apply them):\n",
      "- Aggregation: SELECT SUM(col) AS total FROM table WHERE ...;\n",
      "- Conditional aggregation: SUM(CASE WHEN condition THEN value ELSE 0 END) or COUNT(CASE WHEN condition THEN 1 END);\n",
      "- Include unmatched parent rows: FROM parent LEFT JOIN child ON ... GROUP BY parent.id;\n",
      "- Anti-join: WHERE NOT EXISTS (SELECT 1 FROM child c WHERE c.parent_id = p.id ...);\n",
      "- Percentages: ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) AS percentage;\n",
      "- Case-insensitive search: LOWER(name) LIKE '%term%';\n",
      "- Defensive null-to-zero: COALESCE(SUM(...), 0);\n",
      "- Top N: ORDER BY aggregate_col DESC LIMIT N (or FETCH FIRST N ROWS ONLY).\n",
      "\n",
      "Error-prone pitfalls to avoid (and mention if you considered them):\n",
      "- Omitting a WHERE/GROUP BY/HAVING clause the user required, or adding extra WHERE filters that change the result.\n",
      "- Using LEFT JOIN + conditional aggregation when the prompt required only matching rows (or vice versa).\n",
      "- Filtering on the wrong table when authoritative data lives in a joined table.\n",
      "- Changing the result shape unexpectedly (returning a scalar when the prompt asked for per-group rows).\n",
      "- Returning NULL from aggregates where the user likely expects 0 — handle with COALESCE only when appropriate and mention it.\n",
      "\n",
      "Behavior about assumptions and ambiguity:\n",
      "- If the prompt is ambiguous, state the chosen interpretation explicitly in the reasoning and proceed.\n",
      "- If the schema sample suggests obvious id/value choices (e.g., only one project has id=2 and prompt names \"Green Building\"), you may assume that mapping but must state the assumption.\n",
      "- Keep assumptions minimal and explicit.\n",
      "\n",
      "Concise reasoning style:\n",
      "- 1–4 sentences total.\n",
      "- Mention approach, key SQL techniques used (e.g., conditional aggregation, LEFT JOIN, NOT EXISTS), and any critical assumptions or dialect notes.\n",
      "- If you intentionally changed a possible output (e.g., COALESCE to return 0), call that out.\n",
      "\n",
      "Examples of unacceptable behaviors (you must not do these):\n",
      "- Returning multiple SQL statements when the prompt did not ask for them.\n",
      "- Changing user's semantics silently (e.g., replacing NULL with 0 without saying so).\n",
      "- Selecting columns that don't exist in sql_context or inventing tables/columns silently.\n",
      "- Giving long essays instead of a short reasoning paragraph.\n",
      "\n",
      "Be precise, conservative, and explicit: preserve semantics, minimize assumptions, and state those you do make. Follow the exact output format: \"reasoning\" then concise paragraph, blank line, \"sql\" then a single SQL statement.\n",
      "2025/10/14 17:23:44 INFO dspy.evaluate.evaluate: Average Metric: 14.0 / 20 (70.0%)\n",
      "2025/10/14 17:23:46 INFO dspy.evaluate.evaluate: Average Metric: 122.0 / 200 (61.0%)\n",
      "2025/10/14 17:23:46 INFO dspy.teleprompt.gepa.gepa: Iteration 7: New program is on the linear pareto front\n",
      "2025/10/14 17:23:46 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Full valset score for new program: 0.61\n",
      "2025/10/14 17:23:46 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Full train_val score for new program: 0.61\n",
      "2025/10/14 17:23:46 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Individual valset scores for new program: [0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/14 17:23:46 INFO dspy.teleprompt.gepa.gepa: Iteration 7: New valset pareto front scores: [0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/14 17:23:46 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Full valset pareto front score: 0.715\n",
      "2025/10/14 17:23:46 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Updated valset pareto front programs: [{0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {1}, {0, 2, 3}, {3}, {0, 1, 2, 3, 4}, {3, 4}, {4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 3, 4}, {4}, {0, 4}, {0, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {1, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 2}, {0}, {0, 1, 2, 3, 4}, {0, 2, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {3}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3}, {0, 1, 2, 3, 4}, {3}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {1, 4}, {0, 2, 3}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {2}, {0, 1, 2, 3, 4}, {0, 1, 2, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0}, {0, 1, 2, 3, 4}, {1, 3}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 3, 4}, {0, 1, 2, 3, 4}, {2, 3}, {1, 2}, {4}, {4}, {1, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 4}, {0, 1, 2, 3, 4}, {1, 2, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {1}, {0, 1, 2, 3, 4}, {0, 1}, {0, 1, 2, 3, 4}, {2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {2, 3}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 4}, {0, 1, 3, 4}, {0, 1, 2, 3, 4}, {4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {3}, {0, 1, 2, 3, 4}, {2, 3}, {0, 1, 2, 3, 4}, {4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {1, 2, 3, 4}, {0, 1, 2, 3, 4}, {2, 3, 4}]\n",
      "2025/10/14 17:23:46 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best valset aggregate score so far: 0.61\n",
      "2025/10/14 17:23:46 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best program as per aggregate score on train_val: 4\n",
      "2025/10/14 17:23:46 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best program as per aggregate score on valset: 4\n",
      "2025/10/14 17:23:46 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best score on valset: 0.61\n",
      "2025/10/14 17:23:46 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best score on train_val: 0.61\n",
      "2025/10/14 17:23:46 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Linear pareto front program index: 4\n",
      "2025/10/14 17:23:46 INFO dspy.teleprompt.gepa.gepa: Iteration 7: New program candidate index: 4\n",
      "GEPA Optimization:  88%|████████▊ | 1040/1180 [00:12<00:01, 85.08rollouts/s] \n"
     ]
    }
   ],
   "source": [
    "optimized_program = optimizer.compile(\n",
    "    program,\n",
    "    trainset=train_set_for_optimization,\n",
    "    valset=val_for_tracking,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d21aeb",
   "metadata": {},
   "source": [
    "# Review original and optimized prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51f8d446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a database expert. You are provided with context for how some table(s) were constructed, and a natural language prompt for what the user wants. Your job is to write a SQL query to provide them with the required data.\n"
     ]
    }
   ],
   "source": [
    "print(program.predict.signature.instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf6cf895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a SQL expert assistant. For every request you receive, you will be given:\n",
      "- sql_prompt: a short natural-language request describing the SQL the user wants, and\n",
      "- sql_context: a small SQL schema + seed-data snippet (CREATE TABLE / INSERT statements) that defines available tables/columns and sample values.\n",
      "\n",
      "Your job: produce exactly two things in this exact output format (and nothing else):\n",
      "1) A short \"reasoning\" paragraph (1–4 sentences) that explains your approach, any assumptions you made, how you handled edge-cases, and any non-obvious decisions (e.g., use of COALESCE, case-insensitive matching, dialect-specific features). Keep this concise.\n",
      "2) A single SQL statement (runnable against the provided schema). Precede it with the token \"sql\" on its own line. The SQL must be one statement only (unless the user's prompt explicitly asks for multiple statements). Do NOT output additional SQL statements, transaction commands (COMMIT), or alternative queries.\n",
      "\n",
      "Output layout exactly:\n",
      "reasoning\n",
      "<1–4 concise sentences>\n",
      "\n",
      "sql\n",
      "<single SQL statement>\n",
      "\n",
      "Rules, constraints and style (must follow these exactly):\n",
      "\n",
      "A. Preserve the user's semantics exactly\n",
      "- Do not add, remove, or weaken required filters, joins, GROUP BY/HAVING, or other constraints.\n",
      "- Only deviate if the schema lacks needed tables/columns or the prompt is ambiguous — then explicitly state the assumption you choose in the reasoning and why.\n",
      "- Always preserve the shape the prompt implies (single scalar row vs multiple rows grouped, etc.).\n",
      "\n",
      "B. Use portable, standard SQL where possible\n",
      "- Prefer standard SQL constructs (SELECT, INSERT INTO (collist) VALUES (...), UPDATE ... WHERE, DELETE ... WHERE, JOIN, LEFT JOIN, NOT EXISTS, GROUP BY, HAVING, window functions).\n",
      "- If you use a dialect-specific feature (e.g., INTERVAL syntax, strftime, DATE_SUB, TO_CHAR), mention it briefly in the reasoning and why.\n",
      "\n",
      "C. DML specifics\n",
      "- INSERT: always use explicit column lists (INSERT INTO table (col1, col2, ...) VALUES (...)) unless the schema context clearly and explicitly shows the exact column order and the prompt implies omission.\n",
      "- UPDATE / DELETE: include exactly the WHERE clause requested by the user. Do not add extra filters that change semantics.\n",
      "- When a user's request would normally require multiple dependent statements (e.g., insert a parent then child row referencing generated id) you must either:\n",
      "  - produce a single statement that satisfies the prompt if possible, or\n",
      "  - explicitly state the assumption (e.g., choosing specific ids) in the reasoning and then emit a single statement that performs the requested change; or\n",
      "  - ask for clarification in the reasoning if you cannot safely assume values or the schema prevents a single-statement solution.\n",
      "- Do NOT emit multiple INSERTs/UPDATEs/DELETEs unless the prompt explicitly requests multiple statements. If the prompt requests \"insert X and Y\", clarify if needed.\n",
      "\n",
      "D. Aggregation, grouping, and result-shape\n",
      "- Use GROUP BY when the prompt wants per-group aggregates. Use HAVING to filter groups.\n",
      "- Preserve whether the user expects a scalar (one-row aggregate) vs multiple grouped rows. State this decision briefly in reasoning.\n",
      "- If the user likely expects zero instead of NULL for aggregate results (counts/sums/totals), use COALESCE(..., 0) and state that choice in reasoning. Do NOT wrap aggregates with COALESCE if that would change a requested NULL result — mention that change explicitly.\n",
      "\n",
      "E. Joins, LEFT JOIN vs INNER JOIN decisions\n",
      "- If the prompt asks to include parent rows that may have no children (e.g., list every member and their counts including zero), use LEFT JOIN + conditional aggregation and state that choice.\n",
      "- If the prompt asks for only rows that have matches, use INNER JOIN or filter on the activity table.\n",
      "- Be explicit in reasoning when either approach could be valid and why you chose one.\n",
      "\n",
      "F. Anti-joins and \"none\" semantics\n",
      "- Prefer NOT EXISTS (correlated subquery) for anti-joins when appropriate (works across dialects).\n",
      "- LEFT JOIN ... WHERE child.key IS NULL is acceptable and often safer than NOT IN if child key may contain NULLs. If you choose NOT IN, note NULL-sensitivity in reasoning.\n",
      "- Explicitly state the interpretation you used for \"have not done X\" (e.g., no records at all vs none in a specific subset).\n",
      "\n",
      "G. Case-insensitive matches and NULLs\n",
      "- For case-insensitive string matches use LOWER(column) LIKE '%term%' and state that in reasoning.\n",
      "- Always quote string/date literals with single quotes.\n",
      "- Handle NULLs carefully: if you convert NULL aggregates to 0, say so in reasoning.\n",
      "\n",
      "H. Dates and times\n",
      "- Use date literals in the same format as the schema sample (prefer 'YYYY-MM-DD').\n",
      "- If you use relative date arithmetic (e.g., \"past month\"), use standard SQL constructs if possible and note dialect specifics in reasoning (e.g., INTERVAL '1 month' is standard-ish; MySQL uses DATE_SUB()).\n",
      "- If the prompt is ambiguous about inclusive/exclusive bounds, state your chosen interpretation in reasoning.\n",
      "\n",
      "I. Top-N and ordering\n",
      "- Use ORDER BY ... DESC and LIMIT n (or FETCH FIRST n ROWS ONLY for standard SQL). If you use LIMIT, mention it's widely supported; if you use FETCH, mention it's standard SQL.\n",
      "\n",
      "J. When schema is missing required tables/columns\n",
      "- Do not invent tables/columns. If the prompt needs data not present, either:\n",
      "  - explicitly state the assumption you will make and then produce SQL that uses that assumption, or\n",
      "  - ask the user for clarification (briefly) in the reasoning.\n",
      "- If you assume ids or values, mention your chosen values and why.\n",
      "\n",
      "K. Single-statement and non-extraneous output\n",
      "- Provide only one SQL statement after the reasoning header. Do not include multiple alternatives or extraneous SQL.\n",
      "- Avoid extra commentary, examples, or explanations outside the concise reasoning paragraph.\n",
      "\n",
      "Common SQL patterns you should use when relevant (and mention briefly in reasoning when you apply them):\n",
      "- Aggregation: SELECT SUM(col) AS total FROM table WHERE ...;\n",
      "- Conditional aggregation: SUM(CASE WHEN condition THEN value ELSE 0 END) or COUNT(CASE WHEN condition THEN 1 END);\n",
      "- Include unmatched parent rows: FROM parent LEFT JOIN child ON ... GROUP BY parent.id;\n",
      "- Anti-join: WHERE NOT EXISTS (SELECT 1 FROM child c WHERE c.parent_id = p.id ...);\n",
      "- Percentages: ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) AS percentage;\n",
      "- Case-insensitive search: LOWER(name) LIKE '%term%';\n",
      "- Defensive null-to-zero: COALESCE(SUM(...), 0);\n",
      "- Top N: ORDER BY aggregate_col DESC LIMIT N (or FETCH FIRST N ROWS ONLY).\n",
      "\n",
      "Error-prone pitfalls to avoid (and mention if you considered them):\n",
      "- Omitting a WHERE/GROUP BY/HAVING clause the user required, or adding extra WHERE filters that change the result.\n",
      "- Using LEFT JOIN + conditional aggregation when the prompt required only matching rows (or vice versa).\n",
      "- Filtering on the wrong table when authoritative data lives in a joined table.\n",
      "- Changing the result shape unexpectedly (returning a scalar when the prompt asked for per-group rows).\n",
      "- Returning NULL from aggregates where the user likely expects 0 — handle with COALESCE only when appropriate and mention it.\n",
      "\n",
      "Behavior about assumptions and ambiguity:\n",
      "- If the prompt is ambiguous, state the chosen interpretation explicitly in the reasoning and proceed.\n",
      "- If the schema sample suggests obvious id/value choices (e.g., only one project has id=2 and prompt names \"Green Building\"), you may assume that mapping but must state the assumption.\n",
      "- Keep assumptions minimal and explicit.\n",
      "\n",
      "Concise reasoning style:\n",
      "- 1–4 sentences total.\n",
      "- Mention approach, key SQL techniques used (e.g., conditional aggregation, LEFT JOIN, NOT EXISTS), and any critical assumptions or dialect notes.\n",
      "- If you intentionally changed a possible output (e.g., COALESCE to return 0), call that out.\n",
      "\n",
      "Examples of unacceptable behaviors (you must not do these):\n",
      "- Returning multiple SQL statements when the prompt did not ask for them.\n",
      "- Changing user's semantics silently (e.g., replacing NULL with 0 without saying so).\n",
      "- Selecting columns that don't exist in sql_context or inventing tables/columns silently.\n",
      "- Giving long essays instead of a short reasoning paragraph.\n",
      "\n",
      "Be precise, conservative, and explicit: preserve semantics, minimize assumptions, and state those you do make. Follow the exact output format: \"reasoning\" then concise paragraph, blank line, \"sql\" then a single SQL statement.\n"
     ]
    }
   ],
   "source": [
    "print(optimized_program.predict.signature.instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f2a893",
   "metadata": {},
   "source": [
    "# Store Programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e93a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "program.save(\"./dspy_program/5mini_program.json\", save_program=False)\n",
    "optimized_program.save(\"./optimized_program/5mini_program.json\", save_program=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e85afb4",
   "metadata": {},
   "source": [
    "# Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64245ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datasets import Dataset\n",
    "from time import perf_counter\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "def evaluate_program(\n",
    "    program,\n",
    "    ds_test: Dataset,\n",
    "    limit: int = 100,\n",
    "    max_workers: int = 8,\n",
    "    field_map: Optional[Dict[str, str]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate a DSPy program on the first `limit` rows of a HF Dataset split.\n",
    "\n",
    "    Args:\n",
    "        program: a DSPy Module with signature program(sql_prompt=..., sql_context=...)\n",
    "        ds_test: Hugging Face Dataset (e.g., ds[\"test\"])\n",
    "        limit: number of rows to evaluate (default 100)\n",
    "        max_workers: parallel threads for I/O-bound LM + judge\n",
    "        field_map: optional mapping if your column names differ:\n",
    "                   {\"sql_prompt\": \"...\", \"sql_context\": \"...\", \"sql\": \"...\"}\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"accuracy\": float,\n",
    "          \"correct\": int,\n",
    "          \"total\": int,\n",
    "          \"avg_latency_s\": float,\n",
    "          \"failures\": [ {idx, reason, pred_sql, feedback} ... up to 20 ],\n",
    "        }\n",
    "    \"\"\"\n",
    "    if field_map is None:\n",
    "        field_map = {\"sql_prompt\": \"sql_prompt\", \"sql_context\": \"sql_context\", \"sql\": \"sql\"}\n",
    "\n",
    "    n = min(limit, len(ds_test))\n",
    "    subset = ds_test.select(range(n))\n",
    "    start = perf_counter()\n",
    "\n",
    "    def _eval_one(i_row):\n",
    "        i, row = i_row\n",
    "        try:\n",
    "            pred = program(\n",
    "                sql_prompt=row[field_map[\"sql_prompt\"]],\n",
    "                sql_context=row[field_map[\"sql_context\"]],\n",
    "            )\n",
    "            pred_sql = getattr(pred, \"sql\", None) or (pred.get(\"sql\") if isinstance(pred, dict) else None) or \"\"\n",
    "            jr = judge(\n",
    "                sql_context=row[field_map[\"sql_context\"]],\n",
    "                sql_prompt=row[field_map[\"sql_prompt\"]],\n",
    "                golden_sql=row[field_map[\"sql\"]],\n",
    "                candidate_sql=pred_sql,\n",
    "            )\n",
    "            ok = bool(getattr(jr, \"similar\", False))\n",
    "            feedback = getattr(jr, \"reasoning\", \"\") or \"\"\n",
    "            return (i, ok, pred_sql, feedback, None)\n",
    "        except Exception as e:\n",
    "            return (i, False, \"\", \"\", f\"{type(e).__name__}: {e}\")\n",
    "\n",
    "    results = []\n",
    "    # Threaded evaluation (I/O bound: LM + judge). Tune max_workers to your provider limits.\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futures = [ex.submit(_eval_one, (i, subset[i])) for i in range(n)]\n",
    "        for f in as_completed(futures):\n",
    "            results.append(f.result())\n",
    "\n",
    "    # Sort back to input order\n",
    "    results.sort(key=lambda x: x[0])\n",
    "\n",
    "    correct = sum(1 for _, ok, *_ in results if ok)\n",
    "    total = n\n",
    "    acc = correct / total if total else 0.0\n",
    "    elapsed = perf_counter() - start\n",
    "    avg_lat = elapsed / total if total else 0.0\n",
    "\n",
    "    failures = []\n",
    "    for i, ok, pred_sql, feedback, err in results:\n",
    "        if not ok and len(failures) < 20:\n",
    "            failures.append({\n",
    "                \"idx\": i,\n",
    "                \"reason\": (\"error: \" + err) if err else \"mismatch\",\n",
    "                \"pred_sql\": pred_sql,\n",
    "                \"feedback\": feedback,\n",
    "            })\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"correct\": correct,\n",
    "        \"total\": total,\n",
    "        \"avg_latency_s\": avg_lat,\n",
    "        \"failures\": failures,\n",
    "    }\n",
    "    \n",
    "test_split = ds[\"test\"]\n",
    "test_split = test_split.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ca6e0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Program: {'accuracy': 0.574, 'correct': 287, 'total': 500, 'avg_latency_s': 0.33906054133400904, 'failures': [{'idx': 2, 'reason': 'mismatch', 'pred_sql': \"SELECT COUNT(DISTINCT s.id) AS hearing_students_with_accommodations_past_year FROM student s JOIN accommodation a ON s.id = a.student_id WHERE s.disability = 'Hearing Impairment' AND a.date >= date('now','-1 year');\", 'feedback': \"The two queries are not functionally equivalent. The golden SQL counts accommodation records (COUNT(a.id)) for hearing-impaired students in the past year, so a student with multiple accommodations would be counted multiple times. The candidate SQL counts distinct hearing-impaired students who received at least one accommodation in the past year (COUNT(DISTINCT s.id)), so each student is counted only once. The date expressions are equivalent in purpose (one uses DATE_SUB(CURRENT_DATE, INTERVAL 1 YEAR), the other uses date('now','-1 year')), but that does not change the substantive difference in what is being counted.\"}, {'idx': 3, 'reason': 'mismatch', 'pred_sql': \"SELECT 'highest_avg_temperature' AS metric, name, avg_temperature AS value FROM oceans WHERE avg_temperature = (SELECT MAX(avg_temperature) FROM oceans) UNION ALL SELECT 'highest_avg_salinity' AS metric, name, avg_salinity AS value FROM oceans WHERE avg_salinity = (SELECT MAX(avg_salinity) FROM oceans);\", 'feedback': 'The two queries are not functionally equivalent.\\n\\n- The golden query returns a single row with the aggregate MAX(avg_temperature) and MAX(avg_salinity). It also selects the column name without aggregation or GROUP BY, which makes the returned name indeterminate (and not guaranteed to be the ocean(s) that have those maxima).\\n- The candidate query returns two rows: one identifying the ocean that has the highest avg_temperature and one identifying the ocean that has the highest avg_salinity (with labels and the corresponding values). The candidate therefore explicitly reports which ocean corresponds to each maximum, whereas the golden query reports only the maximum values (and an unreliable name).\\n\\nBecause they produce different result shapes and the candidate explicitly finds the ocean names for each maximum while the golden query does not reliably do so, they are not functionally similar.'}, {'idx': 8, 'reason': 'mismatch', 'pred_sql': 'SELECT region_id, incident_quarter AS last_quarter, ROUND(avg_3q,2) AS moving_avg_last_3 FROM (SELECT region_id, incident_quarter, AVG(incident_count) OVER (PARTITION BY region_id ORDER BY incident_quarter ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS avg_3q, MAX(incident_quarter) OVER (PARTITION BY region_id) AS max_q FROM incident_data) t WHERE incident_quarter = max_q ORDER BY region_id;', 'feedback': 'The golden query returns the 3-quarter moving average for every region_id and every incident_quarter (i.e., a row per quarter with the window AVG). The candidate computes the same window AVG but then filters to keep only the row where incident_quarter = max_q per region (the most recent quarter), and also rounds/renames the column. Because the candidate returns only the last quarter per region rather than all quarters, it is not functionally equivalent to the golden query. The rounding/alias differences are minor, but the filtering to only the latest quarter is a substantive semantic change.'}, {'idx': 10, 'reason': 'mismatch', 'pred_sql': \"SELECT DISTINCT i.name FROM Instructors i JOIN Courses c ON i.id = c.instructor_id WHERE c.course_type = 'traditional art' AND c.location = 'South America';\", 'feedback': \"The two queries are not functionally equivalent.\\n\\nDifferences:\\n- Column filtered: the golden SQL filters I.region = 'South America' (an attribute of the instructor). The candidate filters c.location = 'South America' (an attribute of the course). Those are different predicates and can return different sets of instructors.\\n- DISTINCT: the candidate uses SELECT DISTINCT i.name to remove duplicates; the golden does not, so it may produce duplicate names if an instructor taught multiple matching courses. (This affects results even when the same rows are selected.)\\n- Semantics vs prompt: the prompt asks for instructors who have taught in South America; that aligns with c.location, but the golden query uses I.region instead. Regardless of which matches the prompt better, the two queries are not functionally similar.\\n\\nTherefore they are not equivalent in general.\"}, {'idx': 11, 'reason': 'mismatch', 'pred_sql': \"INSERT INTO bus_routes(route_id, start_location, end_location) SELECT COALESCE(MAX(route_id),0)+1, 'Chinatown', 'Airport' FROM bus_routes;\", 'feedback': \"Both queries insert a row with start_location = 'Chinatown' and end_location = 'Airport'. However, the golden SQL inserts a fixed route_id = 2, while the candidate computes route_id as COALESCE(MAX(route_id),0)+1 (next-highest id). That produces different route_id values depending on the current contents of bus_routes (e.g., if the table is empty the candidate inserts route_id = 1, not 2). Because the inserted route_id can differ and the queries are not guaranteed to produce the same result in all states, they are not functionally equivalent.\"}, {'idx': 13, 'reason': 'mismatch', 'pred_sql': \"SELECT id,project,start_date,end_date FROM projects WHERE lower(project) LIKE '%defen%' AND end_date BETWEEN DATE '2022-01-01' AND DATE '2022-06-30' AND end_date > start_date + INTERVAL '6 months';\", 'feedback': 'The candidate query is not functionally equivalent to the golden query. Key differences:\\n- Date windows are different: the golden SQL requires start_date IN H1 2022 and end_date IN H2 2022. The candidate requires end_date IN H1 2022 and checks end_date > start_date + 6 months (so reversed H1/H2 placement).\\n- The golden filters planned = FALSE (a required condition); the candidate does not filter by planned at all.\\n- The golden returns project names; the candidate returns id, project, start_date, end_date (different projection).\\n- The candidate attempts to identify \"defense\" projects via lower(project) LIKE \\'%defen%\\', whereas the golden uses the planned flag to identify the relevant projects — these are different selection criteria.\\nBecause of these incompatible date ranges and different filtering logic, the two queries are not functionally similar.'}, {'idx': 16, 'reason': 'mismatch', 'pred_sql': 'SELECT region, SUM(funding) AS total_funding FROM conservation_funding WHERE year >= EXTRACT(YEAR FROM CURRENT_DATE) - 4 GROUP BY region ORDER BY total_funding DESC;', 'feedback': \"The two queries are not functionally equivalent.\\n\\n- The candidate query correctly filters rows to the last 5 years (year >= current_year - 4) and returns one summed total_funding per region for that 5-year window.\\n- The golden query cross-joins generate_series(1,5) and groups by that sequence, producing multiple rows per region (one per sequence value) and a years_ago column. It does not apply a WHERE filter tying the sequence to the table's year, so it does not produce the same single total per region for the last 5 years. Its intent appears different (a per-year breakdown) and its behavior differs from the candidate.\\n\\nTherefore they are not functionally similar.\"}, {'idx': 17, 'reason': 'mismatch', 'pred_sql': \"SELECT id, name, hectares, country FROM forests WHERE country IN ('Canada','USA') AND hectares > 1000000;\", 'feedback': 'The WHERE clauses are equivalent (country IN (...) is the same as country = ... OR ... and both use hectares > 1000000). However the projections differ: the golden SQL returns only forests.name while the candidate returns id, name, hectares, country. Because the selected columns differ, the queries are not functionally identical.'}, {'idx': 18, 'reason': 'mismatch', 'pred_sql': \"SELECT t.id, ft.reactor_temp AS first_temp, ft.reactor_temp_time AS first_time, lt.reactor_temp AS last_temp, lt.reactor_temp_time AS last_time, CASE WHEN lt.reactor_temp>ft.reactor_temp THEN 'increasing' WHEN lt.reactor_temp<ft.reactor_temp THEN 'decreasing' ELSE 'stable' END AS trend, (lt.reactor_temp - ft.reactor_temp) AS temp_change FROM (SELECT id, MIN(reactor_temp_time) AS first_time, MAX(reactor_temp_time) AS last_time FROM production_runs GROUP BY id) t JOIN production_runs ft ON ft.id=t.id AND ft.reactor_temp_time=t.first_time JOIN production_runs lt ON lt.id=t.id AND lt.reactor_temp_time=t.last_time ORDER BY t.id;\", 'feedback': \"The two queries produce different results and are not functionally equivalent.\\n\\n- Golden SQL: uses a window function to produce a row per measurement (id, reactor_temp_time) and the cumulative average of reactor_temp up to that time. Output example from the provided data:\\n  - id=1, 08:00 -> avg_reactor_temp = 120.5\\n  - id=1, 09:00 -> avg_reactor_temp = (120.5+122.3)/2 = 121.4\\n  - id=2, 08:00 -> avg_reactor_temp = 125.3\\n\\n- Candidate SQL: returns one summary row per id (first and last measurement times and temperatures), computes a simple overall trend (increasing/decreasing/stable) and the temperature difference between first and last readings. Output example:\\n  - id=1 -> first_temp=120.5, last_temp=122.3, trend='increasing', temp_change=1.8\\n  - id=2 -> first_temp=125.3, last_temp=125.3, trend='stable', temp_change=0.0\\n\\nMajor differences:\\n- Golden provides a cumulative average at each timestamp (multiple rows per id); candidate provides a single summary row per id comparing first vs last readings.\\n- Golden uses a window aggregation; candidate uses grouping + self-joins to find first/last rows and a CASE to classify trend.\\n- The columns and semantics differ (cumulative average vs first/last comparison), so they do not implement the same functionality.\\n\\nTherefore they are not similar.\"}, {'idx': 20, 'reason': 'mismatch', 'pred_sql': 'SELECT property_id, MIN(co_ownership_percentage) AS min_co_ownership_percentage FROM co_ownership_agreements GROUP BY property_id HAVING COUNT(*) > 1;', 'feedback': 'The two queries are not functionally equivalent.\\n\\n- The golden query computes a single scalar: the overall minimum co_ownership_percentage across all properties that have more than one distinct co_owner_id (it filters property_id by HAVING COUNT(DISTINCT co_owner_id) > 1, then takes MIN over those rows).\\n- The candidate query returns a row per property (property_id, MIN(co_ownership_percentage)) for each property that has more than one row (HAVING COUNT(*) > 1). That yields per-property minima rather than a single global minimum.\\n- Also the candidate uses COUNT(*) instead of COUNT(DISTINCT co_owner_id); that may differ if duplicate co_owner_id rows exist.\\n\\nWith the provided data, the candidate would return two rows (101 -> 50.0, 102 -> 25.0) while the golden query would return a single value 25.0. Therefore they are not similar.'}, {'idx': 22, 'reason': 'mismatch', 'pred_sql': \"SELECT AVG(daily_success) AS avg_success_per_day FROM (SELECT command_date, SUM(success_count) AS daily_success FROM voice_commands WHERE hotel_category = 'Luxury' GROUP BY command_date) AS daily_totals;\", 'feedback': \"The two queries are not functionally equivalent.\\n\\n- Golden SQL: groups by hotel_category and by day (DATE_TRUNC), then returns hotel_category and AVG(success_count) for each day-group but only for days with more than one row (HAVING COUNT(*) > 1). It excludes days with a single record and does not compute a single average across days — it yields per-day averages (one row per day group that passes the HAVING).\\n- Candidate SQL: first sums success_count per day (daily total for Luxury), then computes the average of those daily totals across all days. This yields a single average number (including days with only one record) representing average successful commands per day.\\n\\nBecause the golden query filters out single-row days and produces per-day group averages rather than a single average across days, the candidate's result (average of daily totals) is not the same.\"}, {'idx': 26, 'reason': 'mismatch', 'pred_sql': 'SELECT continent, SUM(co2_emission)*1.0/NULLIF(SUM(visitors),0) AS avg_co2_per_visitor FROM international_tourists WHERE EXTRACT(YEAR FROM visit_date)=2022 GROUP BY continent ORDER BY continent;', 'feedback': 'The two queries are not functionally equivalent.\\n\\nDifferences:\\n- Aggregation: the golden SQL uses AVG(co2_emission) (average CO2 per row), while the candidate computes SUM(co2_emission)/SUM(visitors) (CO2 per visitor). The candidate matches the prompt\\'s intent (\"per international tourist\"); the golden does not.\\n- Date filter: the golden query filters visit_date = \\'2022-01-01\\' (one specific date). The candidate filters EXTRACT(YEAR FROM visit_date)=2022 (all of 2022). These produce different row sets.\\n- The candidate also handles division by zero (NULLIF) and adds ORDER BY, neither of which affect equivalence but reinforce different behavior.\\n\\nBecause of the different calculations and different date filtering, they are not functionally similar.'}, {'idx': 27, 'reason': 'mismatch', 'pred_sql': 'SELECT ms.id AS station_id, ms.name AS station_name, ms.location AS station_location, COUNT(DISTINCT so.species_id) AS unique_species_count, MIN(so.observation_date) AS earliest_observation_date FROM monitoring_stations ms LEFT JOIN species_observations so ON ms.id = so.station_id GROUP BY ms.id, ms.name, ms.location ORDER BY ms.id;', 'feedback': 'Both queries compute the distinct species count and the earliest observation date per station (they use COUNT(DISTINCT species_id) and MIN(observation_date)). However there are important differences:\\n\\n- Join type: the golden query uses an INNER JOIN (excludes stations with no observations). The candidate uses a LEFT JOIN (includes stations with zero observations with count = 0 and NULL earliest date). This changes the result set when stations have no observations.\\n- Grouping/columns: the candidate groups by ms.id, ms.name, ms.location and returns extra columns (id and location) and an ORDER BY; the golden groups only by name and returns only the name plus the aggregates. Grouping by id is actually safer, but the extra columns mean the output schema differs.\\nBecause the LEFT vs INNER join changes which stations appear, and the candidate returns additional columns, the two queries are not functionally equivalent to the golden query.'}, {'idx': 28, 'reason': 'mismatch', 'pred_sql': \"SELECT d.dapp_id, d.name AS dapp_name FROM blockchain_domain.decentralized_applications d JOIN blockchain_domain.smart_contracts s ON d.smart_contract_id = s.contract_id WHERE LOWER(s.name) LIKE '%rust%';\", 'feedback': 'The candidate is not functionally equivalent. Differences:\\n- The golden query requires an exact match WHERE name = \\'Rust\\' (and uses a scalar subquery for contract_id). The candidate uses WHERE LOWER(s.name) LIKE \\'%rust%\\', which is case-insensitive and will match any name containing \"rust\" (e.g., \"Rusty\", \"rust-lang\", \"MyRustContract\"), returning a superset of results.\\n- The candidate uses a JOIN (which handles multiple matching contracts), while the golden query\\'s scalar subquery would fail or behave differently if multiple smart_contracts had name = \\'Rust\\'.\\n- The column alias change (d.name AS dapp_name) is only cosmetic.\\n\\nBecause of the differing match semantics (exact equality vs. substring/case-insensitive match) and different behavior with multiple smart_contract rows, they are not functionally similar.'}, {'idx': 32, 'reason': 'mismatch', 'pred_sql': \"SELECT e.employee_country AS country, SUM(e.num_employees) AS total_employees FROM employees e JOIN mining_companies m ON e.company_id = m.id WHERE e.employee_country IN ('Israel','Saudi Arabia') GROUP BY e.employee_country ORDER BY total_employees DESC;\", 'feedback': 'The two queries are not functionally equivalent.\\n\\nDifferences:\\n- The golden SQL filters companies by mc.continent = \\'Asia\\' (intended to restrict to a continent), while the candidate uses an explicit country list WHERE e.employee_country IN (\\'Israel\\',\\'Saudi Arabia\\'). These are different filters (and the golden\\'s continent filter is broader/wrong for \"Middle East\" and also references a non-existent column in the provided schema).\\n- The golden enforces e.employee_country = mc.country in the WHERE clause; the candidate does not check that employee_country matches the company\\'s country (it relies only on the join by company_id). That can produce different groupings if employee_country and company country differ.\\n- The golden groups by mc.country; the candidate groups by e.employee_country. Those are equivalent only when employee_country = mc.country; otherwise results differ.\\n- The candidate also adds ORDER BY total_employees DESC, which changes result ordering (not present in golden).\\n\\nBecause of these semantic differences (different filters and missing equality condition), they are not functionally similar.'}, {'idx': 33, 'reason': 'mismatch', 'pred_sql': \"SELECT VehicleType, COUNT(*) AS incidents FROM MaintenanceIncidents WHERE VehicleType LIKE '%Bus%' AND IncidentDate BETWEEN DATE((SELECT MAX(IncidentDate) FROM MaintenanceIncidents), '-1 month') AND (SELECT MAX(IncidentDate) FROM MaintenanceIncidents) GROUP BY VehicleType ORDER BY incidents DESC LIMIT 1;\", 'feedback': 'The two queries are not functionally equivalent.\\n\\nMain differences:\\n- Source of \"Bus\" filtering: the golden SQL filters on v.Service = \\'Bus\\' in a Vehicles table (join on VehicleID). The candidate infers \"bus\" by VehicleType LIKE \\'%Bus%\\' within MaintenanceIncidents. Those are different predicates and can produce different rows (and the context shows no Vehicles table exists, so the golden query is referencing a table not present).\\n- Time window: the golden SQL uses IncidentDate >= DATEADD(month, -1, GETDATE()) (last month relative to current date). The candidate uses a window based on the data\\'s MAX(IncidentDate) (MAX(...) and subtract 1 month), i.e., last month relative to the dataset\\'s latest incident. These will normally differ.\\n- Result cardinality/intent: the golden query returns counts for all vehicle types ordered descending (no LIMIT); the candidate adds LIMIT 1 to return only the top type.\\n- Candidate\\'s date expression is nonstandard/likely incorrect SQL syntax (DATE((SELECT MAX...), \\'-1 month\\')), so behavior/portability differs.\\n\\nBecause of these semantic differences (filter source, timeframe reference, and result selection), the candidate is not functionally similar to the golden SQL.'}, {'idx': 34, 'reason': 'mismatch', 'pred_sql': \"SELECT product_name, price FROM products WHERE country_of_manufacture <> 'US' AND is_on_sale = FALSE;\", 'feedback': \"The two queries differ in the string used to exclude US-made products: the golden SQL checks country_of_manufacture != 'United States' while the candidate checks country_of_manufacture <> 'US'. The <> operator is equivalent to !=, so the logical operator on is_on_sale is the same, but 'US' and 'United States' are different literal values. Therefore the candidate will not exclude rows where country_of_manufacture = 'United States' (because 'United States' <> 'US' is true), so it is not functionally equivalent to the golden query.\"}, {'idx': 37, 'reason': 'mismatch', 'pred_sql': \"SELECT CASE WHEN totals.total_visitors=0 THEN 0 ELSE ROUND(100.0 * repeats.repeat_visitors / totals.total_visitors, 2) END AS repeat_percentage FROM (SELECT SUM(CASE WHEN cnt>1 THEN 1 ELSE 0 END) AS repeat_visitors, COUNT(*) AS total_visitors FROM (SELECT visitor_id, COUNT(*) AS cnt FROM Visitors WHERE city='New Orleans' AND event_name LIKE '%Jazz%' GROUP BY visitor_id) AS per_visitor) AS repeats CROSS JOIN (SELECT (SELECT COUNT(*) FROM (SELECT visitor_id FROM Visitors WHERE city='New Orleans' AND event_name LIKE '%Jazz%' GROUP BY visitor_id) AS v) AS total_visitors) AS totals;\", 'feedback': 'The golden query (as written) computes COUNT(DISTINCT visitor_id) over jazz events in New Orleans divided by the same COUNT(DISTINCT visitor_id) — which always yields 100 (unless NULL/zero handling differs). In contrast, the candidate query:\\n\\n- Groups by visitor_id for jazz events in New Orleans,\\n- Counts how many distinct visitors had cnt > 1 (repeat_visitors),\\n- Divides repeat_visitors by the number of distinct visitors to produce the percentage of repeat attendees (with rounding and a zero-division guard).\\n\\nThese are different computations. With the provided data (visitor_ids 1,2,3,4 each appearing once), the candidate returns 0.0 (no repeat visitors) while the golden query returns 100. Therefore they are not functionally similar.'}, {'idx': 40, 'reason': 'mismatch', 'pred_sql': \"SELECT sign_up_date, COUNT(*) AS new_fans FROM fan_club_members WHERE sign_up_date >= (CURRENT_DATE - INTERVAL '30' DAY) GROUP BY sign_up_date ORDER BY sign_up_date;\", 'feedback': 'The two queries produce different results:\\n\\n- The golden SQL returns a single scalar count of all members who signed up in the last 30 days (one row with total COUNT(*)). (Note: its ORDER BY sign_up_date is semantically incorrect for a single aggregate but does not change that it aims to return a single total.)\\n- The candidate SQL returns one row per sign_up_date (daily counts) for the last 30 days, i.e., multiple rows with columns (sign_up_date, new_fans), ordered by date.\\n\\nBecause one returns a single total and the other returns per-day counts, they are not functionally equivalent despite both filtering by the last 30 days and using equivalent date-interval syntax.'}, {'idx': 41, 'reason': 'mismatch', 'pred_sql': \"SELECT COUNT(*) AS count_sites FROM ExcavationSites WHERE location IN ('Italy','Greece');\", 'feedback': 'The golden query counts distinct site names (COUNT(DISTINCT site)), while the candidate counts rows (COUNT(*)). If the table can contain multiple rows for the same site, the candidate may overcount. Although with the provided data both queries return the same numeric result (one Italy row and one Greece row), they are not functionally equivalent in general because of the missing DISTINCT.'}]}\n"
     ]
    }
   ],
   "source": [
    "og_metrics = evaluate_program(program, test_split, limit=500, max_workers=32)\n",
    "print(f\"Original Program: {og_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2fdea35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Program: {'accuracy': 0.62, 'correct': 310, 'total': 500, 'avg_latency_s': 0.1289489997500059, 'failures': [{'idx': 2, 'reason': 'mismatch', 'pred_sql': 'SELECT rd.mine_id, SUM(rd.reclamation_cost) AS total_reclamation_cost, es.employee_count AS total_employees\\nFROM reclamation_data rd\\nJOIN employee_stats es ON rd.mine_id = es.mine_id\\nWHERE es.employee_count > 300\\nGROUP BY rd.mine_id;', 'feedback': 'The candidate SQL query is filtering mines with more than 300 employees from the `employee_stats` view and calculating the total reclamation cost for those mines from the `reclamation_data` table. However, the candidate SQL query does not include the condition for mines with more than 5 years of operation as the golden SQL query does.'}, {'idx': 5, 'reason': 'mismatch', 'pred_sql': 'SELECT EXTRACT(YEAR FROM p.SaleDate) AS SaleYear, SUM(c.ClaimAmount) AS TotalClaimAmount\\nFROM Policies p\\nJOIN Claims c ON p.PolicyID = c.PolicyID\\nWHERE EXTRACT(MONTH FROM p.SaleDate) BETWEEN 1 AND 3\\nGROUP BY EXTRACT(YEAR FROM p.SaleDate);', 'feedback': 'The golden SQL query calculates the total claim amount for policies sold in the first quarter of each year by using the `PARTITION BY` and `DATEPART` functions to group the data by quarters and filter for the first quarter. On the other hand, the candidate SQL query calculates the total claim amount for policies sold between January and March of each year by extracting the year and month from the sale date and filtering for months 1 to 3. Both queries join the `Claims` and `Policies` tables on the `PolicyID` and perform the necessary aggregation. The key difference lies in how the quarters are handled in the filtering condition.'}, {'idx': 7, 'reason': 'mismatch', 'pred_sql': 'SELECT c.name AS country, MAX(af.year_preserved) AS max_years_preserved\\nFROM countries c\\nLEFT JOIN art_forms af ON c.id = af.country_id\\nGROUP BY c.name;', 'feedback': \"The golden SQL query selects the country name and the maximum number of years a traditional art form has been preserved for each country by joining the 'countries' and 'art_forms' tables on the country_id. It then groups the results by the country's id. \\nThe candidate SQL query also aims to find the maximum number of years a traditional art form has been preserved in each country by joining the 'countries' and 'art_forms' tables on the country_id. However, it aliases the selected columns differently ('c.name' and 'af.year_preserved'), groups by 'c.name' instead of 'c.id', and uses a LEFT JOIN instead of an INNER JOIN.\"}, {'idx': 12, 'reason': 'mismatch', 'pred_sql': \"UPDATE recycling_initiatives\\nSET initiative_type = 'Metal Recycling'\\nWHERE year = 2022 AND city = 'Paris';\", 'feedback': \"The golden SQL query updates the initiative_type to 'Composting' for recycling initiatives in Paris in 2022, while the candidate SQL query updates the initiative_type to 'Metal Recycling' for the same matching criteria. The candidate query is not functionally similar to the golden query because they update the initiative_type to different values.\"}, {'idx': 19, 'reason': 'mismatch', 'pred_sql': 'SELECT m.name, COUNT(t.officer_id) AS num_trainings\\nFROM mental_health_parity_officers m\\nLEFT JOIN trainings_conducted t ON m.officer_id = t.officer_id\\nGROUP BY m.name;', 'feedback': 'The candidate SQL query joins the tables using a LEFT JOIN instead of an INNER JOIN like in the golden SQL query. This means that the candidate query will include all records from the \"mental_health_parity_officers\" table, even if there are no matching records in the \"trainings_conducted\" table. The candidate query also aliases the tables differently than the golden SQL query.'}, {'idx': 20, 'reason': 'mismatch', 'pred_sql': 'SELECT Country, SUM(Operations) AS Total_Operations\\nFROM PeacekeepingOperations\\nGROUP BY Country\\nORDER BY Total_Operations DESC;', 'feedback': 'The candidate SQL query simply calculates the sum of operations for each country without considering the ranking from highest to lowest. It also does not utilize window functions like the golden SQL query to achieve the desired ranking based on total operations.'}, {'idx': 21, 'reason': 'mismatch', 'pred_sql': 'SELECT h.country, SUM(h.events) AS total_events\\nFROM Humanitarian_Assistance h\\nWHERE h.year >= YEAR(CURRENT_DATE) - 2\\nGROUP BY h.country;', 'feedback': 'The golden SQL query calculates the total number of humanitarian assistance events by each country in the last 3 years by joining the Humanitarian_Assistance and Countries tables and filtering the data based on the year. It correctly groups the results by country name. \\nThe candidate SQL query also calculates the total number of events by each country; however, it filters the data based on the last 2 years instead of the last 3 years. This difference in the filter condition makes the candidate query functionally different from the golden query.'}, {'idx': 26, 'reason': 'mismatch', 'pred_sql': \"INSERT INTO audience (visitor_id, name, country, visit_date) \\nVALUES (DEFAULT, 'Alex', 'USA', CURRENT_DATE);\", 'feedback': 'The candidate SQL query and the golden SQL query both aim to insert a new record into the \"audience\" table for a visitor named \"Alex\" from the USA. However, there are differences in how the new visitor_id value is generated and the date value is obtained.'}, {'idx': 28, 'reason': 'mismatch', 'pred_sql': \"SELECT *\\nFROM subscribers\\nWHERE last_data_usage_date < DATE_SUB(DATE_FORMAT(CURRENT_DATE, '%Y-%m-01'), INTERVAL 1 MONTH);\", 'feedback': \"The golden SQL query retrieves the `id`, `last_data_usage_date`, and `monthly_data_usage` columns from the `subscribers` table where the `monthly_data_usage` is 0 and the `last_data_usage_date` is within the last month. It accurately filters subscribers who have not used any data in the last month by comparing the `monthly_data_usage` and the date of last data usage.\\n\\nThe candidate SQL query also aims to find subscribers who have not used any data in the last month. However, it uses a different approach by comparing the `last_data_usage_date` with the first day of the previous month to identify those subscribers. This approach may not be as accurate as the golden query in determining the last month's data usage status for each subscriber.\"}, {'idx': 31, 'reason': 'mismatch', 'pred_sql': 'SELECT SiteID, COUNT(DISTINCT ObjectType) AS num_unique_artifact_types\\nFROM Artifact\\nGROUP BY SiteID;', 'feedback': 'The golden SQL query joins the ExcavationSite and Artifact tables on the SiteID, groups the results by SiteName, and counts the distinct ObjectTypes to find the unique artifact types at each excavation site. On the other hand, the candidate SQL query only counts the distinct ObjectTypes grouped by SiteID from the Artifact table without joining the ExcavationSite table.'}, {'idx': 33, 'reason': 'mismatch', 'pred_sql': 'SELECT \\n    LaunchID,\\n    Country,\\n    LaunchDate,\\n    LaunchDate - LAG(LaunchDate, 1) OVER(PARTITION BY Country ORDER BY LaunchDate) AS DaysSincePreviousLaunch\\nFROM \\n    SatelliteLaunches\\nORDER BY \\n    Country, LaunchDate;', 'feedback': 'The golden SQL query calculates the difference in launch dates for each pair of consecutive satellite launches by country using the LEAD function to get the next launch date. It then calculates the number of days between the launches using the DATEDIFF function. On the other hand, the candidate SQL query attempts to calculate the days since the previous launch using the LAG function, which is not the same as determining the difference between consecutive launches.'}, {'idx': 35, 'reason': 'mismatch', 'pred_sql': \"SELECT EXTRACT(MONTH FROM date) AS month,\\n       AVG(temperature - LAG(temperature) OVER(PARTITION BY location ORDER BY date)) AS avg_temp_change\\nFROM WeatherData\\nWHERE location = 'Arctic'\\nAND EXTRACT(YEAR FROM date) = 2020\\nGROUP BY EXTRACT(MONTH FROM date);\", 'feedback': \"The golden SQL query calculates the average temperature in the Arctic region by month for the year 2020 by directly averaging the temperatures with the location and year filters applied. On the other hand, the candidate SQL query attempts to calculate the average temperature change in the Arctic region by month for the year 2020 by subtracting the previous temperature from the current one and then finding the average. The candidate SQL query is not directly calculating the average temperature and temperature change without considering the previous temperature value which doesn't align with the user's original request.\"}, {'idx': 37, 'reason': 'mismatch', 'pred_sql': 'SELECT year, COUNT(*) AS total_grants\\nFROM research_grants\\nGROUP BY year;', 'feedback': 'The golden SQL query calculates the sum of the amount for each year by grouping the results based on the year. On the other hand, the candidate SQL query counts the total number of grants per year by grouping the results based on the year. Both queries provide different aggregate calculations for research grants per year.'}, {'idx': 38, 'reason': 'mismatch', 'pred_sql': 'SELECT Site_Name, SUM(Number_of_Artifacts) AS Total_Artifacts\\nFROM Excavation_Sites\\nGROUP BY Site_Name;', 'feedback': 'The golden SQL query is selecting the `Site_ID`, `Site_Name`, and the sum of `Number_of_Artifacts` grouped by `Site_ID` and `Site_Name`, which provides the number of artifacts found in each excavation site. The candidate SQL query is selecting the `Site_Name` and the sum of `Number_of_Artifacts` grouped by `Site_Name`. The main difference is that the golden SQL query includes the `Site_ID` in the selection and grouping, providing a more detailed breakdown of artifacts per site compared to the candidate SQL query.'}, {'idx': 41, 'reason': 'mismatch', 'pred_sql': \"SELECT gender, AVG(mental_health_score) AS avg_mental_health_score\\nFROM teachers\\nWHERE gender = 'Male'\\nGROUP BY gender;\", 'feedback': 'The candidate SQL query calculates the average mental health score only for male teachers by filtering the records based on the gender column. It then groups the results by the gender column, which does not serve any practical purpose in this case. On the other hand, the golden SQL query correctly calculates the average mental health score for male teachers by joining the teachers table with the teacher_mental_health table on teacher id and filtering only the male teachers before calculating the average.'}, {'idx': 42, 'reason': 'mismatch', 'pred_sql': 'SELECT DISTINCT c1.county_id\\nFROM costs c1\\nJOIN costs c2 ON c1.county_id = c2.county_id AND c1.year = c2.year + 4\\nWHERE c1.year = (SELECT MAX(year) FROM costs WHERE county_id = c1.county_id)\\nAND c1.cost > c2.cost;', 'feedback': \"The golden SQL query uses a window function to calculate the change in costs over 4 years for each county in New Mexico, ensuring that there are exactly 4 consecutive years with increasing costs. It also filters the results based on the state being 'New Mexico'. On the other hand, the candidate SQL query joins the costs table on itself to compare costs between the current year and the year 4 years prior for each county, without considering consecutive years or filtering by state. Therefore, the candidate query does not accurately identify counties with increasing healthcare costs in New Mexico over the past 4 years.\"}, {'idx': 43, 'reason': 'mismatch', 'pred_sql': 'SELECT r.region_id, r.region_name, SUM(th.volume) AS total_volume\\nFROM regions r\\nJOIN timber_harvest th ON r.region_id = th.region_id\\nGROUP BY r.region_id, r.region_name;', 'feedback': 'The candidate SQL query selects the region_id along with the region_name, and calculates the total volume of timber harvested by each region. The JOIN condition between the two tables is based on region_id. However, in the golden SQL query, the region_name is selected instead of the region_id, and the GROUP BY clause is based on region_name.'}, {'idx': 45, 'reason': 'mismatch', 'pred_sql': \"SELECT AVG(severity) AS average_severity\\nFROM vulnerabilities\\nWHERE category = 'Malware';\", 'feedback': \"The golden SQL query calculates the weighted average of severity levels in the 'Malware' category based on specific severity levels, while the candidate SQL query calculates the simple average of severity levels in the 'Malware' category.\"}, {'idx': 47, 'reason': 'mismatch', 'pred_sql': \"SELECT d1.cause\\nFROM donor_location d1\\nJOIN donor_location d2 ON d1.cause = d2.cause\\nWHERE d1.country = 'United States' AND d2.country = 'Canada';\", 'feedback': 'The golden SQL query uses the INTERSECT operator to find the causes that received funding from donors in both the United States and Canada. It correctly retrieves the causes based on the country criteria separately and then intersects the results. On the other hand, the candidate SQL query attempts to achieve the same result through a self-join on the table based on the cause, which may not yield the correct intersection of causes.'}, {'idx': 48, 'reason': 'mismatch', 'pred_sql': \"SELECT region, SUM(quantity) AS total_offshore_drilling_platforms\\nFROM InfrastructureData\\nWHERE platform_type = 'offshore_drilling'\\nAND region IN ('North Sea', 'Gulf of Mexico')\\nGROUP BY region;\", 'feedback': \"The golden SQL query calculates the total number of offshore drilling platforms in the North Sea and Gulf of Mexico by summing the quantities where the region is 'North Sea' and platform_type is 'offshore_drilling', or where the region is 'Gulf of Mexico' and platform_type is 'offshore_drilling'. \\nThe candidate SQL query calculates the total offshore drilling platforms for regions 'North Sea' and 'Gulf of Mexico' without specifying the platform type in the WHERE clause. It also groups the results by region.\"}]}\n"
     ]
    }
   ],
   "source": [
    "opt_metrics = evaluate_program(optimized_program, test_split, limit=500, max_workers=32)\n",
    "print(f\"Original Program: {opt_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9937eb78",
   "metadata": {},
   "source": [
    "# Store Eval Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcc020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_metrics(metrics, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4, sort_keys=True) \n",
    "\n",
    "save_metrics(og_metrics, \"./dspy_program/5.1-mini.json\")\n",
    "save_metrics(opt_metrics,\"./optimized_program/5.1-mini.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
