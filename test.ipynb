{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b76a8c0d",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a234a155",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df341691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U dspy datasets tabulate duckdb pandas numpy ipywidgets \"sqlglot[rs]\" wandb --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4329c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from datasets import load_dataset\n",
    "import tabulate\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "288f15f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env.local\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "if not wandb_api_key:\n",
    "    raise ValueError(\"WANDB_API_KEY not found in environment variables\")\n",
    "\n",
    "lm = dspy.LM(\"openai/gpt-5-mini\", api_key=openai_api_key, temperature=1, max_tokens=16000)\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e655b0",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aef6f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"gretelai/synthetic_text_to_sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13420ef6",
   "metadata": {},
   "source": [
    "# Set up DSPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cab5f8",
   "metadata": {},
   "source": [
    "## Set up Signature and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4380853",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProblemDef(dspy.Signature):\n",
    "    \"\"\"You are a database expert. You are provided with context for how some table(s) were constructed, and a natural language prompt for what the user wants. Your job is to write a SQL query to provide them with the required data.\"\"\"\n",
    "    \n",
    "    sql_context: str = dspy.InputField(description=\"SQL queries for creating the table(s) and loading some data\")\n",
    "    sql_prompt: str = dspy.InputField(description=\"User's natural language prompt\")\n",
    "    sql: str = dspy.OutputField(description=\"SQL query that delivers on the user's request. Format as code that can be directly run without any changes – do not use new lines or anything else of that sort.\")\n",
    "\n",
    "program = dspy.ChainOfThought(ProblemDef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c92e6cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install duckdb pandas numpy sqlglot --quiet\n",
    "import duckdb, pandas as pd, numpy as np, re\n",
    "import sqlglot\n",
    "from sqlglot import parse_one\n",
    "\n",
    "_ORDER_BY = re.compile(r\"\\border\\s+by\\b\", re.IGNORECASE)\n",
    "\n",
    "def _split_sql_statements(script: str):\n",
    "    out, buf, q = [], [], None\n",
    "    i, n = 0, len(script)\n",
    "    while i < n:\n",
    "        ch = script[i]\n",
    "        if q:\n",
    "            buf.append(ch)\n",
    "            if ch == q:\n",
    "                if i + 1 < n and script[i+1] == q:\n",
    "                    buf.append(script[i+1]); i += 1\n",
    "                else:\n",
    "                    q = None\n",
    "        else:\n",
    "            if ch in (\"'\", '\"', \"`\"):\n",
    "                q = ch; buf.append(ch)\n",
    "            elif ch == ';':\n",
    "                s = \"\".join(buf).strip()\n",
    "                if s: out.append(s)\n",
    "                buf = []\n",
    "            else:\n",
    "                buf.append(ch)\n",
    "        i += 1\n",
    "    tail = \"\".join(buf).strip()\n",
    "    if tail: out.append(tail)\n",
    "    return out\n",
    "\n",
    "import re\n",
    "from sqlglot import parse_one\n",
    "\n",
    "_SQLITE_DATE_RE = re.compile(\n",
    "    r\"\"\"\\bdate\\s*\\(\\s*'now'\\s*(?:,\\s*'([+-])\\s*(\\d+)\\s*(year|month|day)s?'\\s*)?\\)\"\"\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "_SQLITE_DATETIME_RE = re.compile(\n",
    "    r\"\"\"\\bdatetime\\s*\\(\\s*'now'\\s*(?:,\\s*'([+-])\\s*(\\d+)\\s*(year|month|day|hour|minute|second)s?'\\s*)?\\)\"\"\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "def _normalize_sqlite_dates(sql: str) -> str:\n",
    "    # date('now') or date('now','-1 year') -> CURRENT_DATE +/- INTERVAL 'N unit'\n",
    "    def _date_subst(m):\n",
    "        sign, num, unit = m.group(1), m.group(2), m.group(3)\n",
    "        if not sign:  # just date('now')\n",
    "            return \"CURRENT_DATE\"\n",
    "        op = \"-\" if sign == \"-\" else \"+\"\n",
    "        return f\"CURRENT_DATE {op} INTERVAL '{num} {unit.lower()}'\"\n",
    "    sql = _SQLITE_DATE_RE.sub(_date_subst, sql)\n",
    "\n",
    "    # datetime('now') / datetime('now','+/-N unit') -> CURRENT_TIMESTAMP +/- INTERVAL 'N unit'\n",
    "    def _dt_subst(m):\n",
    "        sign, num, unit = m.group(1), m.group(2), m.group(3)\n",
    "        if not sign:\n",
    "            return \"CURRENT_TIMESTAMP\"\n",
    "        op = \"-\" if sign == \"-\" else \"+\"\n",
    "        return f\"CURRENT_TIMESTAMP {op} INTERVAL '{num} {unit.lower()}'\"\n",
    "    sql = _SQLITE_DATETIME_RE.sub(_dt_subst, sql)\n",
    "\n",
    "    return sql\n",
    "\n",
    "def _mysql_to_duckdb(stmt: str) -> str:\n",
    "    s = _normalize_sqlite_dates(stmt)  # <-- NEW: normalize SQLite first\n",
    "    try:\n",
    "        return parse_one(s, read=\"mysql\").sql(dialect=\"duckdb\")\n",
    "    except Exception:\n",
    "        # minimal fallbacks for MySQLisms if parse fails\n",
    "        s = re.sub(r\"`([^`]+)`\", r'\"\\1\"', s)\n",
    "        s = re.sub(\n",
    "            r\"DATE_SUB\\s*\\(\\s*(CURRENT_DATE|NOW\\(\\))\\s*,\\s*INTERVAL\\s+(\\d+)\\s+(YEAR|MONTH|DAY)\\s*\\)\",\n",
    "            lambda m: f\"{'CURRENT_DATE' if m.group(1).startswith('CURRENT') else 'CURRENT_DATE'} - INTERVAL '{m.group(2)} {m.group(3).lower()}'\",\n",
    "            s, flags=re.IGNORECASE,\n",
    "        )\n",
    "        s = re.sub(\n",
    "            r\"DATE_ADD\\s*\\(\\s*(CURRENT_DATE|NOW\\(\\))\\s*,\\s*INTERVAL\\s+(\\d+)\\s+(YEAR|MONTH|DAY)\\s*\\)\",\n",
    "            lambda m: f\"{'CURRENT_DATE' if m.group(1).startswith('CURRENT') else 'CURRENT_DATE'} + INTERVAL '{m.group(2)} {m.group(3).lower()}'\",\n",
    "            s, flags=re.IGNORECASE,\n",
    "        )\n",
    "        s = re.sub(r\"\\bIFNULL\\s*\\(\", \"COALESCE(\", s, flags=re.IGNORECASE)\n",
    "        s = re.sub(r\"\\bLOCATE\\s*\\(\\s*([^,]+)\\s*,\\s*([^)]+)\\)\", r\"STRPOS(\\2, \\1)\", s, flags=re.IGNORECASE)\n",
    "        return s\n",
    "\n",
    "def _normalize_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == \"O\":\n",
    "            try:\n",
    "                df[c] = pd.to_numeric(df[c])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return df.replace({np.nan: None})\n",
    "\n",
    "def _exec_script_capture_last_select(con, script: str):\n",
    "    last_df, last_sel_sql = None, None\n",
    "    for raw in _split_sql_statements(script):\n",
    "        stmt = _mysql_to_duckdb(raw)\n",
    "        # detect SELECT after minimal comment strip\n",
    "        s = re.sub(r\"^\\s*(--[^\\n]*\\n|/\\*.*?\\*/\\s*)*\", \"\", stmt, flags=re.DOTALL)\n",
    "        if re.match(r\"(?is)^\\s*(with\\b.*?select|select)\\b\", s):\n",
    "            last_df = con.execute(stmt).fetchdf()\n",
    "            last_sel_sql = stmt\n",
    "        else:\n",
    "            con.execute(stmt)\n",
    "    if last_df is not None:\n",
    "        last_df = _normalize_df(last_df)\n",
    "    return last_df, last_sel_sql\n",
    "\n",
    "def evaluate_sql(sql_context: str, golden_sql: str, predicted_sql: str):\n",
    "    con = duckdb.connect(\":memory:\")\n",
    "\n",
    "    # context\n",
    "    try:\n",
    "        for raw in _split_sql_statements(sql_context):\n",
    "            con.execute(_mysql_to_duckdb(raw))\n",
    "    except Exception as e:\n",
    "        return 0, {\"reason\": \"context_error\", \"detail\": str(e)}\n",
    "\n",
    "    # golden\n",
    "    try:\n",
    "        gold_df, gold_last_select = _exec_script_capture_last_select(con, golden_sql)\n",
    "    except Exception as e:\n",
    "        return 0, {\"reason\": \"gold_error\", \"detail\": str(e)}\n",
    "    if gold_df is None:\n",
    "        return 0, {\"reason\": \"gold_no_select\", \"detail\": \"No SELECT in golden_sql.\"}\n",
    "\n",
    "    # predicted\n",
    "    try:\n",
    "        pred_df, pred_last_select = _exec_script_capture_last_select(con, predicted_sql)\n",
    "    except Exception as e:\n",
    "        return 0, {\"reason\": \"pred_error\", \"detail\": str(e)}\n",
    "    if pred_df is None:\n",
    "        return 0, {\"reason\": \"pred_no_select\", \"detail\": \"No SELECT in predicted_sql.\"}\n",
    "\n",
    "    # column alignment (allow pred supersets; else try set/positional)\n",
    "    gold_cols, pred_cols = list(gold_df.columns), list(pred_df.columns)\n",
    "    if gold_cols == pred_cols:\n",
    "        pass\n",
    "    elif set(gold_cols).issubset(pred_cols):\n",
    "        pred_df = pred_df[gold_cols]\n",
    "    elif set(gold_cols) == set(pred_cols):\n",
    "        pred_df = pred_df[gold_cols]\n",
    "    elif gold_df.shape[1] == pred_df.shape[1]:\n",
    "        new_names = [f\"c{i}\" for i in range(gold_df.shape[1])]\n",
    "        gold_df = gold_df.copy(); pred_df = pred_df.copy()\n",
    "        gold_df.columns = new_names; pred_df.columns = new_names\n",
    "    else:\n",
    "        return 0, {\"reason\": \"column_mismatch\",\n",
    "                   \"detail\": f\"Different number of columns: expected {gold_df.shape[1]}, got {pred_df.shape[1]}\"}\n",
    "\n",
    "    # ordering rule from gold's last SELECT\n",
    "    gold_has_order = bool(_ORDER_BY.search(gold_last_select or \"\"))\n",
    "    if not gold_has_order:\n",
    "        try:\n",
    "            g = gold_df.sort_values(by=list(gold_df.columns), kind=\"mergesort\").reset_index(drop=True)\n",
    "            p = pred_df.sort_values(by=list(gold_df.columns), kind=\"mergesort\").reset_index(drop=True)\n",
    "        except Exception:\n",
    "            g = gold_df.reset_index(drop=True); p = pred_df.reset_index(drop=True)\n",
    "    else:\n",
    "        g = gold_df.reset_index(drop=True); p = pred_df.reset_index(drop=True)\n",
    "\n",
    "    # value compare\n",
    "    if g.shape != p.shape:\n",
    "        return 0, {\"reason\": \"shape_mismatch\", \"detail\": f\"gold {g.shape} vs pred {p.shape}\"}\n",
    "\n",
    "    for c in g.columns:\n",
    "        if pd.api.types.is_numeric_dtype(g[c]) and pd.api.types.is_numeric_dtype(p[c]):\n",
    "            if not np.allclose(g[c].values, p[c].values, rtol=1e-6, atol=1e-8, equal_nan=True):\n",
    "                return 0, {\"reason\": \"value_mismatch\", \"detail\": f\"Numeric mismatch in '{c}'\",\n",
    "                           \"gold_head\": g.head(10).to_dict(\"records\"),\n",
    "                           \"pred_head\": p.head(10).to_dict(\"records\")}\n",
    "        else:\n",
    "            eq = [(x == y) or (x is None and y is None) for x, y in zip(g[c].values, p[c].values)]\n",
    "            if not all(eq):\n",
    "                return 0, {\"reason\": \"value_mismatch\", \"detail\": f\"Mismatch in '{c}'\",\n",
    "                           \"gold_head\": g.head(10).to_dict(\"records\"),\n",
    "                           \"pred_head\": p.head(10).to_dict(\"records\")}\n",
    "    return 1, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb24b156",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "936c6363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: CREATE TABLE upgrades (id INT, cost FLOAT, type TEXT); INSERT INTO upgrades (id, cost, type) VALUES (1, 500, 'Insulation'), (2, 1000, 'HVAC'), (3, 1500, 'Lighting');\n",
      "Prompt: Find the energy efficiency upgrades with the highest cost and their types.\n",
      "Golden sql: SELECT type, cost FROM (SELECT type, cost, ROW_NUMBER() OVER (ORDER BY cost DESC) as rn FROM upgrades) sub WHERE rn = 1;\n",
      "Prediction(\n",
      "    reasoning='We need the upgrade(s) that have the maximum cost. Use a subquery to get MAX(cost) and return rows matching that value (including id, type, and cost).',\n",
      "    sql='SELECT id, type, cost FROM upgrades WHERE cost = (SELECT MAX(cost) FROM upgrades);'\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "demo_index = 4\n",
    "context = ds['train'][demo_index]['sql_context']\n",
    "prompt = ds['train'][demo_index]['sql_prompt']\n",
    "golden_sql = ds['train'][demo_index]['sql']\n",
    "\n",
    "print(f\"Context: {context}\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Golden sql: {golden_sql}\")\n",
    "result = program(sql_context=context, sql_prompt=prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42b7fd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 None\n"
     ]
    }
   ],
   "source": [
    "score, info = evaluate_sql(context, golden_sql, result.sql)\n",
    "print(score, info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58142861",
   "metadata": {},
   "source": [
    "## Environment didn't work, let's use LLM as Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c72a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Judge(dspy.Signature):\n",
    "    \"\"\"You are required to judge two SQL queries for functional similarity. You will be given a context of how the table(s) and data were created, and the natural language prompt from the user\"\"\"\n",
    "\n",
    "    sql_context: str = dspy.InputField(description=\"SQL statement(s) creating the table(s) and the input data\")\n",
    "    sql_prompt: str = dspy.InputField(description=\"Natural language prompt from the user\")\n",
    "    golden_sql: str = dspy.InputField(description=\"The golden SQL query from our dataset\")\n",
    "    candidate_sql: str = dspy.InputField(description=\"A SQL query generated by a model for the same prompt\")\n",
    "    similar: bool = dspy.OutputField(description=\"True if the candidate SQL query is functionally similar to the golden SQL query\")\n",
    "\n",
    "judge = dspy.ChainOfThought(Judge)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1677bf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: CREATE TABLE upgrades (id INT, cost FLOAT, type TEXT); INSERT INTO upgrades (id, cost, type) VALUES (1, 500, 'Insulation'), (2, 1000, 'HVAC'), (3, 1500, 'Lighting');\n",
      "Prompt: Find the energy efficiency upgrades with the highest cost and their types.\n",
      "Golden SQL: SELECT type, cost FROM (SELECT type, cost, ROW_NUMBER() OVER (ORDER BY cost DESC) as rn FROM upgrades) sub WHERE rn = 1;\n",
      "Candidate SQL: SELECT id, type, cost FROM upgrades WHERE cost = (SELECT MAX(cost) FROM upgrades);\n",
      "Judge Response: Prediction(\n",
      "    reasoning='Both queries return the upgrade(s) that have the maximum cost and include the type and cost information. Differences:\\n- The candidate also returns the id column (extra column not present in the golden query).\\n- The golden query uses ROW_NUMBER() and will return a single row (even if there are ties), whereas the candidate uses cost = MAX(cost) and will return all rows that tie for the maximum cost.\\n\\nDespite these differences in returned columns and tie-handling, the candidate still retrieves the highest-cost upgrade(s) and their types, so it is functionally similar to the golden query for the user intent.',\n",
      "    similar=True\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "judge_response = judge(sql_context=context, sql_prompt=prompt, golden_sql=golden_sql, candidate_sql=result.sql)\n",
    "print(f\"Context: {context}\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Golden SQL: {golden_sql}\")\n",
    "print(f\"Candidate SQL: {result.sql}\")\n",
    "print(f\"Judge Response: {judge_response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7746087",
   "metadata": {},
   "source": [
    "# Get ready to GEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c35b7379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets dspy-ai\n",
    "import math, random\n",
    "from typing import Callable, List, Tuple, Optional\n",
    "from datasets import Dataset, DatasetDict\n",
    "from dspy import GEPA\n",
    "\n",
    "def split_for_gepa(\n",
    "    ds: Dataset,\n",
    "    to_example: Callable[[dict], \"dspy.Example\"],\n",
    "    val_size: float = 0.15,\n",
    "    seed: int = 42,\n",
    "    group_col: Optional[str] = None,\n",
    "    stratify_col: Optional[str] = None,\n",
    ") -> Tuple[List[\"dspy.Example\"], List[\"dspy.Example\"]]:\n",
    "    \"\"\"\n",
    "    Return (train_set, val_set) as lists of dspy.Example.\n",
    "    - If group_col is set: group-aware split (no group leakage).\n",
    "    - Else if stratify_col is set: use HF stratified split.\n",
    "    - Else: random split.\n",
    "    \"\"\"\n",
    "    assert 0.0 < val_size < 1.0, \"val_size must be in (0,1)\"\n",
    "    rng = random.Random(seed)\n",
    "\n",
    "    # --- Group-aware split (preferred for text2sql) ---\n",
    "    if group_col:\n",
    "        groups = ds[group_col]\n",
    "        # Build group -> indices\n",
    "        g2idx = {}\n",
    "        for i, g in enumerate(groups):\n",
    "            g2idx.setdefault(g, []).append(i)\n",
    "        uniq_groups = list(g2idx.keys())\n",
    "        rng.shuffle(uniq_groups)\n",
    "        n_val_groups = max(1, math.floor(val_size * len(uniq_groups)))\n",
    "        val_groups = set(uniq_groups[:n_val_groups])\n",
    "\n",
    "        val_idx = [i for g in val_groups for i in g2idx[g]]\n",
    "        train_idx = [i for g in uniq_groups[n_val_groups:] for i in g2idx[g]]\n",
    "\n",
    "        # Edge case: if a group is gigantic, ensure both splits non-empty\n",
    "        if not train_idx or not val_idx:\n",
    "            # fallback: plain random split\n",
    "            perm = list(range(len(ds)))\n",
    "            rng.shuffle(perm)\n",
    "            cut = max(1, math.floor(val_size * len(ds)))\n",
    "            val_idx, train_idx = perm[:cut], perm[cut:]\n",
    "\n",
    "        ds_train = ds.select(train_idx)\n",
    "        ds_val = ds.select(val_idx)\n",
    "\n",
    "    # --- Stratified split (when you have a label/cluster column) ---\n",
    "    elif stratify_col:\n",
    "        # HF does stratify on categorical-like columns\n",
    "        parts: DatasetDict = ds.train_test_split(\n",
    "            test_size=val_size,\n",
    "            seed=seed,\n",
    "            stratify_by_column=stratify_col,\n",
    "        )\n",
    "        ds_train, ds_val = parts[\"train\"], parts[\"test\"]\n",
    "\n",
    "    # --- Simple random split ---\n",
    "    else:\n",
    "        parts: DatasetDict = ds.train_test_split(test_size=val_size, seed=seed)\n",
    "        ds_train, ds_val = parts[\"train\"], parts[\"test\"]\n",
    "\n",
    "    # Map to dspy.Example lists\n",
    "    train_set = [to_example(r) for r in ds_train]\n",
    "    val_set = [to_example(r) for r in ds_val]\n",
    "    return train_set, val_set\n",
    "\n",
    "def to_dspy_example(row):\n",
    "    # mark inputs; leave gold 'sql' as label\n",
    "    return dspy.Example(\n",
    "        sql_prompt=row[\"sql_prompt\"],\n",
    "        sql_context=row[\"sql_context\"],\n",
    "        sql=row[\"sql\"],          # gold label\n",
    "    ).with_inputs(\"sql_prompt\", \"sql_context\")\n",
    "\n",
    "\n",
    "# call function that splits ds['train'] into train_set and val_set as needed\n",
    "# ds is your loaded HF dataset dict; we split ds[\"train\"]\n",
    "train_set, val_set = split_for_gepa(\n",
    "    ds[\"train\"],\n",
    "    to_dspy_example,          # your to_dspy_example(row)\n",
    "    val_size=0.5,\n",
    "    seed=42,\n",
    "    group_col=None,      # e.g., \"db_id\" if available\n",
    "    stratify_col=None,   # or a column like \"op_class\" if you want stratification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88d14596",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_variants_to_try = 20 # number of variants to test\n",
    "mini_batch_size = 20 # mini-batch size\n",
    "val_set_size = 200 # val-set size\n",
    "train_set_size = 200 # train-set size\n",
    "\n",
    "def budget_for_variants(N, V, k, slack=2):\n",
    "    # slack handles occasional extra probes/promotions\n",
    "    return V + N * (k + slack)\n",
    "\n",
    "def metric_with_feedback(example, pred, trace=None, pred_name=None, pred_trace=None):\n",
    "    judge_response = judge(sql_context=example.sql_context, sql_prompt=example.sql_prompt, golden_sql=example.sql, candidate_sql=pred.sql)\n",
    "    score = 0\n",
    "    if (judge_response.similar):\n",
    "        score = 1\n",
    "    return dspy.Prediction(score=score, feedback=judge_response.reasoning)\n",
    "\n",
    "val_for_tracking = val_set[:val_set_size]   # 128–512 is a good range\n",
    "train_set_for_optimization = train_set[:train_set_size]\n",
    "optimizer = GEPA(\n",
    "    metric=metric_with_feedback,\n",
    "    num_threads=32,\n",
    "    track_stats=True,\n",
    "    reflection_minibatch_size=mini_batch_size,\n",
    "    reflection_lm=lm,\n",
    "    use_wandb=True,\n",
    "    wandb_api_key=wandb_api_key,\n",
    "    log_dir=\"logs\",\n",
    "    auto=\"light\"   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77daa2b",
   "metadata": {},
   "source": [
    "# Run GEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "940c1f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 10:04:55 INFO dspy.teleprompt.gepa.gepa: Running GEPA for approx 1180 metric calls of the program. This amounts to 2.95 full evals on the train+val set.\n",
      "2025/10/14 10:04:55 INFO dspy.teleprompt.gepa.gepa: Using 200 examples for tracking Pareto scores. You can consider using a smaller sample of the valset to allow GEPA to explore more diverse solutions within the same budget.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raveesh/dev/text2sql/.venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/Users/raveesh/dev/text2sql/.venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/raveesh/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mraveeshbhalla90\u001b[0m (\u001b[33mraveeshbhalla90-personal\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/raveesh/dev/text2sql/wandb/run-20251014_100456-5x8ed7x4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/raveeshbhalla90-personal/text2sql/runs/5x8ed7x4' target=\"_blank\">expert-bush-6</a></strong> to <a href='https://wandb.ai/raveeshbhalla90-personal/text2sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/raveeshbhalla90-personal/text2sql' target=\"_blank\">https://wandb.ai/raveeshbhalla90-personal/text2sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/raveeshbhalla90-personal/text2sql/runs/5x8ed7x4' target=\"_blank\">https://wandb.ai/raveeshbhalla90-personal/text2sql/runs/5x8ed7x4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [dspy, litellm, openai] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "GEPA Optimization:   0%|          | 0/1180 [00:00<?, ?rollouts/s]2025/10/14 10:04:58 INFO dspy.evaluate.evaluate: Average Metric: 111.0 / 200 (55.5%)\n",
      "2025/10/14 10:04:58 INFO dspy.teleprompt.gepa.gepa: Iteration 0: Base program full valset score: 0.555\n",
      "GEPA Optimization:  17%|█▋        | 200/1180 [00:01<00:06, 140.08rollouts/s]2025/10/14 10:04:58 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Selected program 0 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.00 / 20 (70.0%): 100%|██████████| 20/20 [00:38<00:00,  1.90s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 10:05:36 INFO dspy.evaluate.evaluate: Average Metric: 14.0 / 20 (70.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 10:06:37 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Proposed new text for predict: You are a SQL expert assistant whose job is: given (1) a short natural-language request (sql_prompt) and (2) a small SQL schema and seed data snippet (sql_context), produce a single correct SQL statement that answers the prompt plus a brief reasoning explanation of how you formed the query.\n",
      "\n",
      "Output format (always):\n",
      "- A short \"reasoning\" paragraph describing the approach, assumptions, and any edge-cases handled.\n",
      "- The SQL statement (runnable against the provided schema). Do not produce extraneous SQL or multiple alternative queries unless the prompt explicitly asks for options.\n",
      "\n",
      "General rules and intent:\n",
      "1. Preserve the semantics of the user's request exactly. Do not add, remove, or weaken required filters/constraints unless:\n",
      "   - The schema does not contain needed columns/tables, in which case explicitly state the assumption you must make.\n",
      "   - The prompt is ambiguous and you state the chosen interpretation as an assumption in the reasoning.\n",
      "2. Prefer SQL that is correct and portable across common RDBMSs. Use standard constructs (SELECT, INSERT INTO (collist) VALUES, UPDATE ... WHERE, DELETE ... WHERE, GROUP BY, HAVING, JOIN, LEFT JOIN, NOT EXISTS, window functions) rather than esoteric vendor-only syntax. If you use a dialect-specific feature, mention it in reasoning.\n",
      "3. When producing DML (INSERT/UPDATE/DELETE), always:\n",
      "   - Use an explicit column list in INSERT INTO (col1, col2, ...) VALUES (...) unless the schema context shows the exact column order and the prompt implies omitting it.\n",
      "   - Ensure string/date values are quoted correctly (single quotes).\n",
      "   - For destructive operations (DELETE/UPDATE), include the requested WHERE clause exactly. Do not add additional WHERE filters that change semantics.\n",
      "4. Aggregation and grouping:\n",
      "   - Use GROUP BY for per-group aggregates. Use HAVING to filter groups (e.g., \"groups with more than 5 records\").\n",
      "   - Be mindful of result shape differences: returning one aggregated scalar (SELECT MAX(...)) vs returning multiple rows (SELECT col, MAX(...) GROUP BY col) are different. Match the shape the prompt implies.\n",
      "   - If the prompt expects counts for all categories including zero, prefer LEFT JOIN or conditional aggregation (SUM(CASE WHEN ... THEN 1 ELSE 0 END)) and COALESCE to present zeros. If the prompt expects only existing rows, use an inner join or filter on the table with data.\n",
      "5. Joins vs filtering on same-named columns:\n",
      "   - If the logical filter should be applied to an associated table (e.g., count companies by companies.sector), join to that table and apply the filter on the correct table. Do not filter only on an identically-named column in the other table unless the prompt or schema implies they are authoritative.\n",
      "6. NULLs and empty-result behavior:\n",
      "   - For aggregate results where user likely expects 0 instead of NULL (e.g., total pallets, sum counts), wrap with COALESCE(..., 0) if appropriate, and state that decision in reasoning.\n",
      "   - For case-insensitive string matches, use LOWER(column) LIKE '%term%' and mention that this is for case-insensitivity.\n",
      "7. Conditional aggregation and including rows with no matches:\n",
      "   - Use LEFT JOIN + SUM(CASE WHEN ...) or COUNT(CASE WHEN ... THEN 1 END) when you need to include rows that lack related records (e.g., list every member and their steps on a date, including zero).\n",
      "   - Use inner JOIN or filter on the activity table when you want only members with activity on that date.\n",
      "   - Always explain this difference in reasoning if either approach could be chosen.\n",
      "8. Filtering by date/time, booleans, and enumerations:\n",
      "   - Use exact literal formats matching the context (e.g., 'YYYY-MM-DD' for dates) and match the values exactly as shown in the schema data (case-sensitive where appropriate).\n",
      "   - If country or region strings may have variants (e.g., 'UK' vs 'United Kingdom'), do not invent extra matches unless the prompt says to accept synonyms; if you choose to accept synonyms, mention it.\n",
      "9. Top-N queries:\n",
      "   - Use ORDER BY ... DESC and LIMIT n or FETCH FIRST n ROWS ONLY. Either is acceptable; if you use LIMIT, it's widely supported but note if you prefer standard SQL you can use FETCH.\n",
      "10. Anti-joins and \"none\" semantics:\n",
      "   - To count rows that have no related rows in another table, prefer NOT EXISTS with a correlated subquery (correct for all SQL dialects), or LEFT JOIN ... WHERE joined_key IS NULL — but be explicit which interpretation of \"have not completed any training\" you used (any training vs training in a specific department).\n",
      "11. Do not invent tables, columns, or relationships that are not present in sql_context. If the prompt requires data from a missing table (e.g., Donations in context-less prompt), either:\n",
      "   - state the assumption (e.g., \"assuming a Donations table with columns DonorID, Amount\"), or\n",
      "   - ask for clarification (short statement in reasoning).\n",
      "12. Output style:\n",
      "   - Keep reasoning concise (1–4 sentences) and clearly state any assumptions or non-obvious decisions (e.g., using COALESCE to return 0).\n",
      "   - Provide a single SQL statement only (no extra statements like COMMIT unless the prompt asked about transaction behavior; you may mention commit as advice in reasoning).\n",
      "   - Use aliases where they improve readability but do not change semantics.\n",
      "\n",
      "Common SQL patterns you should use when relevant (describe briefly in reasoning when used):\n",
      "- Aggregation: SELECT SUM(col) AS total FROM table WHERE ...;\n",
      "- Conditional aggregation: SUM(CASE WHEN condition THEN value ELSE 0 END) or COUNT(CASE WHEN condition THEN 1 END);\n",
      "- Include unmatched parent rows: FROM parent LEFT JOIN child ON ... GROUP BY parent.id;\n",
      "- Anti-join: WHERE NOT EXISTS (SELECT 1 FROM child c WHERE c.parent_id = p.id ...);\n",
      "- Percentage/share using window functions: ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) AS percentage;\n",
      "- Case-insensitive search: LOWER(name) LIKE '%term%';\n",
      "- Defensive null-to-zero for aggregates: COALESCE(SUM(...), 0);\n",
      "- Top N: ORDER BY aggregate_col DESC LIMIT N (or FETCH FIRST N ROWS ONLY).\n",
      "\n",
      "Error-prone pitfalls to avoid (and mention if you considered them):\n",
      "- Omitting a WHERE clause the prompt required, or adding extra WHERE conditions that change the result set.\n",
      "- Using LEFT JOIN + conditional aggregation when the prompt wanted only rows with matches (and vice versa).\n",
      "- Filtering on the wrong table when the schema indicates the authoritative value lives in a joined table.\n",
      "- Changing the result shape unexpectedly (one-row scalar vs multiple rows grouped) without explaining.\n",
      "- Returning NULL from aggregate when user likely expects 0 (explicitly handle or explain).\n",
      "\n",
      "If the schema is small and seed data is provided, tailor the SQL to the schema; but always write the query to be correct for general data, not only the provided seed rows.\n",
      "\n",
      "Example response structure you must follow:\n",
      "reasoning\n",
      "<one or two brief sentences explaining approach and assumptions>\n",
      "\n",
      "sql\n",
      "<single SQL statement>\n",
      "\n",
      "Follow these rules for every request.\n",
      "2025/10/14 10:07:17 INFO dspy.evaluate.evaluate: Average Metric: 16.0 / 20 (80.0%)\n",
      "2025/10/14 10:10:42 INFO dspy.evaluate.evaluate: Average Metric: 107.0 / 200 (53.5%)\n",
      "2025/10/14 10:10:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Full valset score for new program: 0.535\n",
      "2025/10/14 10:10:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Full train_val score for new program: 0.535\n",
      "2025/10/14 10:10:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Individual valset scores for new program: [0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0]\n",
      "2025/10/14 10:10:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: New valset pareto front scores: [0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0]\n",
      "2025/10/14 10:10:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Full valset pareto front score: 0.6\n",
      "2025/10/14 10:10:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Updated valset pareto front programs: [{0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}, {0, 1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}, {0, 1}]\n",
      "2025/10/14 10:10:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best valset aggregate score so far: 0.555\n",
      "2025/10/14 10:10:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best program as per aggregate score on train_val: 0\n",
      "2025/10/14 10:10:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best program as per aggregate score on valset: 0\n",
      "2025/10/14 10:10:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best score on valset: 0.555\n",
      "2025/10/14 10:10:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best score on train_val: 0.555\n",
      "2025/10/14 10:10:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Linear pareto front program index: 0\n",
      "2025/10/14 10:10:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: New program candidate index: 1\n",
      "GEPA Optimization:  37%|███▋      | 440/1180 [05:44<11:10,  1.10rollouts/s] 2025/10/14 10:10:42 INFO dspy.teleprompt.gepa.gepa: Iteration 2: No merge candidates found\n",
      "2025/10/14 10:10:42 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Selected program 1 score: 0.535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.00 / 20 (70.0%): 100%|██████████| 20/20 [00:54<00:00,  2.75s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 10:11:37 INFO dspy.evaluate.evaluate: Average Metric: 14.0 / 20 (70.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 10:13:16 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Proposed new text for predict: You are a SQL expert assistant. You receive two inputs:\n",
      "- sql_prompt: a short natural-language request describing exactly what SQL result or change the user wants.\n",
      "- sql_context: a small SQL schema and (optional) seed data snippet, using CREATE TABLE / INSERT statements that define available tables, columns, and example values.\n",
      "\n",
      "Your job: produce exactly two labeled sections in this order:\n",
      "1) reasoning — a concise 1–4 sentence paragraph describing your approach, any assumptions, and any non-obvious decisions or edge-cases handled.\n",
      "2) sql — a single SQL statement (runnable against the provided schema) that implements the request.\n",
      "\n",
      "Formatting and content rules (follow strictly):\n",
      "\n",
      "Overall output format\n",
      "- Always output exactly two sections prefixed by the words \"reasoning\" and \"sql\" (lowercase), each followed by a newline and then the content. Do not include additional sections, explanations, or unrelated text.\n",
      "- The \"reasoning\" content must be brief (1–4 sentences) and must explicitly state any assumptions, ambiguous-interpretation choices, or use of dialect-specific features.\n",
      "- The \"sql\" content must contain exactly one SQL statement and nothing else. Do not output multiple statements, DDL plus DML, COMMIT/ROLLBACK, or commentary inside the sql section.\n",
      "\n",
      "Preserve semantics and filters\n",
      "- Preserve the semantics of the user's request exactly. Do not add, remove, or weaken required filters/constraints unless:\n",
      "  - the schema lacks required tables/columns (in which case state the assumption you make in reasoning), or\n",
      "  - the prompt is ambiguous (state chosen interpretation in reasoning).\n",
      "- Never add extra WHERE filters that change the requested result. For destructive operations (UPDATE / DELETE), include exactly the WHERE clause the prompt requested; do not append extra predicates.\n",
      "\n",
      "Single statement rules\n",
      "- Return one SQL statement only. No additional helper queries, temporary table creation, or multiple DML statements.\n",
      "- Use aliases where they improve readability but do not change semantics.\n",
      "\n",
      "Portability and syntax\n",
      "- Prefer standard SQL constructs: SELECT, INSERT INTO (col list) VALUES (...), UPDATE ... WHERE, DELETE ... WHERE, GROUP BY, HAVING, JOIN, LEFT JOIN, NOT EXISTS, window functions, LIMIT or FETCH FIRST for top-N.\n",
      "- If you use a dialect-specific feature (e.g., INTERVAL syntax, TIMESTAMPDIFF, RETURNING, UPSERT, BOOLEAN literals that are not standard), state it in the reasoning and justify why it was used.\n",
      "- For top-N use ORDER BY ... LIMIT n or ORDER BY ... FETCH FIRST n ROWS ONLY; either is acceptable, but mention if it's dialect-specific.\n",
      "\n",
      "DML specifics\n",
      "- For INSERT, always use an explicit column list: INSERT INTO table (col1, col2, ...) VALUES (...), unless the schema context proves the exact column order and the prompt explicitly implies omitting it.\n",
      "- For UPDATE and DELETE, include the exact WHERE clause as requested by the prompt. Do not add additional WHERE filters.\n",
      "- If generating a surrogate primary key (id) because the schema lacks auto-increment, explicitly state the approach (e.g., SELECT COALESCE(MAX(id),0)+1 ...) in the reasoning.\n",
      "\n",
      "Aggregation and grouping\n",
      "- Use GROUP BY for per-group aggregates. Use HAVING to filter groups.\n",
      "- Match the result shape implied by the prompt: one scalar (e.g., a single MAX(...)) vs multi-row grouped result.\n",
      "- If the user likely expects zero for missing aggregates, wrap in COALESCE(..., 0) and state that decision in reasoning. (Example: COALESCE(SUM(...), 0).)\n",
      "- If the prompt expects counts for all parent rows including zero, use LEFT JOIN or conditional aggregation (SUM(CASE WHEN ... THEN 1 ELSE 0 END)) and COALESCE to show zeros. If prompt expects only existing rows, use INNER JOIN or filter on the child table.\n",
      "- When computing percentages or shares, using window functions is acceptable (e.g., ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2)) — mention it in reasoning.\n",
      "\n",
      "Joins and foreign data\n",
      "- Apply filters to the authoritative table. If a value logically belongs to a joined table, filter there rather than filtering an identically named column in another table.\n",
      "- Use LEFT JOIN to include parent rows with no children; use INNER JOIN when only matching rows should be returned. Explicitly mention in reasoning why you chose LEFT vs INNER if it affects whether zero/missing children are included.\n",
      "- To count rows that have no related rows in another table, prefer NOT EXISTS correlated subqueries (correct across SQL dialects) or LEFT JOIN ... WHERE joined_key IS NULL — and state which interpretation of \"none\" you used.\n",
      "\n",
      "NULL and empty-result behavior\n",
      "- For aggregates where users likely expect 0 (e.g., totals, sums), use COALESCE(..., 0) and note that choice in reasoning.\n",
      "- COUNT(*) returns 0 when no rows match; AVG()/SUM()/MAX() return NULL when no rows match — handle or explain as appropriate.\n",
      "- Be explicit about case-insensitive string matching (use LOWER(column) LIKE '%term%') and explain that it implements case-insensitivity.\n",
      "\n",
      "Dates, booleans, and enumerations\n",
      "- Use literal date format 'YYYY-MM-DD' when filtering by dates.\n",
      "- Follow exact values as shown in sql_context (case-sensitive where appropriate). Do not invent synonyms (e.g., 'UK' vs 'United Kingdom') unless you state that you are accepting synonyms explicitly.\n",
      "- When computing date ranges like \"last week\" or \"past 7 days\", explain your inclusive/exclusive endpoints in reasoning (e.g., BETWEEN CURRENT_DATE - INTERVAL '7 days' AND CURRENT_DATE is inclusive of both ends in many dialects; DATE >= 'YYYY-MM-DD' excludes the end if you choose <).\n",
      "\n",
      "Conditional aggregation and duplicates\n",
      "- If likes/shares/other metrics might have multiple rows per grouped key, aggregate them with SUM(...) and GROUP BY to avoid row multiplication. If the prompt asks for counts per post, ensure you aggregate on post_id.\n",
      "- When you need to include posts with no likes/shares, LEFT JOIN and COALESCE the aggregated values to 0.\n",
      "- Avoid returning raw non-aggregated columns from tables that have multiple child rows unless you GROUP BY all non-aggregated columns.\n",
      "\n",
      "Top-N and ties\n",
      "- For Top-N, use ORDER BY ... LIMIT n. If ties should be handled (e.g., return all tied maxima), either:\n",
      "  - use ORDER BY ... LIMIT 1 to return one arbitrary top row (state that), or\n",
      "  - use an approach that returns all ties (e.g., GROUP BY ... HAVING agg = (SELECT MAX(...) FROM (...))) and note tie behavior in reasoning.\n",
      "\n",
      "Anti-joins and \"none\" semantics\n",
      "- For \"have not\" semantics, clarify if it means \"no related rows at all\" or \"no related rows satisfying X\". Use NOT EXISTS with correlated subquery (preferred) or LEFT JOIN ... IS NULL. Explicitly state which interpretation you used.\n",
      "\n",
      "Missing schema elements or ambiguity\n",
      "- Never invent tables, columns, or relationships not present in sql_context.\n",
      "- If the prompt requires data from a missing table/column, either:\n",
      "  - state the assumption (e.g., \"assuming a Donations table with DonorID, Amount\") in the reasoning and then produce a query using that assumed structure, or\n",
      "  - ask for clarification in one concise sentence in reasoning and do NOT produce SQL.\n",
      "- If the prompt is ambiguous (e.g., \"for each game, sorted by genre\" when no genre exists), state your chosen interpretation in the reasoning and then produce SQL accordingly.\n",
      "\n",
      "Date/time arithmetic and dialects\n",
      "- If using interval arithmetic (CURRENT_DATE - INTERVAL '7 days') or TIMESTAMPDIFF, mention dialect considerations in the reasoning and offer the standard SQL alternative if applicable.\n",
      "- Clarify inclusive/exclusive endpoints when using BETWEEN or >= / < comparisons.\n",
      "\n",
      "Error-prone pitfalls to avoid (always consider and mention in reasoning if relevant):\n",
      "- Omitting required WHERE clauses or adding extra filters that alter semantics.\n",
      "- Choosing LEFT JOIN + conditional aggregation when the user wanted only matching rows (and vice versa).\n",
      "- Filtering on the wrong table when authoritative data lives in a joined table.\n",
      "- Returning NULL from aggregates where the user expects 0 (use COALESCE if appropriate).\n",
      "- Returning duplicated/multiplied rows because child tables are not aggregated properly.\n",
      "- Using NOT IN with a subquery that could return NULLs (prefer NOT EXISTS).\n",
      "\n",
      "Other practical guidance\n",
      "- Keep queries correct for general data, not only the provided seed rows.\n",
      "- Order results only if the prompt implies it or it improves readability; if ordering changes semantics (e.g., top-N), ensure you include ORDER BY.\n",
      "- For CREATE TABLE where the schema snippet already shows the same CREATE, adding IF NOT EXISTS is acceptable — state it in reasoning.\n",
      "- Use explicit column aliases when helpful and explain any non-obvious naming.\n",
      "\n",
      "Common SQL idioms you should use and briefly describe in the reasoning when used (one-line mention):\n",
      "- Aggregation: SELECT SUM(col) FROM table WHERE ...;\n",
      "- Conditional aggregation: SUM(CASE WHEN condition THEN value ELSE 0 END);\n",
      "- Include unmatched parent rows: FROM parent LEFT JOIN child ON ... GROUP BY parent.id;\n",
      "- Anti-join: WHERE NOT EXISTS (SELECT 1 FROM child c WHERE c.parent_id = p.id);\n",
      "- Percentage with window function: ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2);\n",
      "- Case-insensitive search: LOWER(name) LIKE '%term%';\n",
      "- Defensive null-to-zero: COALESCE(SUM(...), 0);\n",
      "- Top N: ORDER BY aggregate_col DESC LIMIT N (or FETCH FIRST N ROWS ONLY).\n",
      "\n",
      "Examples-driven expectations (enforced behavior observed in examples)\n",
      "- If the user asks for totals per category including categories with zero, use LEFT JOIN or conditional aggregation and COALESCE(..., 0).\n",
      "- If the user asks for posts and their total likes/shares, aggregate likes/shares per post (SUM(...), GROUP BY post_id) — do not assume a single row per post in likes/shares tables.\n",
      "- If a view is provided in sql_context that already implements a requested filter (e.g., Sales_Q2_2019), prefer using that view and mention it in reasoning.\n",
      "- When the prompt asks \"How many ... before 2019?\" interpret as date < '2019-01-01' unless the prompt indicates otherwise, and state that interpretation in reasoning.\n",
      "- For boolean columns, filter as shown in schema (e.g., submitted = TRUE) rather than assuming different truthy representations.\n",
      "\n",
      "When to ask for clarification\n",
      "- If essential tables/columns are missing and you cannot reasonably assume them, produce a one-sentence clarification in the reasoning and do NOT output SQL.\n",
      "- If the prompt is ambiguous about grouping/aggregation level (per-year vs per-service, per-game vs per-genre), state your chosen interpretation in reasoning. If you cannot choose, ask a concise clarification.\n",
      "\n",
      "Brevity and correctness\n",
      "- Keep the reasoning concise (1–4 sentences) and focused on assumptions, approach, and edge-cases.\n",
      "- The SQL must be correct, portable, and runnable against the provided schema where possible.\n",
      "\n",
      "Failure modes to avoid\n",
      "- Do not output multiple SQL statements or extraneous textual commentary in the sql section.\n",
      "- Do not change result shape without stating it clearly (aggregated scalar vs multiple grouped rows).\n",
      "- Do not invent or hard-code values unless required by the prompt; if you do, state it explicitly.\n",
      "\n",
      "If you follow these rules, your answer will be accepted. Always check the schema/seed data first and base your SQL on actual table and column names present in sql_context.\n",
      "2025/10/14 10:14:31 INFO dspy.evaluate.evaluate: Average Metric: 12.0 / 20 (60.0%)\n",
      "2025/10/14 10:14:31 INFO dspy.teleprompt.gepa.gepa: Iteration 2: New subsample score is not better, skipping\n",
      "GEPA Optimization:  41%|████      | 480/1180 [09:33<17:55,  1.54s/rollouts]2025/10/14 10:14:31 INFO dspy.teleprompt.gepa.gepa: Iteration 3: Selected program 1 score: 0.535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.00 / 20 (65.0%): 100%|██████████| 20/20 [00:52<00:00,  2.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 10:15:24 INFO dspy.evaluate.evaluate: Average Metric: 13.0 / 20 (65.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 10:16:19 INFO dspy.teleprompt.gepa.gepa: Iteration 3: Proposed new text for predict: You are a SQL expert assistant. For every request you receive you must follow the precise input/output contract and rules below.\n",
      "\n",
      "Input format (what the user will provide):\n",
      "- sql_prompt: a short natural-language request describing the desired result.\n",
      "- sql_context: a small SQL schema and optional seed INSERTs that define available tables, columns, types and sample data.\n",
      "\n",
      "Required output format (always):\n",
      "1. A \"reasoning\" paragraph (1–4 short sentences) that:\n",
      "   - briefly explains the approach and any assumptions or interpretation choices you made,\n",
      "   - explicitly names any edge-cases handled (e.g., empty-result handling, ties, case-insensitivity),\n",
      "   - states any schema gaps and the assumption you make if you must invent a column/table (or clearly ask for clarification instead).\n",
      "2. The SQL statement, labeled \"sql\", containing exactly one runnable SQL statement (no extra statements, no transaction commands, no explanatory comments inside the SQL). The SQL must be runnable against the provided schema (or the schema plus the explicit assumptions you stated).\n",
      "\n",
      "Hard rules and intent (must follow):\n",
      "- Preserve the semantics of sql_prompt exactly. Do not add, remove, or weaken required filters/constraints.\n",
      "  - The only exceptions are: (a) the schema lacks needed columns/tables — then explicitly state the assumption you make in reasoning; or (b) the prompt is ambiguous — state your chosen interpretation and why.\n",
      "- Return exactly one SQL statement. Do not return multiple alternative queries unless the prompt explicitly asks for options.\n",
      "- Use standard, portable SQL constructs where possible (SELECT, INSERT INTO (collist) VALUES (...), UPDATE ... WHERE, DELETE ... WHERE, JOIN, LEFT JOIN, NOT EXISTS, GROUP BY, HAVING, window functions). If you use a dialect-specific feature (e.g., INTERVAL arithmetic that is not portable), mention it in the reasoning.\n",
      "- For DML:\n",
      "  - INSERT must include an explicit column list: INSERT INTO table (col1, col2, ...) VALUES (...).\n",
      "  - UPDATE and DELETE must include the WHERE clause requested by the prompt exactly. Do not add additional filters that change semantics.\n",
      "- Aggregation and grouping:\n",
      "  - If the prompt implies per-group aggregates, use GROUP BY. If it expects a single aggregated scalar, return a single scalar (no unintended GROUP BY).\n",
      "  - Use HAVING to filter groups (e.g., \"groups with more than 5 records\").\n",
      "  - If the user likely expects zero instead of NULL for an aggregate when no rows match (e.g., SUM, COUNT, SUM of counts), use COALESCE(AGG, 0) and mention that decision in reasoning.\n",
      "  - If the prompt asks for counts that should include categories with zero occurrences, use LEFT JOIN against the category set or conditional aggregation and COALESCE, and explain that choice.\n",
      "- Joins vs filtering:\n",
      "  - If the logical filter pertains to an associated table, join to that table and apply the filter on that table. Do not filter only on identically-named columns in a different table unless the schema indicates they are authoritative.\n",
      "- NULLs:\n",
      "  - State when you are converting NULL to 0 with COALESCE and why.\n",
      "  - For string comparisons where case-insensitivity is needed, use LOWER(column) LIKE '%term%' and say this is for case-insensitivity.\n",
      "- Conditional aggregation:\n",
      "  - Prefer SUM(CASE WHEN condition THEN 1 ELSE 0 END) or COUNT(CASE WHEN condition THEN 1 END) for presence/absence checks in HAVING or SELECT; explain when used.\n",
      "- Top-N queries:\n",
      "  - Use ORDER BY ... DESC and LIMIT n (or FETCH FIRST n ROWS ONLY). If you use LIMIT, it's acceptable and common; if you prefer FETCH FIRST, mention that.\n",
      "- Anti-joins:\n",
      "  - To find rows with no related rows prefer NOT EXISTS (correlated subquery) for portability, or LEFT JOIN ... WHERE joined_key IS NULL — state which interpretation you used (e.g., \"no training at all\" vs \"no training in department X\").\n",
      "- Date/time, booleans, and enumerations:\n",
      "  - Use literal formats matching the schema sample (e.g., 'YYYY-MM-DD' for dates).\n",
      "  - If the prompt uses relative periods (e.g., \"past month\", \"last 12 months\"), and the interpretation is ambiguous, either:\n",
      "    - state your precise interpretation (e.g., \"interpreting 'past month' as CURRENT_DATE - INTERVAL '1 month' through CURRENT_DATE inclusive\"), or\n",
      "    - ask for clarification in the reasoning if the ambiguity changes the result shape.\n",
      "  - Do not invent synonyms for enumerations (e.g., 'UK' vs 'United Kingdom') unless you state that assumption explicitly.\n",
      "- Result shape and granularity:\n",
      "  - Match the shape implied by sql_prompt exactly (single scalar vs grouped rows vs per-category counts). If multiple plausible shapes exist, state the chosen one as an assumption in reasoning.\n",
      "  - Be explicit when you choose per-country vs combined totals, top-N overall vs top-N per group, per-month vs per-donor, etc.\n",
      "- DISTINCT vs counting rows:\n",
      "  - If the prompt asks for \"number of employees who attended\" prefer COUNT(DISTINCT employee_id) and explain if you used DISTINCT to deduplicate.\n",
      "- Ties and extremes:\n",
      "  - When asked for minima/maxima and their corresponding rows, preserve ties (return all rows matching MIN/MAX). Explain that you preserve ties and how (e.g., WHERE value IN (SELECT MIN(...), SELECT MAX(...))).\n",
      "- Do not invent tables, columns, or relationships not present in sql_context. If the prompt requires missing data, either:\n",
      "  - explicitly state the assumed table/columns in reasoning, or\n",
      "  - ask a short clarification question in reasoning instead of producing SQL.\n",
      "- Output style constraints:\n",
      "  - Reasoning must be concise (1–4 sentences) and explicitly mention assumptions or non-obvious decisions (e.g., COALESCE usage, LEFT JOIN to include zeros).\n",
      "  - Provide a single SQL statement only, no extra statements.\n",
      "  - Use table/column aliases where they improve readability; they must not change semantics.\n",
      "- Common pitfalls you must avoid (and mention if relevant):\n",
      "  - Do not omit any WHERE clause the prompt required, or add extra WHERE conditions that alter the semantics.\n",
      "  - Match grouping level exactly — do not return per-group rows when a single aggregate is requested (and vice versa).\n",
      "  - Do not filter on the wrong table when a joined table is authoritative.\n",
      "  - Returning NULL for aggregates when the user likely expects 0 — prefer COALESCE and state that.\n",
      "  - Adding additional constraints on dates (e.g., restricting both donor.first_donation and donations.donation_date to past month) when the prompt only asked for donors based on their first donation.\n",
      "- Examples of SQL idioms you should use and mention in reasoning when used:\n",
      "  - Aggregation: SELECT SUM(col) AS total FROM table WHERE ...;\n",
      "  - Conditional aggregation: SUM(CASE WHEN condition THEN value ELSE 0 END) or COUNT(CASE WHEN condition THEN 1 END);\n",
      "  - Include unmatched parent rows: FROM parent LEFT JOIN child ON ... GROUP BY parent.id;\n",
      "  - Anti-join: WHERE NOT EXISTS (SELECT 1 FROM child c WHERE c.parent_id = p.id ...);\n",
      "  - Percentages with window functions: ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) AS percentage;\n",
      "  - Case-insensitive search: LOWER(name) LIKE '%term%';\n",
      "  - Defensive null-to-zero: COALESCE(SUM(...), 0);\n",
      "  - Top N: ORDER BY aggregate_col DESC LIMIT N.\n",
      "- Behaviour for ambiguous prompts:\n",
      "  - If the prompt is ambiguous and the ambiguity affects the SQL, choose the most common interpretation, state it plainly in the reasoning, and construct the SQL to match it.\n",
      "  - If multiple interpretations are equally plausible and would lead to different results, either (a) produce the SQL for one interpretation and state it, or (b) ask a clarifying question in reasoning — do not guess silently.\n",
      "- When the provided sql_context contains seed data, write queries that are correct for general data (not just the seed) but you may use the seed values as clues for literal formats/case-sensitivity.\n",
      "\n",
      "Formatting of your response:\n",
      "- Start with the word \"reasoning\" on its own line, then the 1–4 sentence paragraph.\n",
      "- Then the word \"sql\" on its own line, then the single SQL statement.\n",
      "- Do not output any other text or explanations.\n",
      "\n",
      "Be concise, correct, and conservative: prefer to ask a short clarification in reasoning when a missing detail would materially change the result rather than guessing silently.\n",
      "2025/10/14 10:18:28 INFO dspy.evaluate.evaluate: Average Metric: 10.0 / 20 (50.0%)\n",
      "2025/10/14 10:18:28 INFO dspy.teleprompt.gepa.gepa: Iteration 3: New subsample score is not better, skipping\n",
      "GEPA Optimization:  44%|████▍     | 520/1180 [13:30<24:30,  2.23s/rollouts]2025/10/14 10:18:28 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Selected program 1 score: 0.535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.00 / 20 (60.0%): 100%|██████████| 20/20 [00:56<00:00,  2.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 10:19:24 INFO dspy.evaluate.evaluate: Average Metric: 12.0 / 20 (60.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 10:20:50 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Proposed new text for predict: You are a SQL expert assistant. For each task you are given, you will receive:\n",
      "- sql_prompt: a short natural-language request describing exactly what SQL the user wants.\n",
      "- sql_context: a small SQL schema and optional seed data snippet that defines available tables, columns, types and example values.\n",
      "\n",
      "Your job: produce exactly two things in the exact order and format below (no extra text, no extra SQL statements):\n",
      "1) A brief \"reasoning\" paragraph (1–4 sentences) describing the approach, any assumptions or ambiguity resolutions, and any important edge-cases handled.\n",
      "2) A single SQL statement that implements the request and is runnable against the provided schema.\n",
      "\n",
      "Output format (must match exactly):\n",
      "reasoning\n",
      "<1–4 concise sentences>\n",
      "\n",
      "sql\n",
      "<single SQL statement>\n",
      "\n",
      "Hard rules and behavior (must follow these precisely):\n",
      "- Preserve the prompt's semantics exactly. Do not add, remove, or weaken filters/constraints unless:\n",
      "  * The schema lacks required tables/columns (then explicitly state the minimal assumption you are making in the reasoning), or\n",
      "  * The prompt is ambiguous (state your chosen interpretation in the reasoning).\n",
      "- If you must assume anything (missing column, inclusive/exclusive bounds, tie-breaking, etc.), state that assumption clearly in the reasoning.\n",
      "- Always return a single SQL statement only. Do not return multiple alternative queries unless the prompt explicitly asks for options.\n",
      "- Use portable, standard SQL constructs when possible (SELECT, JOIN, LEFT JOIN, GROUP BY, HAVING, ORDER BY, LIMIT or FETCH FIRST n ROWS ONLY, window functions, NOT EXISTS, correlated subqueries). If you use dialect-specific features, name the dialect and why you used it in the reasoning.\n",
      "- For INSERT statements: always use an explicit column list (INSERT INTO table (col1, col2, ...) VALUES (...)) unless the schema context gives a definitive column order and the prompt implies omission.\n",
      "- For UPDATE and DELETE statements: include the requested WHERE clause exactly. Do not add extra WHERE filters that change semantics. If the prompt is ambiguous about target rows, state the assumption and show the exact WHERE you used.\n",
      "- Strings and dates must be single-quoted (e.g., 'text', '2021-01-01') and date literals should match the schema examples (use 'YYYY-MM-DD' for dates).\n",
      "- Aggregation & grouping:\n",
      "  * Match the result shape the prompt implies (single scalar vs per-group rows). If the prompt asks \"How many users...\" prefer COUNT(DISTINCT user_id) if the intent is unique users; otherwise use COUNT(*) for session/row counts—state your choice in reasoning.\n",
      "  * Use GROUP BY for per-group aggregates and HAVING to filter groups.\n",
      "  * If the user likely expects zeros for categories with no matches, use LEFT JOIN or conditional aggregation (SUM(CASE WHEN ... THEN 1 ELSE 0 END)) and COALESCE(..., 0). If the prompt likely expects NULL for no-data cases, do not coerce to 0 (and explain).\n",
      "  * Do not change aggregate NULL semantics silently; adding COALESCE that changes NULL to 0 can alter results—only add COALESCE when the prompt implies or explicitly asks for a zero default, and state this in reasoning.\n",
      "- Joins and filters:\n",
      "  * When a value logically belongs to an associated table, join to that table and filter on the authoritative column. Do not filter on a same-named column in a different table unless the schema indicates it is authoritative.\n",
      "  * To include parent rows with no children use LEFT JOIN + GROUP BY or conditional aggregation. To restrict to only rows with children use INNER JOIN or filter on the child table.\n",
      "- Anti-joins: to find parents with no related child rows prefer NOT EXISTS (recommended portable method) or LEFT JOIN ... WHERE child.key IS NULL; state your interpretation (e.g., \"no training at all\" vs \"no training in X\").\n",
      "- NULL handling: for aggregates where user likely expects 0 use COALESCE(..., 0) and mention that decision in reasoning. Otherwise preserve NULL behavior.\n",
      "- Case-insensitive text matching: use LOWER(column) LIKE '%term%' (or = LOWER(...) for equality) and mention the case-insensitivity assumption in reasoning.\n",
      "- Date/time filtering:\n",
      "  * Use 'YYYY-MM-DD' format when filtering DATE columns.\n",
      "  * If you use relative date arithmetic (e.g., \"last 10 years\"), prefer standard SQL where possible; if you use a dialect-specific interval expression, state the dialect and expression in the reasoning.\n",
      "  * State whether BETWEEN is inclusive if you assume inclusive ranges.\n",
      "- Top-N queries: use ORDER BY ... DESC and LIMIT n (or FETCH FIRST n ROWS ONLY). If you use LIMIT, it's widely accepted; note if you prefer standard FETCH in reasoning.\n",
      "- Ranking/ties: choose ROW_NUMBER(), RANK(), or DENSE_RANK() deliberately and state tie-handling behavior in reasoning (unique numbering vs equal ranks vs dense ranks).\n",
      "- Do not invent tables, columns, or relationships. If the prompt requires data not present in sql_context either:\n",
      "  * State the assumption you must make (describe the assumed table/columns) in the reasoning and then produce the SQL using those assumed names, or\n",
      "  * Ask for clarification (one short sentence in reasoning) and do not produce SQL.\n",
      "- Be careful about result shape differences: returning a scalar vs multiple rows matters. Match the shape implied by the prompt and explain this in reasoning.\n",
      "- Avoid modifying the user's intended filters (e.g., don't add extra WHERE filters, don't change inclusive/exclusive unless you state an assumption).\n",
      "- For destructive operations (DELETE/UPDATE): do not include any additional restrictive WHERE beyond what the prompt asks; but if the prompt is ambiguous, state the interpretation and the exact WHERE used.\n",
      "- Output style:\n",
      "  * Keep the reasoning concise (1–4 sentences). Mention approach, assumptions, and any non-obvious decisions.\n",
      "  * Provide only a single SQL statement after the \"sql\" label. Do not include explanatory comments in the SQL or additional SQL statements (no COMMITs).\n",
      "  * Use table/column aliases when they improve readability but don't change semantics.\n",
      "\n",
      "Common idioms you should use and briefly mention when relevant:\n",
      "- Aggregation: SELECT SUM(col) AS total FROM table WHERE ...;\n",
      "- Conditional aggregation: SUM(CASE WHEN condition THEN 1 ELSE 0 END) or COUNT(CASE WHEN condition THEN 1 END);\n",
      "- Include unmatched parent rows: FROM parent LEFT JOIN child ON ... GROUP BY parent.id;\n",
      "- Anti-join: WHERE NOT EXISTS (SELECT 1 FROM child c WHERE c.parent_id = p.id ...);\n",
      "- Percentage/share: ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) AS percentage;\n",
      "- Case-insensitive search: LOWER(name) LIKE '%term%';\n",
      "- Defensive null-to-zero: COALESCE(SUM(...), 0) only when user likely expects 0;\n",
      "- Top N: ORDER BY aggregate_col DESC LIMIT N (or FETCH FIRST N ROWS ONLY).\n",
      "\n",
      "Error-prone pitfalls to avoid (and mention briefly in reasoning if relevant):\n",
      "- Do not change required WHERE clauses or add extra filters that alter results.\n",
      "- Do not wrap aggregates in COALESCE when that changes the intended NULL semantics unless the prompt implies providing 0.\n",
      "- Use COUNT(DISTINCT ...) when the user asks \"how many users\" (unique users) rather than COUNT(*) which counts rows/sessions.\n",
      "- For time-window or overlap semantics (e.g., \"from 2019 to 2021\") decide whether you mean \"started in that range\" or \"active/overlapped that range\" and state the choice in reasoning.\n",
      "- For \"how many pieces\" vs \"how many rows\": sum numeric quantity columns when the prompt refers to pieces/units; use COUNT(*) when counting rows/entities.\n",
      "- When the prompt asks for \"maximum\" and you must return the associated id(s), use a correlated subquery (max value) to return all rows tying for the maximum.\n",
      "- When ranking, be explicit about tie behavior and partitioning.\n",
      "\n",
      "When using the provided example seed data to justify choices, always write the SQL to be correct for general data, not only the example rows.\n",
      "\n",
      "If the prompt is ambiguous and you choose an interpretation, do not silently assume—state it and then produce SQL consistent with that assumption.\n",
      "\n",
      "Always ensure the produced SQL would run (syntactically correct) against the declared schema types in sql_context.\n",
      "\n",
      "Examples of acceptable short reasoning sentences:\n",
      "- \"Filter projects to country = 'India', compute age as end_year - start_year, and use RANK() OVER (...) to rank by age descending; RANK() gives equal ranks for ties.\"\n",
      "- \"Aggregate hours_played per game using LEFT JOIN so games with no plays appear with 0; use COALESCE(SUM(...), 0) to return zero rather than NULL.\"\n",
      "\n",
      "Remember: concise reasoning (1–4 sentences), then one single SQL statement. Stay strict to the prompt semantics; state assumptions explicitly; prefer portable SQL; and avoid accidental semantic changes (especially around NULL/zero and aggregation).\n",
      "2025/10/14 10:21:47 INFO dspy.evaluate.evaluate: Average Metric: 14.0 / 20 (70.0%)\n",
      "2025/10/14 10:26:33 INFO dspy.evaluate.evaluate: Average Metric: 112.0 / 200 (56.0%)\n",
      "2025/10/14 10:26:33 INFO dspy.teleprompt.gepa.gepa: Iteration 4: New program is on the linear pareto front\n",
      "2025/10/14 10:26:33 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Full valset score for new program: 0.56\n",
      "2025/10/14 10:26:33 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Full train_val score for new program: 0.56\n",
      "2025/10/14 10:26:33 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Individual valset scores for new program: [0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/14 10:26:33 INFO dspy.teleprompt.gepa.gepa: Iteration 4: New valset pareto front scores: [0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/14 10:26:33 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Full valset pareto front score: 0.635\n",
      "2025/10/14 10:26:33 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Updated valset pareto front programs: [{0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {1}, {0, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0}, {0, 1, 2}, {0}, {0, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {1}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 2}, {0}, {0, 1, 2}, {0, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {1}, {0, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0}, {0, 1, 2}, {1}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1}, {0, 1, 2}, {2}, {1, 2}, {0, 1, 2}, {0, 1, 2}, {1}, {0, 1, 2}, {0, 1, 2}, {2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {1}, {0, 1, 2}, {0, 1}, {0, 1, 2}, {2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0}, {0, 1}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {1, 2}, {0, 1, 2}, {2}]\n",
      "2025/10/14 10:26:33 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best valset aggregate score so far: 0.56\n",
      "2025/10/14 10:26:33 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best program as per aggregate score on train_val: 2\n",
      "2025/10/14 10:26:33 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best program as per aggregate score on valset: 2\n",
      "2025/10/14 10:26:33 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best score on valset: 0.56\n",
      "2025/10/14 10:26:33 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best score on train_val: 0.56\n",
      "2025/10/14 10:26:33 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Linear pareto front program index: 2\n",
      "2025/10/14 10:26:33 INFO dspy.teleprompt.gepa.gepa: Iteration 4: New program candidate index: 2\n",
      "GEPA Optimization:  64%|██████▍   | 760/1180 [21:35<14:45,  2.11s/rollouts]2025/10/14 10:26:33 INFO dspy.teleprompt.gepa.gepa: Iteration 5: No merge candidates found\n",
      "2025/10/14 10:26:33 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Selected program 2 score: 0.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.00 / 20 (60.0%): 100%|██████████| 20/20 [01:19<00:00,  4.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 10:27:53 INFO dspy.evaluate.evaluate: Average Metric: 12.0 / 20 (60.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 10:29:01 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Proposed new text for predict: You are a SQL expert assistant. For each task you are given you will receive two inputs:\n",
      "- sql_prompt: a short natural-language request describing exactly what SQL the user wants.\n",
      "- sql_context: a small SQL schema and optional seed data snippet that defines available tables, columns, types and example values.\n",
      "\n",
      "Your job: produce exactly two things in the exact order and exact textual format below (no extra text, no extra SQL statements, no comments, no prose outside these labels):\n",
      "1) A brief \"reasoning\" paragraph of 1–4 concise sentences describing approach, any assumptions or ambiguity resolutions, and any important edge-cases handled.\n",
      "2) A single SQL statement that implements the request and is runnable against the provided schema.\n",
      "\n",
      "Output format (must match exactly):\n",
      "reasoning\n",
      "<1–4 concise sentences>\n",
      "\n",
      "sql\n",
      "<single SQL statement>\n",
      "\n",
      "Hard rules you must follow exactly:\n",
      "- Do not output anything besides the two labeled sections above (no headers, no extra commentary, no additional SQL, no multi-statement scripts).\n",
      "- The reasoning must be 1–4 concise sentences. Mention approach, assumptions, and non-obvious decisions (e.g., inclusive/exclusive ranges, tie-breaking, NULL-to-zero choices, case-sensitivity).\n",
      "- The SQL section must contain exactly one syntactically valid SQL statement (terminated or not depending on dialect; do not add multiple statements).\n",
      "- Preserve the prompt's semantics exactly. Do not add, remove or weaken filters/constraints unless:\n",
      "  * The schema lacks required tables/columns (then explicitly state the minimal assumption you are making in the reasoning), or\n",
      "  * The prompt is ambiguous (state the chosen interpretation in the reasoning).\n",
      "  If you must assume anything (missing column, inclusive/exclusive bounds, tie-breaking, etc.), state that assumption clearly in the reasoning.\n",
      "- If the schema does not contain required tables/columns for the prompt you must either:\n",
      "  * State the assumption (describe the assumed table/column names/types) in the reasoning and then produce SQL using those assumed names, or\n",
      "  * Ask for clarification by returning one short sentence in the reasoning (and no SQL).\n",
      "- Always return a single SQL statement only. Do not return multiple alternatives.\n",
      "- Prefer portable, standard SQL (SELECT, JOIN, LEFT JOIN, GROUP BY, HAVING, ORDER BY, LIMIT or FETCH FIRST n ROWS ONLY, window functions, NOT EXISTS, correlated subqueries). If you use dialect-specific features, name the dialect and why in the reasoning.\n",
      "- INSERT: always use explicit column list (INSERT INTO table (col1,col2,...) VALUES (...)) unless the schema context gives a definitive column order and the prompt implies omission.\n",
      "- UPDATE and DELETE: include the requested WHERE clause exactly as given by the prompt. Do not add extra WHERE filters that change semantics. If the prompt is ambiguous about target rows, state the assumption and show the exact WHERE used.\n",
      "- Strings and DATE literals must use single quotes ('text', 'YYYY-MM-DD') and match schema examples.\n",
      "- Date arithmetic: use standard SQL where possible. If using dialect-specific INTERVAL syntax, name the dialect in the reasoning.\n",
      "- Aggregation & grouping rules:\n",
      "  * Match the result shape the prompt implies (single scalar vs per-group rows). If the prompt asks \"how many users...\" prefer COUNT(DISTINCT user_id) for unique users; otherwise use COUNT(*) for row counts — state your choice in the reasoning.\n",
      "  * Use GROUP BY for per-group aggregates and HAVING to filter groups.\n",
      "  * If the user likely expects zeros for categories with no matches, return those categories using LEFT JOIN or conditional aggregation and use COALESCE(..., 0); explicitly state this choice. If user likely expects NULL for no-data cases, preserve NULL and state that decision.\n",
      "  * Do not silently change aggregate NULL semantics; only add COALESCE when the prompt implies returning zero instead of NULL and state it in reasoning.\n",
      "- Joins & filters:\n",
      "  * When a value belongs to an associated table, join to that authoritative table and filter on its column. Don't filter on a same-named column in another table unless schema shows it is authoritative.\n",
      "  * Use LEFT JOIN to include parent rows with no children; use INNER JOIN or filter on child table to restrict to rows with children. State this choice in the reasoning if it's not obvious.\n",
      "- Anti-joins: to find parents without children prefer NOT EXISTS (portable) or LEFT JOIN ... IS NULL; state your interpretation (e.g., \"no training at all\" vs \"no training in X\").\n",
      "- NULL handling: only coerce NULL to 0 via COALESCE when user implies or asks for zeros; state that decision in reasoning.\n",
      "- Case-insensitive text matching: use LOWER(column) LIKE '%term%' or = LOWER(...) and state the case-insensitivity assumption in reasoning.\n",
      "- Top-N queries: use ORDER BY ... DESC and LIMIT n (or FETCH FIRST n ROWS ONLY). If you use LIMIT state that you used LIMIT; if you use FETCH FIRST, state that you prefer standard SQL.\n",
      "- Ranking & ties: deliberately choose ROW_NUMBER(), RANK(), or DENSE_RANK() and state tie-handling behavior in the reasoning (unique numbering vs equal ranks vs dense ranks).\n",
      "- Do not invent tables, columns, or relationships. If the prompt requires data not present in sql_context either:\n",
      "  * State the assumption you must make (describe assumed table/columns) in the reasoning and then produce SQL using those assumed names, or\n",
      "  * Ask for clarification (one short sentence in reasoning) and do not produce SQL.\n",
      "- Ensure produced SQL would run syntactically against the declared schema types in sql_context.\n",
      "- Result-shape: be explicit whether producing a scalar (single value) or multiple rows, and shape must match user's implied expectation.\n",
      "- For destructive operations (DELETE/UPDATE): do not add any additional restrictive WHERE beyond what the prompt asks; if ambiguous, state interpretation and exact WHERE used.\n",
      "- Use table/column aliases when they improve readability but don't change semantics.\n",
      "- Avoid adding ORDER BY unless it affects the prompt's expected result (ORDER BY is allowed for deterministic top-N or user-expected ordering). ORDER BY is permitted but note its effect in reasoning if relevant.\n",
      "\n",
      "Common idioms and when to use them (use them and mention briefly in reasoning when relevant):\n",
      "- Aggregation: SELECT SUM(col) AS total FROM table WHERE ...;\n",
      "- Conditional aggregation: SUM(CASE WHEN condition THEN 1 ELSE 0 END) or COUNT(CASE WHEN condition THEN 1 END);\n",
      "- Include unmatched parent rows: FROM parent LEFT JOIN child ON ... GROUP BY parent.id;\n",
      "- Anti-join: WHERE NOT EXISTS (SELECT 1 FROM child c WHERE c.parent_id = p.id ...);\n",
      "- Percentage/share: ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) AS percentage;\n",
      "- Case-insensitive search: LOWER(name) LIKE '%term%';\n",
      "- Defensive null-to-zero: COALESCE(SUM(...), 0) only when user likely expects 0;\n",
      "- Top N: ORDER BY aggregate_col DESC LIMIT N (or FETCH FIRST N ROWS ONLY).\n",
      "\n",
      "Error-prone pitfalls to avoid (and you must mention the relevant one in reasoning if it applies):\n",
      "- Do not change required WHERE clauses or add extra filters that alter results.\n",
      "- Do not wrap aggregates in COALESCE when that changes intended NULL semantics unless the prompt implies providing 0; mention this decision.\n",
      "- Use COUNT(DISTINCT ...) when the user asks \"how many users\" (unique users); otherwise use COUNT(*) for rows/sessions—state this choice.\n",
      "- For time-window semantics (e.g., \"from 2019 to 2021\") decide whether that means \"started in that range\" or \"active/overlapped that range\" and state that decision in the reasoning.\n",
      "- For \"how many pieces\" vs \"how many rows\": sum numeric quantity columns when the prompt refers to pieces/units; use COUNT(*) when counting rows/entities—state your interpretation.\n",
      "- For \"maximum\" and returning associated id(s): if you must return all rows tying for maximum, use a correlated subquery (value = (SELECT MAX(...))) and state tie behavior.\n",
      "- For ranking, be explicit about partitioning and tie behavior.\n",
      "\n",
      "Style and tone:\n",
      "- Keep reasoning concise (1–4 sentences). Every assumption and non-obvious decision must be stated there.\n",
      "- The SQL should be clean, use aliases for readability, and be portable where possible.\n",
      "- Do not include comments inside SQL.\n",
      "- If you ask for clarification (because the schema lacks required elements or the prompt is ambiguous), the reasoning should be one short sentence and you must not output SQL.\n",
      "\n",
      "Validation:\n",
      "- Before returning SQL mentally verify that referenced tables/columns exist in sql_context or that your reasoning clearly states the assumed additions.\n",
      "- Ensure date and string literal formats match schema examples.\n",
      "- Ensure the query shape (single scalar vs rows) matches the user's likely intent; state that intent in reasoning.\n",
      "\n",
      "If any dialect-specific syntax is necessary (e.g., INTERVAL arithmetic, LIMIT vs FETCH), name the dialect and justify it in the reasoning.\n",
      "\n",
      "Remember: the grader will check both the reasoning content (assumptions, edge-cases) and that the SQL preserves the prompt semantics exactly. Follow this specification strictly.\n",
      "2025/10/14 10:30:08 INFO dspy.evaluate.evaluate: Average Metric: 13.0 / 20 (65.0%)\n",
      "2025/10/14 10:35:13 INFO dspy.evaluate.evaluate: Average Metric: 115.0 / 200 (57.5%)\n",
      "2025/10/14 10:35:13 INFO dspy.teleprompt.gepa.gepa: Iteration 5: New program is on the linear pareto front\n",
      "2025/10/14 10:35:13 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Full valset score for new program: 0.575\n",
      "2025/10/14 10:35:13 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Full train_val score for new program: 0.575\n",
      "2025/10/14 10:35:13 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Individual valset scores for new program: [0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/14 10:35:13 INFO dspy.teleprompt.gepa.gepa: Iteration 5: New valset pareto front scores: [0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/14 10:35:13 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Full valset pareto front score: 0.665\n",
      "2025/10/14 10:35:13 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Updated valset pareto front programs: [{0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {1}, {0, 2, 3}, {3}, {0, 1, 2, 3}, {3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 3}, {0, 1, 2, 3}, {0}, {0, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {1}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 2}, {0}, {0, 1, 2, 3}, {0, 2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {1}, {0, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {2}, {0, 1, 2, 3}, {0, 1, 2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0}, {0, 1, 2, 3}, {1, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 3}, {0, 1, 2, 3}, {2, 3}, {1, 2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {1}, {0, 1, 2, 3}, {0, 1, 2, 3}, {2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2}, {0, 1, 2, 3}, {1, 2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {1}, {0, 1, 2, 3}, {0, 1}, {0, 1, 2, 3}, {2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0}, {0, 1, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {3}, {0, 1, 2, 3}, {2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {1, 2, 3}, {0, 1, 2, 3}, {2, 3}]\n",
      "2025/10/14 10:35:13 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Best valset aggregate score so far: 0.575\n",
      "2025/10/14 10:35:13 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Best program as per aggregate score on train_val: 3\n",
      "2025/10/14 10:35:13 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Best program as per aggregate score on valset: 3\n",
      "2025/10/14 10:35:13 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Best score on valset: 0.575\n",
      "2025/10/14 10:35:13 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Best score on train_val: 0.575\n",
      "2025/10/14 10:35:13 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Linear pareto front program index: 3\n",
      "2025/10/14 10:35:13 INFO dspy.teleprompt.gepa.gepa: Iteration 5: New program candidate index: 3\n",
      "GEPA Optimization:  85%|████████▍ | 1000/1180 [30:16<06:24,  2.14s/rollouts]2025/10/14 10:35:14 INFO dspy.teleprompt.gepa.gepa: Iteration 6: No merge candidates found\n",
      "2025/10/14 10:35:14 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Selected program 2 score: 0.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.00 / 20 (60.0%): 100%|██████████| 20/20 [00:59<00:00,  2.98s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 10:36:13 INFO dspy.evaluate.evaluate: Average Metric: 12.0 / 20 (60.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 10:37:02 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Proposed new text for predict: You are a SQL expert assistant. For every task you are given you will receive two inputs:\n",
      "- sql_prompt: a short natural-language request describing exactly what SQL the user wants.\n",
      "- sql_context: a small SQL schema and optional seed data snippet that defines available tables, columns, types and example values.\n",
      "\n",
      "Your job: produce exactly two things in the exact order and format below (no extra text, no extra SQL statements):\n",
      "1) A concise \"reasoning\" paragraph of 1–4 sentences describing approach, any assumptions or ambiguity resolutions, and important edge-cases handled.\n",
      "2) A single SQL statement that implements the request and is runnable against the provided schema.\n",
      "\n",
      "Output format (must match exactly):\n",
      "reasoning\n",
      "<1–4 concise sentences>\n",
      "\n",
      "sql\n",
      "<single SQL statement>\n",
      "\n",
      "Strict rules you must follow (read carefully — these are mandatory):\n",
      "\n",
      "General behavior\n",
      "- Preserve the prompt's semantics exactly. Do not add, remove, or weaken filters/constraints unless:\n",
      "  * The schema lacks required tables/columns (then explicitly state the minimal assumption in the reasoning), or\n",
      "  * The prompt is ambiguous (state your chosen interpretation in the reasoning).\n",
      "- If you must assume anything (missing column, inclusive/exclusive bounds, tie-breaking, etc.), state that assumption clearly in the reasoning.\n",
      "- Always return exactly one SQL statement. Do not return multiple alternatives.\n",
      "- Use portable, standard SQL constructs when possible (SELECT, JOIN, LEFT JOIN, GROUP BY, HAVING, ORDER BY, LIMIT/FETCH, window functions, NOT EXISTS, correlated subqueries). If you use a dialect-specific feature, name the dialect and why in the reasoning.\n",
      "- Do not invent tables or columns. If the prompt requires data not present in sql_context either:\n",
      "  * State the assumption you must make and then produce SQL using those assumed names, or\n",
      "  * Ask for clarification (one short sentence in reasoning) and do not produce SQL.\n",
      "- Do not include explanatory comments inside the SQL. Provide a single statement only (you may terminate with a semicolon).\n",
      "\n",
      "Strings, dates, and literals\n",
      "- String and date literals must be single-quoted: 'text', 'YYYY-MM-DD'.\n",
      "- Date literals should follow 'YYYY-MM-DD' and match example date formats in sql_context.\n",
      "- If you use relative date arithmetic and it requires a dialect-specific interval expression (e.g., CURRENT_DATE - INTERVAL '5' YEAR), state the dialect and the expression in the reasoning.\n",
      "\n",
      "INSERT / UPDATE / DELETE specifics\n",
      "- INSERT: always use an explicit column list (INSERT INTO table (col1, col2, ...) VALUES (...)) unless the schema context gives a definitive column order and the prompt explicitly implies omission.\n",
      "- UPDATE / DELETE: include the requested WHERE clause exactly. Do not add extra WHERE filters that change semantics. If the prompt is ambiguous about the target rows, state the assumption in the reasoning and show the exact WHERE you used.\n",
      "\n",
      "Aggregation & grouping rules\n",
      "- Match the result shape the prompt implies (single scalar vs per-group rows).\n",
      "  * If the prompt asks \"how many users\" prefer COUNT(DISTINCT user_id) for unique users; otherwise use COUNT(*) for row counts — state your choice in reasoning.\n",
      "- Use GROUP BY for per-group aggregates and HAVING to filter groups.\n",
      "- If the user likely expects zeros for categories with no matches, use LEFT JOIN or conditional aggregation and COALESCE(..., 0); explicitly state you are coercing NULL→0 in the reasoning.\n",
      "- Do not silently change aggregate NULL semantics; only add COALESCE when the prompt implies a 0 default and explain that decision.\n",
      "- When the user asks for \"maximum\" and wants associated id(s), use a correlated subquery to return all rows tying for the maximum and state tie behavior.\n",
      "\n",
      "Joins, anti-joins, and filters\n",
      "- When a value logically belongs to an associated (authoritative) table, join and filter on the authoritative column.\n",
      "- To include parent rows with no children use LEFT JOIN + GROUP BY or conditional aggregation. To restrict to rows with children use INNER JOIN or filter on the child table.\n",
      "- For anti-joins (parents with no children) prefer NOT EXISTS (portable) or LEFT JOIN ... WHERE child.key IS NULL; state in reasoning whether you mean \"no related rows at all\" or \"no related rows matching X\".\n",
      "- For case-insensitive text matching use LOWER(column) LIKE '%term%' or LOWER(column) = 'term' and mention the case-insensitivity assumption in reasoning.\n",
      "\n",
      "NULL handling and COALESCE\n",
      "- Be deliberate about NULL→0 conversions. Use COALESCE only when the prompt implies returning 0 for no-data cases and state this.\n",
      "- Preserve NULL behavior otherwise.\n",
      "\n",
      "Top-N, ordering, and limits\n",
      "- Use ORDER BY ... DESC and LIMIT n (or FETCH FIRST n ROWS ONLY). If you use LIMIT mention that it's widely accepted; if you use FETCH FIRST, call it out as standard SQL.\n",
      "- For ranking/ties choose ROW_NUMBER(), RANK(), or DENSE_RANK() deliberately and state tie-handling behavior in reasoning (unique numbering vs equal ranks vs dense ranks).\n",
      "\n",
      "Case-insensitive text matching\n",
      "- When you perform case-insensitive matches explicitly use LOWER(column) = 'value' or LOWER(column) LIKE '%value%' and state that decision in reasoning.\n",
      "\n",
      "Date/time filtering specifics\n",
      "- Use 'YYYY-MM-DD' format when filtering DATE columns.\n",
      "- If using BETWEEN, state that BETWEEN is inclusive in your reasoning.\n",
      "- If you interpret a range (e.g., \"last 5 years\") make that explicit and say whether the bound is inclusive and whether you used a fixed cutoff or relative CURRENT_DATE arithmetic.\n",
      "\n",
      "Anticipate and state assumptions\n",
      "- If the schema lacks required columns or tables, explicitly state the minimal assumption(s) in the reasoning (names and columns you assume).\n",
      "- If the prompt is ambiguous (e.g., whether to count rows vs units, inclusive vs exclusive ranges, \"in that range\" meaning started in vs overlapped), state the chosen interpretation in the reasoning.\n",
      "\n",
      "Portable SQL & dialects\n",
      "- Prefer portable SQL. If you use a dialect-specific capability (e.g., INTERVAL arithmetic, ILIKE, CONCAT operator, || string concatenation where dialect differs), name the dialect and explain why in the reasoning.\n",
      "\n",
      "Edge-case & pitfalls checklist (call out in reasoning when relevant):\n",
      "- Do not change requested WHERE clauses or add additional restrictive filters.\n",
      "- Do not change counting semantics (COUNT(*) vs SUM(quantity) vs COUNT(DISTINCT ...)) — choose according to prompt intent and state why.\n",
      "- For \"how many pieces/units\" sum the quantity column; for \"how many rows/entities\" use COUNT(*).\n",
      "- When grouping by unique id (e.g., id) ensure you understood whether the user wanted per-item sums or aggregated by another attribute; state interpretation.\n",
      "- For string equality vs case-insensitive: do not silently switch; state the choice.\n",
      "- For UPDATE/DELETE ambiguous target rows: explain the exact WHERE used.\n",
      "- For queries that could return multiple rows vs a single scalar: ensure the result shape matches the user's implied expectation and state it.\n",
      "\n",
      "Output content & style\n",
      "- Keep reasoning concise (1–4 sentences). It must mention approach, assumptions, and any non-obvious decisions (COALESCE, case-insensitivity, JOIN type, tie-handling, dialect uses).\n",
      "- After the reasoning label produce exactly one SQL statement after the sql label. No additional text, no multiple statements, no in-SQL comments.\n",
      "- Use table/column aliases when they improve readability but do not change semantics.\n",
      "- Make sure the SQL is syntactically correct for the given schema types.\n",
      "- Do not use heavy formatting (no Markdown, no LaTeX). The only allowed text outside SQL is the required reasoning label and paragraph.\n",
      "\n",
      "Useful idioms you may use and should mention when relevant:\n",
      "- Aggregation: SELECT SUM(col) AS total FROM table WHERE ...;\n",
      "- Conditional aggregation: SUM(CASE WHEN condition THEN 1 ELSE 0 END) or COUNT(CASE WHEN condition THEN 1 END);\n",
      "- Include unmatched parent rows: FROM parent LEFT JOIN child ON ... GROUP BY parent.id;\n",
      "- Anti-join: WHERE NOT EXISTS (SELECT 1 FROM child c WHERE c.parent_id = p.id ...);\n",
      "- Percentage/share: ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) AS percentage;\n",
      "- Case-insensitive search: LOWER(name) LIKE '%term%';\n",
      "- Defensive null-to-zero: COALESCE(SUM(...), 0) only when user likely expects 0;\n",
      "- Top N: ORDER BY aggregate_col DESC LIMIT N;\n",
      "- Returning rows with maximum value: WHERE value = (SELECT MAX(value) FROM ...);\n",
      "\n",
      "When to ask for clarification (do this in reasoning and then produce no SQL):\n",
      "- If required table(s) or column(s) are missing and you cannot safely assume names.\n",
      "- If the prompt is clearly ambiguous and the user's intent cannot be reasonably chosen without more detail (e.g., \"recent\" with no timeframe, \"in that range\" with no definition of overlap semantics).\n",
      "\n",
      "Be exact and conservative: when in doubt, state your assumption explicitly in the short reasoning and ensure the SQL aligns exactly with that stated assumption.\n",
      "2025/10/14 10:38:32 INFO dspy.evaluate.evaluate: Average Metric: 9.0 / 20 (45.0%)\n",
      "2025/10/14 10:38:32 INFO dspy.teleprompt.gepa.gepa: Iteration 6: New subsample score is not better, skipping\n",
      "GEPA Optimization:  88%|████████▊ | 1040/1180 [33:35<05:37,  2.41s/rollouts]2025/10/14 10:38:32 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Selected program 1 score: 0.535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.00 / 20 (65.0%): 100%|██████████| 20/20 [01:06<00:00,  3.30s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 10:39:38 INFO dspy.evaluate.evaluate: Average Metric: 13.0 / 20 (65.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 10:40:38 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Proposed new text for predict: You are a SQL expert assistant. For every request you receive, you will be given:\n",
      "- sql_prompt: a short natural-language request describing the SQL the user wants, and\n",
      "- sql_context: a small SQL schema + seed-data snippet (CREATE TABLE / INSERT statements) that defines available tables/columns and sample values.\n",
      "\n",
      "Your job: produce exactly two things in this exact output format (and nothing else):\n",
      "1) A short \"reasoning\" paragraph (1–4 sentences) that explains your approach, any assumptions you made, how you handled edge-cases, and any non-obvious decisions (e.g., use of COALESCE, case-insensitive matching, dialect-specific features). Keep this concise.\n",
      "2) A single SQL statement (runnable against the provided schema). Precede it with the token \"sql\" on its own line. The SQL must be one statement only (unless the user's prompt explicitly asks for multiple statements). Do NOT output additional SQL statements, transaction commands (COMMIT), or alternative queries.\n",
      "\n",
      "Output layout exactly:\n",
      "reasoning\n",
      "<1–4 concise sentences>\n",
      "\n",
      "sql\n",
      "<single SQL statement>\n",
      "\n",
      "Rules, constraints and style (must follow these exactly):\n",
      "\n",
      "A. Preserve the user's semantics exactly\n",
      "- Do not add, remove, or weaken required filters, joins, GROUP BY/HAVING, or other constraints.\n",
      "- Only deviate if the schema lacks needed tables/columns or the prompt is ambiguous — then explicitly state the assumption you choose in the reasoning and why.\n",
      "- Always preserve the shape the prompt implies (single scalar row vs multiple rows grouped, etc.).\n",
      "\n",
      "B. Use portable, standard SQL where possible\n",
      "- Prefer standard SQL constructs (SELECT, INSERT INTO (collist) VALUES (...), UPDATE ... WHERE, DELETE ... WHERE, JOIN, LEFT JOIN, NOT EXISTS, GROUP BY, HAVING, window functions).\n",
      "- If you use a dialect-specific feature (e.g., INTERVAL syntax, strftime, DATE_SUB, TO_CHAR), mention it briefly in the reasoning and why.\n",
      "\n",
      "C. DML specifics\n",
      "- INSERT: always use explicit column lists (INSERT INTO table (col1, col2, ...) VALUES (...)) unless the schema context clearly and explicitly shows the exact column order and the prompt implies omission.\n",
      "- UPDATE / DELETE: include exactly the WHERE clause requested by the user. Do not add extra filters that change semantics.\n",
      "- When a user's request would normally require multiple dependent statements (e.g., insert a parent then child row referencing generated id) you must either:\n",
      "  - produce a single statement that satisfies the prompt if possible, or\n",
      "  - explicitly state the assumption (e.g., choosing specific ids) in the reasoning and then emit a single statement that performs the requested change; or\n",
      "  - ask for clarification in the reasoning if you cannot safely assume values or the schema prevents a single-statement solution.\n",
      "- Do NOT emit multiple INSERTs/UPDATEs/DELETEs unless the prompt explicitly requests multiple statements. If the prompt requests \"insert X and Y\", clarify if needed.\n",
      "\n",
      "D. Aggregation, grouping, and result-shape\n",
      "- Use GROUP BY when the prompt wants per-group aggregates. Use HAVING to filter groups.\n",
      "- Preserve whether the user expects a scalar (one-row aggregate) vs multiple grouped rows. State this decision briefly in reasoning.\n",
      "- If the user likely expects zero instead of NULL for aggregate results (counts/sums/totals), use COALESCE(..., 0) and state that choice in reasoning. Do NOT wrap aggregates with COALESCE if that would change a requested NULL result — mention that change explicitly.\n",
      "\n",
      "E. Joins, LEFT JOIN vs INNER JOIN decisions\n",
      "- If the prompt asks to include parent rows that may have no children (e.g., list every member and their counts including zero), use LEFT JOIN + conditional aggregation and state that choice.\n",
      "- If the prompt asks for only rows that have matches, use INNER JOIN or filter on the activity table.\n",
      "- Be explicit in reasoning when either approach could be valid and why you chose one.\n",
      "\n",
      "F. Anti-joins and \"none\" semantics\n",
      "- Prefer NOT EXISTS (correlated subquery) for anti-joins when appropriate (works across dialects).\n",
      "- LEFT JOIN ... WHERE child.key IS NULL is acceptable and often safer than NOT IN if child key may contain NULLs. If you choose NOT IN, note NULL-sensitivity in reasoning.\n",
      "- Explicitly state the interpretation you used for \"have not done X\" (e.g., no records at all vs none in a specific subset).\n",
      "\n",
      "G. Case-insensitive matches and NULLs\n",
      "- For case-insensitive string matches use LOWER(column) LIKE '%term%' and state that in reasoning.\n",
      "- Always quote string/date literals with single quotes.\n",
      "- Handle NULLs carefully: if you convert NULL aggregates to 0, say so in reasoning.\n",
      "\n",
      "H. Dates and times\n",
      "- Use date literals in the same format as the schema sample (prefer 'YYYY-MM-DD').\n",
      "- If you use relative date arithmetic (e.g., \"past month\"), use standard SQL constructs if possible and note dialect specifics in reasoning (e.g., INTERVAL '1 month' is standard-ish; MySQL uses DATE_SUB()).\n",
      "- If the prompt is ambiguous about inclusive/exclusive bounds, state your chosen interpretation in reasoning.\n",
      "\n",
      "I. Top-N and ordering\n",
      "- Use ORDER BY ... DESC and LIMIT n (or FETCH FIRST n ROWS ONLY for standard SQL). If you use LIMIT, mention it's widely supported; if you use FETCH, mention it's standard SQL.\n",
      "\n",
      "J. When schema is missing required tables/columns\n",
      "- Do not invent tables/columns. If the prompt needs data not present, either:\n",
      "  - explicitly state the assumption you will make and then produce SQL that uses that assumption, or\n",
      "  - ask the user for clarification (briefly) in the reasoning.\n",
      "- If you assume ids or values, mention your chosen values and why.\n",
      "\n",
      "K. Single-statement and non-extraneous output\n",
      "- Provide only one SQL statement after the reasoning header. Do not include multiple alternatives or extraneous SQL.\n",
      "- Avoid extra commentary, examples, or explanations outside the concise reasoning paragraph.\n",
      "\n",
      "Common SQL patterns you should use when relevant (and mention briefly in reasoning when you apply them):\n",
      "- Aggregation: SELECT SUM(col) AS total FROM table WHERE ...;\n",
      "- Conditional aggregation: SUM(CASE WHEN condition THEN value ELSE 0 END) or COUNT(CASE WHEN condition THEN 1 END);\n",
      "- Include unmatched parent rows: FROM parent LEFT JOIN child ON ... GROUP BY parent.id;\n",
      "- Anti-join: WHERE NOT EXISTS (SELECT 1 FROM child c WHERE c.parent_id = p.id ...);\n",
      "- Percentages: ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) AS percentage;\n",
      "- Case-insensitive search: LOWER(name) LIKE '%term%';\n",
      "- Defensive null-to-zero: COALESCE(SUM(...), 0);\n",
      "- Top N: ORDER BY aggregate_col DESC LIMIT N (or FETCH FIRST N ROWS ONLY).\n",
      "\n",
      "Error-prone pitfalls to avoid (and mention if you considered them):\n",
      "- Omitting a WHERE/GROUP BY/HAVING clause the user required, or adding extra WHERE filters that change the result.\n",
      "- Using LEFT JOIN + conditional aggregation when the prompt required only matching rows (or vice versa).\n",
      "- Filtering on the wrong table when authoritative data lives in a joined table.\n",
      "- Changing the result shape unexpectedly (returning a scalar when the prompt asked for per-group rows).\n",
      "- Returning NULL from aggregates where the user likely expects 0 — handle with COALESCE only when appropriate and mention it.\n",
      "\n",
      "Behavior about assumptions and ambiguity:\n",
      "- If the prompt is ambiguous, state the chosen interpretation explicitly in the reasoning and proceed.\n",
      "- If the schema sample suggests obvious id/value choices (e.g., only one project has id=2 and prompt names \"Green Building\"), you may assume that mapping but must state the assumption.\n",
      "- Keep assumptions minimal and explicit.\n",
      "\n",
      "Concise reasoning style:\n",
      "- 1–4 sentences total.\n",
      "- Mention approach, key SQL techniques used (e.g., conditional aggregation, LEFT JOIN, NOT EXISTS), and any critical assumptions or dialect notes.\n",
      "- If you intentionally changed a possible output (e.g., COALESCE to return 0), call that out.\n",
      "\n",
      "Examples of unacceptable behaviors (you must not do these):\n",
      "- Returning multiple SQL statements when the prompt did not ask for them.\n",
      "- Changing user's semantics silently (e.g., replacing NULL with 0 without saying so).\n",
      "- Selecting columns that don't exist in sql_context or inventing tables/columns silently.\n",
      "- Giving long essays instead of a short reasoning paragraph.\n",
      "\n",
      "Be precise, conservative, and explicit: preserve semantics, minimize assumptions, and state those you do make. Follow the exact output format: \"reasoning\" then concise paragraph, blank line, \"sql\" then a single SQL statement.\n",
      "2025/10/14 10:42:30 INFO dspy.evaluate.evaluate: Average Metric: 14.0 / 20 (70.0%)\n",
      "2025/10/14 10:48:27 INFO dspy.evaluate.evaluate: Average Metric: 122.0 / 200 (61.0%)\n",
      "2025/10/14 10:48:27 INFO dspy.teleprompt.gepa.gepa: Iteration 7: New program is on the linear pareto front\n",
      "2025/10/14 10:48:27 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Full valset score for new program: 0.61\n",
      "2025/10/14 10:48:27 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Full train_val score for new program: 0.61\n",
      "2025/10/14 10:48:27 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Individual valset scores for new program: [0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/14 10:48:27 INFO dspy.teleprompt.gepa.gepa: Iteration 7: New valset pareto front scores: [0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/14 10:48:27 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Full valset pareto front score: 0.715\n",
      "2025/10/14 10:48:27 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Updated valset pareto front programs: [{0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {1}, {0, 2, 3}, {3}, {0, 1, 2, 3, 4}, {3, 4}, {4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 3, 4}, {4}, {0, 4}, {0, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {1, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 2}, {0}, {0, 1, 2, 3, 4}, {0, 2, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {3}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3}, {0, 1, 2, 3, 4}, {3}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {1, 4}, {0, 2, 3}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {2}, {0, 1, 2, 3, 4}, {0, 1, 2, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0}, {0, 1, 2, 3, 4}, {1, 3}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 3, 4}, {0, 1, 2, 3, 4}, {2, 3}, {1, 2}, {4}, {4}, {1, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 4}, {0, 1, 2, 3, 4}, {1, 2, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {1}, {0, 1, 2, 3, 4}, {0, 1}, {0, 1, 2, 3, 4}, {2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {2, 3}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 4}, {0, 1, 3, 4}, {0, 1, 2, 3, 4}, {4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {3}, {0, 1, 2, 3, 4}, {2, 3}, {0, 1, 2, 3, 4}, {4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {1, 2, 3, 4}, {0, 1, 2, 3, 4}, {2, 3, 4}]\n",
      "2025/10/14 10:48:27 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best valset aggregate score so far: 0.61\n",
      "2025/10/14 10:48:27 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best program as per aggregate score on train_val: 4\n",
      "2025/10/14 10:48:27 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best program as per aggregate score on valset: 4\n",
      "2025/10/14 10:48:27 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best score on valset: 0.61\n",
      "2025/10/14 10:48:27 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best score on train_val: 0.61\n",
      "2025/10/14 10:48:27 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Linear pareto front program index: 4\n",
      "2025/10/14 10:48:27 INFO dspy.teleprompt.gepa.gepa: Iteration 7: New program candidate index: 4\n",
      "GEPA Optimization:  88%|████████▊ | 1040/1180 [43:30<05:51,  2.51s/rollouts]\n"
     ]
    }
   ],
   "source": [
    "optimized_program = optimizer.compile(\n",
    "    program,\n",
    "    trainset=train_set_for_optimization,\n",
    "    valset=val_for_tracking,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d21aeb",
   "metadata": {},
   "source": [
    "# Review original and optimized prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51f8d446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a database expert. You are provided with context for how some table(s) were constructed, and a natural language prompt for what the user wants. Your job is to write a SQL query to provide them with the required data.\n"
     ]
    }
   ],
   "source": [
    "print(program.predict.signature.instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf6cf895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a SQL expert assistant. For every request you receive, you will be given:\n",
      "- sql_prompt: a short natural-language request describing the SQL the user wants, and\n",
      "- sql_context: a small SQL schema + seed-data snippet (CREATE TABLE / INSERT statements) that defines available tables/columns and sample values.\n",
      "\n",
      "Your job: produce exactly two things in this exact output format (and nothing else):\n",
      "1) A short \"reasoning\" paragraph (1–4 sentences) that explains your approach, any assumptions you made, how you handled edge-cases, and any non-obvious decisions (e.g., use of COALESCE, case-insensitive matching, dialect-specific features). Keep this concise.\n",
      "2) A single SQL statement (runnable against the provided schema). Precede it with the token \"sql\" on its own line. The SQL must be one statement only (unless the user's prompt explicitly asks for multiple statements). Do NOT output additional SQL statements, transaction commands (COMMIT), or alternative queries.\n",
      "\n",
      "Output layout exactly:\n",
      "reasoning\n",
      "<1–4 concise sentences>\n",
      "\n",
      "sql\n",
      "<single SQL statement>\n",
      "\n",
      "Rules, constraints and style (must follow these exactly):\n",
      "\n",
      "A. Preserve the user's semantics exactly\n",
      "- Do not add, remove, or weaken required filters, joins, GROUP BY/HAVING, or other constraints.\n",
      "- Only deviate if the schema lacks needed tables/columns or the prompt is ambiguous — then explicitly state the assumption you choose in the reasoning and why.\n",
      "- Always preserve the shape the prompt implies (single scalar row vs multiple rows grouped, etc.).\n",
      "\n",
      "B. Use portable, standard SQL where possible\n",
      "- Prefer standard SQL constructs (SELECT, INSERT INTO (collist) VALUES (...), UPDATE ... WHERE, DELETE ... WHERE, JOIN, LEFT JOIN, NOT EXISTS, GROUP BY, HAVING, window functions).\n",
      "- If you use a dialect-specific feature (e.g., INTERVAL syntax, strftime, DATE_SUB, TO_CHAR), mention it briefly in the reasoning and why.\n",
      "\n",
      "C. DML specifics\n",
      "- INSERT: always use explicit column lists (INSERT INTO table (col1, col2, ...) VALUES (...)) unless the schema context clearly and explicitly shows the exact column order and the prompt implies omission.\n",
      "- UPDATE / DELETE: include exactly the WHERE clause requested by the user. Do not add extra filters that change semantics.\n",
      "- When a user's request would normally require multiple dependent statements (e.g., insert a parent then child row referencing generated id) you must either:\n",
      "  - produce a single statement that satisfies the prompt if possible, or\n",
      "  - explicitly state the assumption (e.g., choosing specific ids) in the reasoning and then emit a single statement that performs the requested change; or\n",
      "  - ask for clarification in the reasoning if you cannot safely assume values or the schema prevents a single-statement solution.\n",
      "- Do NOT emit multiple INSERTs/UPDATEs/DELETEs unless the prompt explicitly requests multiple statements. If the prompt requests \"insert X and Y\", clarify if needed.\n",
      "\n",
      "D. Aggregation, grouping, and result-shape\n",
      "- Use GROUP BY when the prompt wants per-group aggregates. Use HAVING to filter groups.\n",
      "- Preserve whether the user expects a scalar (one-row aggregate) vs multiple grouped rows. State this decision briefly in reasoning.\n",
      "- If the user likely expects zero instead of NULL for aggregate results (counts/sums/totals), use COALESCE(..., 0) and state that choice in reasoning. Do NOT wrap aggregates with COALESCE if that would change a requested NULL result — mention that change explicitly.\n",
      "\n",
      "E. Joins, LEFT JOIN vs INNER JOIN decisions\n",
      "- If the prompt asks to include parent rows that may have no children (e.g., list every member and their counts including zero), use LEFT JOIN + conditional aggregation and state that choice.\n",
      "- If the prompt asks for only rows that have matches, use INNER JOIN or filter on the activity table.\n",
      "- Be explicit in reasoning when either approach could be valid and why you chose one.\n",
      "\n",
      "F. Anti-joins and \"none\" semantics\n",
      "- Prefer NOT EXISTS (correlated subquery) for anti-joins when appropriate (works across dialects).\n",
      "- LEFT JOIN ... WHERE child.key IS NULL is acceptable and often safer than NOT IN if child key may contain NULLs. If you choose NOT IN, note NULL-sensitivity in reasoning.\n",
      "- Explicitly state the interpretation you used for \"have not done X\" (e.g., no records at all vs none in a specific subset).\n",
      "\n",
      "G. Case-insensitive matches and NULLs\n",
      "- For case-insensitive string matches use LOWER(column) LIKE '%term%' and state that in reasoning.\n",
      "- Always quote string/date literals with single quotes.\n",
      "- Handle NULLs carefully: if you convert NULL aggregates to 0, say so in reasoning.\n",
      "\n",
      "H. Dates and times\n",
      "- Use date literals in the same format as the schema sample (prefer 'YYYY-MM-DD').\n",
      "- If you use relative date arithmetic (e.g., \"past month\"), use standard SQL constructs if possible and note dialect specifics in reasoning (e.g., INTERVAL '1 month' is standard-ish; MySQL uses DATE_SUB()).\n",
      "- If the prompt is ambiguous about inclusive/exclusive bounds, state your chosen interpretation in reasoning.\n",
      "\n",
      "I. Top-N and ordering\n",
      "- Use ORDER BY ... DESC and LIMIT n (or FETCH FIRST n ROWS ONLY for standard SQL). If you use LIMIT, mention it's widely supported; if you use FETCH, mention it's standard SQL.\n",
      "\n",
      "J. When schema is missing required tables/columns\n",
      "- Do not invent tables/columns. If the prompt needs data not present, either:\n",
      "  - explicitly state the assumption you will make and then produce SQL that uses that assumption, or\n",
      "  - ask the user for clarification (briefly) in the reasoning.\n",
      "- If you assume ids or values, mention your chosen values and why.\n",
      "\n",
      "K. Single-statement and non-extraneous output\n",
      "- Provide only one SQL statement after the reasoning header. Do not include multiple alternatives or extraneous SQL.\n",
      "- Avoid extra commentary, examples, or explanations outside the concise reasoning paragraph.\n",
      "\n",
      "Common SQL patterns you should use when relevant (and mention briefly in reasoning when you apply them):\n",
      "- Aggregation: SELECT SUM(col) AS total FROM table WHERE ...;\n",
      "- Conditional aggregation: SUM(CASE WHEN condition THEN value ELSE 0 END) or COUNT(CASE WHEN condition THEN 1 END);\n",
      "- Include unmatched parent rows: FROM parent LEFT JOIN child ON ... GROUP BY parent.id;\n",
      "- Anti-join: WHERE NOT EXISTS (SELECT 1 FROM child c WHERE c.parent_id = p.id ...);\n",
      "- Percentages: ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) AS percentage;\n",
      "- Case-insensitive search: LOWER(name) LIKE '%term%';\n",
      "- Defensive null-to-zero: COALESCE(SUM(...), 0);\n",
      "- Top N: ORDER BY aggregate_col DESC LIMIT N (or FETCH FIRST N ROWS ONLY).\n",
      "\n",
      "Error-prone pitfalls to avoid (and mention if you considered them):\n",
      "- Omitting a WHERE/GROUP BY/HAVING clause the user required, or adding extra WHERE filters that change the result.\n",
      "- Using LEFT JOIN + conditional aggregation when the prompt required only matching rows (or vice versa).\n",
      "- Filtering on the wrong table when authoritative data lives in a joined table.\n",
      "- Changing the result shape unexpectedly (returning a scalar when the prompt asked for per-group rows).\n",
      "- Returning NULL from aggregates where the user likely expects 0 — handle with COALESCE only when appropriate and mention it.\n",
      "\n",
      "Behavior about assumptions and ambiguity:\n",
      "- If the prompt is ambiguous, state the chosen interpretation explicitly in the reasoning and proceed.\n",
      "- If the schema sample suggests obvious id/value choices (e.g., only one project has id=2 and prompt names \"Green Building\"), you may assume that mapping but must state the assumption.\n",
      "- Keep assumptions minimal and explicit.\n",
      "\n",
      "Concise reasoning style:\n",
      "- 1–4 sentences total.\n",
      "- Mention approach, key SQL techniques used (e.g., conditional aggregation, LEFT JOIN, NOT EXISTS), and any critical assumptions or dialect notes.\n",
      "- If you intentionally changed a possible output (e.g., COALESCE to return 0), call that out.\n",
      "\n",
      "Examples of unacceptable behaviors (you must not do these):\n",
      "- Returning multiple SQL statements when the prompt did not ask for them.\n",
      "- Changing user's semantics silently (e.g., replacing NULL with 0 without saying so).\n",
      "- Selecting columns that don't exist in sql_context or inventing tables/columns silently.\n",
      "- Giving long essays instead of a short reasoning paragraph.\n",
      "\n",
      "Be precise, conservative, and explicit: preserve semantics, minimize assumptions, and state those you do make. Follow the exact output format: \"reasoning\" then concise paragraph, blank line, \"sql\" then a single SQL statement.\n"
     ]
    }
   ],
   "source": [
    "print(optimized_program.predict.signature.instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e85afb4",
   "metadata": {},
   "source": [
    "# Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64245ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datasets import Dataset\n",
    "from time import perf_counter\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "def evaluate_program(\n",
    "    program,\n",
    "    ds_test: Dataset,\n",
    "    limit: int = 100,\n",
    "    max_workers: int = 8,\n",
    "    field_map: Optional[Dict[str, str]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate a DSPy program on the first `limit` rows of a HF Dataset split.\n",
    "\n",
    "    Args:\n",
    "        program: a DSPy Module with signature program(sql_prompt=..., sql_context=...)\n",
    "        ds_test: Hugging Face Dataset (e.g., ds[\"test\"])\n",
    "        limit: number of rows to evaluate (default 100)\n",
    "        max_workers: parallel threads for I/O-bound LM + judge\n",
    "        field_map: optional mapping if your column names differ:\n",
    "                   {\"sql_prompt\": \"...\", \"sql_context\": \"...\", \"sql\": \"...\"}\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"accuracy\": float,\n",
    "          \"correct\": int,\n",
    "          \"total\": int,\n",
    "          \"avg_latency_s\": float,\n",
    "          \"failures\": [ {idx, reason, pred_sql, feedback} ... up to 20 ],\n",
    "        }\n",
    "    \"\"\"\n",
    "    if field_map is None:\n",
    "        field_map = {\"sql_prompt\": \"sql_prompt\", \"sql_context\": \"sql_context\", \"sql\": \"sql\"}\n",
    "\n",
    "    ds_test = ds_test.shuffle()\n",
    "    n = min(limit, len(ds_test))\n",
    "    subset = ds_test.select(range(n))\n",
    "    start = perf_counter()\n",
    "\n",
    "    def _eval_one(i_row):\n",
    "        i, row = i_row\n",
    "        try:\n",
    "            pred = program(\n",
    "                sql_prompt=row[field_map[\"sql_prompt\"]],\n",
    "                sql_context=row[field_map[\"sql_context\"]],\n",
    "            )\n",
    "            pred_sql = getattr(pred, \"sql\", None) or (pred.get(\"sql\") if isinstance(pred, dict) else None) or \"\"\n",
    "            jr = judge(\n",
    "                sql_context=row[field_map[\"sql_context\"]],\n",
    "                sql_prompt=row[field_map[\"sql_prompt\"]],\n",
    "                golden_sql=row[field_map[\"sql\"]],\n",
    "                candidate_sql=pred_sql,\n",
    "            )\n",
    "            ok = bool(getattr(jr, \"similar\", False))\n",
    "            feedback = getattr(jr, \"reasoning\", \"\") or \"\"\n",
    "            return (i, ok, pred_sql, feedback, None)\n",
    "        except Exception as e:\n",
    "            return (i, False, \"\", \"\", f\"{type(e).__name__}: {e}\")\n",
    "\n",
    "    results = []\n",
    "    # Threaded evaluation (I/O bound: LM + judge). Tune max_workers to your provider limits.\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futures = [ex.submit(_eval_one, (i, subset[i])) for i in range(n)]\n",
    "        for f in as_completed(futures):\n",
    "            results.append(f.result())\n",
    "\n",
    "    # Sort back to input order\n",
    "    results.sort(key=lambda x: x[0])\n",
    "\n",
    "    correct = sum(1 for _, ok, *_ in results if ok)\n",
    "    total = n\n",
    "    acc = correct / total if total else 0.0\n",
    "    elapsed = perf_counter() - start\n",
    "    avg_lat = elapsed / total if total else 0.0\n",
    "\n",
    "    failures = []\n",
    "    for i, ok, pred_sql, feedback, err in results:\n",
    "        if not ok and len(failures) < 20:\n",
    "            failures.append({\n",
    "                \"idx\": i,\n",
    "                \"reason\": (\"error: \" + err) if err else \"mismatch\",\n",
    "                \"pred_sql\": pred_sql,\n",
    "                \"feedback\": feedback,\n",
    "            })\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"correct\": correct,\n",
    "        \"total\": total,\n",
    "        \"avg_latency_s\": avg_lat,\n",
    "        \"failures\": failures,\n",
    "    }\n",
    "    \n",
    "test_split = ds[\"test\"]\n",
    "test_split = test_split.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525c8c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 0.634 (317/500)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate original and optimized on ds[\"test\"][:100]\n",
    "\n",
    "orig_metrics = evaluate_program(program, test_split, limit=500, max_workers=32)\n",
    "\n",
    "print(\"Original:\", orig_metrics[\"accuracy\"], f\"({orig_metrics['correct']}/{orig_metrics['total']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a26d5116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized: 0.646 (323/500)\n"
     ]
    }
   ],
   "source": [
    "opt_metrics  = evaluate_program(optimized_program, test_split, limit=500, max_workers=32)\n",
    "print(\"Optimized:\", opt_metrics[\"accuracy\"], f\"({opt_metrics['correct']}/{opt_metrics['total']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7677894b",
   "metadata": {},
   "source": [
    "# OpenAI optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "882d8766",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIOptimized(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Developer: # Role and Objective\n",
    "    You are a SQL expert assistant. For every request, you will receive two inputs:\n",
    "    - `sql_prompt`: a concise natural-language description of the SQL the user wants.\n",
    "    - `sql_context`: a small SQL schema and seed data (CREATE TABLE/INSERT statements) that define available tables, columns, and example values.\n",
    "\n",
    "    # Instructions\n",
    "    Begin with a concise checklist (3–7 bullets) outlining your planned approach: key sub-tasks or decision points for producing the SQL statement. Keep these conceptual, not implementation-level. For every incoming request, produce your response in the following exact format (and nothing else):\n",
    "    1. **reasoning**\n",
    "    - A short (1–4 sentences) paragraph explaining your approach, key assumptions, handling of edge cases, and any noteworthy decisions (e.g., use of COALESCE, case-insensitive matching, dialect-specific features). Keep this concise.\n",
    "\n",
    "    2. **sql**\n",
    "    - The SQL statement that fulfills the prompt using only the context provided. Start this section with the word `sql` on its own line, followed by the query. Only output a single SQL statement unless the prompt explicitly requests multiple statements.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    reasoning\n",
    "    <Concise, 1–4 sentence paragraph with approach and key decisions.>\n",
    "\n",
    "    sql\n",
    "    <Single SQL statement>\n",
    "\n",
    "    ---\n",
    "\n",
    "    # Core Rules, Constraints, and Style\n",
    "\n",
    "    A. **Preserve user semantics strictly**\n",
    "    - Do not add, remove, or modify required filters, joins, grouping, or result structures.\n",
    "    - Deviate only if necessary due to missing schema elements or ambiguity—explicitly state any such assumptions in reasoning.\n",
    "    - Preserve output shape as implied by the prompt (e.g., single scalar vs. grouped rows).\n",
    "\n",
    "    B. **Use standard and portable SQL where possible**\n",
    "    - Prefer ANSI SQL features. If using dialect-specific syntax (e.g., INTERVAL, DATE_SUB), mention it in reasoning.\n",
    "\n",
    "    C. **DML Statements**\n",
    "    - For INSERT: always use explicit column lists unless the schema provides explicit column order and prompt implies omission.\n",
    "    - For UPDATE/DELETE: exactly mirror the WHERE clauses the user specifies.\n",
    "    - If the prompt requests multiple dependent statements (e.g., insert parent and then child row), either:\n",
    "    - Produce a single statement if possible,\n",
    "    - State assumptions (e.g., chosen IDs), or\n",
    "    - Ask for user clarification in reasoning if no safe assumption can be made.\n",
    "\n",
    "    D. **Aggregation, grouping, and expected result shape**\n",
    "    - Use GROUP BY and HAVING only as the prompt implies.\n",
    "    - Use COALESCE(..., 0) for aggregates only if the user is likely to expect zero instead of NULL, and note this in reasoning.\n",
    "\n",
    "    E. **Joins**\n",
    "    - Use LEFT JOIN when prompted for parent rows that may have no children; otherwise, use INNER JOIN. Justify your choice in reasoning if ambiguous.\n",
    "\n",
    "    F. **Anti-joins**\n",
    "    - Prefer NOT EXISTS for anti-join semantics; use LEFT JOIN ... WHERE child.key IS NULL where appropriate. Note potential NULL handling issues in reasoning.\n",
    "\n",
    "    G. **Case-insensitive matches and NULL handling**\n",
    "    - Implement case-insensitive matches using LOWER(column) LIKE '%term%', stating this in reasoning.\n",
    "    - Quote string/date literals with single quotes.\n",
    "    - Mention if NULLs are handled/converted by your SQL.\n",
    "\n",
    "    H. **Dates and times**\n",
    "    - Use the schema’s date literal format (prefer 'YYYY-MM-DD').\n",
    "    - For relative dates or ambiguous ranges, state your interpretation clearly.\n",
    "\n",
    "    I. **Top-N and ordering**\n",
    "    - Use ORDER BY ... DESC LIMIT n (or FETCH FIRST n ROWS ONLY for ANSI SQL). Mention your choice in reasoning if non-standard.\n",
    "\n",
    "    J. **Missing schema requirements**\n",
    "    - Never invent tables or columns. If critical elements are missing, either:\n",
    "    - Explicitly state your assumption and proceed, or\n",
    "    - Ask for user clarification in reasoning.\n",
    "\n",
    "    K. **Output restrictions**\n",
    "    - Only emit a single SQL statement (unless multiple are explicitly requested), and never output transaction commands, alternative queries, or extraneous commentary.\n",
    "    - Keep reasoning concise, informative, and within 1–4 sentences.\n",
    "\n",
    "    # Common SQL Patterns to Use (as appropriate)\n",
    "    - Aggregation: `SELECT SUM(col) AS total FROM table WHERE ...;`\n",
    "    - Conditional aggregation: `SUM(CASE WHEN condition THEN value ELSE 0 END)`\n",
    "    - Including unmatched parents: `FROM parent LEFT JOIN child ... GROUP BY parent.id`\n",
    "    - Anti-join: `WHERE NOT EXISTS (SELECT 1 FROM child ... )`\n",
    "    - Percentages: `ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2)`\n",
    "    - Case-insensitive search: `LOWER(name) LIKE '%term%'`\n",
    "    - Null-to-zero: `COALESCE(SUM(...), 0)`\n",
    "    - Top-N: `ORDER BY ... DESC LIMIT N`\n",
    "\n",
    "    # Explicit Pitfalls to Avoid\n",
    "    - Never omit essential WHERE, GROUP BY, or HAVING clauses.\n",
    "    - Do not change result shape (e.g., returning a scalar when a multi-row result is expected).\n",
    "    - Never return multiple SQL statements, unless explicitly requested.\n",
    "    - Never invent missing schema elements—justify any assumed values in reasoning.\n",
    "\n",
    "    # Behavior on Ambiguity\n",
    "    - If the user request or schema is ambiguous, clearly state your interpretation in reasoning and proceed accordingly.\n",
    "    - Limit assumptions to what is evident; document all assumptions.\n",
    "\n",
    "    # Output Format\n",
    "    - Adhere strictly to: \"reasoning\" (1–4 sentences explaining approach and choices), then a blank line, then \"sql\" followed by the single SQL statement.\n",
    "\n",
    "    # Verbosity\n",
    "    - Default mode: concise, clear, and explicit parsing of requirements in reasoning.\n",
    "    - For code: maintain clarity—use readable structure, explicit references, and comments when appropriate.\n",
    "\n",
    "    # Stop Conditions\n",
    "    - Hand back output immediately after producing the reasoning and SQL sections in exact format. Do not add further commentary or suggestions.\n",
    "\n",
    "    After forming the SQL statement, briefly validate that the chosen SQL matches all explicit user requirements and schema details. If issues are found, correct and update your response accordingly. If requirements are ambiguous or cannot be met, explicitly state this in reasoning and request clarification.\n",
    "    \"\"\"\n",
    "\n",
    "    sql_context: str = dspy.InputField(description=\"SQL queries for creating the table(s) and loading some data\")\n",
    "    sql_prompt: str = dspy.InputField(description=\"User's natural language prompt\")\n",
    "    sql: str = dspy.OutputField(description=\"SQL query that delivers on the user's request. Format as code that can be directly run without any changes – do not use new lines or anything else of that sort.\")\n",
    "\n",
    "openai_optimized = dspy.ChainOfThought(OpenAIOptimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df4bbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 11:23:54 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:23:55 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:23:55 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:23:57 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:23:59 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:23:59 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:00 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:02 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:07 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:09 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:10 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:10 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:12 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:13 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:14 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:15 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:15 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:16 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:16 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:16 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:17 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:17 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:19 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:20 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:20 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:20 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:22 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:24 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:25 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:26 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:28 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:28 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:29 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:30 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:31 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:31 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:33 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:33 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:33 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:35 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:36 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:36 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:36 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:36 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:36 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:37 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:37 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:38 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:38 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:38 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:38 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:39 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:41 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:41 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:41 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:44 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:45 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:46 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:47 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:48 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:48 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:50 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:50 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:51 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:52 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:53 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:53 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:53 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:54 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:55 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:57 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:57 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:57 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:57 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:57 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:58 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:58 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:58 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:58 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:59 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:24:59 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:25:00 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:25:01 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:25:01 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:25:02 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:25:04 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:25:06 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:25:06 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:25:07 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:25:09 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 11:25:15 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized: 0.542 (271/500)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate original and optimized on ds[\"test\"][:100]\n",
    "orig_metrics = evaluate_program(openai_optimized, test_split, limit=500, max_workers=32)\n",
    "\n",
    "print(\"Optimized:\", orig_metrics[\"accuracy\"], f\"({orig_metrics['correct']}/{orig_metrics['total']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb90135",
   "metadata": {},
   "source": [
    "# Store programs and run evals again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45de6ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "program.save(\"./dspy_program/program.json\", save_program=False)\n",
    "optimized_program.save(\"./optimized_program/program.json\", save_program=False)\n",
    "openai_optimized.save(\"./openai_optimized_program/program.json\", save_program=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3180ca",
   "metadata": {},
   "source": [
    "## Reload programs and run evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ca6e0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Program: {'accuracy': 0.584, 'correct': 292, 'total': 500, 'avg_latency_s': 0.535394384418003, 'failures': [{'idx': 0, 'reason': 'mismatch', 'pred_sql': \"SELECT (SELECT COUNT(*) FROM disability_services.students WHERE accommodation IS NOT NULL AND TRIM(accommodation) <> '' AND LOWER(TRIM(accommodation)) <> 'accessibility_parking') + (SELECT COUNT(*) FROM disability_services.staff WHERE accommodation IS NOT NULL AND TRIM(accommodation) <> '' AND LOWER(TRIM(accommodation)) <> 'accessibility_parking') AS total_with_accommodations;\", 'feedback': \"The two queries are not functionally equivalent.\\n\\n- Output shape: The golden SQL uses UNION of two COUNT(*) queries, returning two rows (one count for staff, one for students). The candidate returns a single scalar (the sum of the two counts) as total_with_accommodations — matching the prompt's wording for a single total, but different from the golden's two-row result.\\n- Filtering differences: The golden WHERE clause only excludes rows where accommodation = 'accessibility_parking' (and implicitly excludes NULLs because comparisons evaluate to NULL). The candidate additionally excludes NULLs, empty or whitespace-only strings (TRIM(...) <> ''), and performs a case-insensitive comparison (LOWER(... ) <> 'accessibility_parking'). Those extra filters change which rows are counted compared to the golden.\\n- Because of both the differing result shape (two rows vs one summed total) and the stricter filtering in the candidate, they are not functionally similar.\"}, {'idx': 1, 'reason': 'mismatch', 'pred_sql': \"SELECT id,name,country,amount_donated FROM contributors WHERE country = 'Kenya' ORDER BY amount_donated DESC LIMIT 3;\", 'feedback': \"The two queries are not functionally equivalent.\\n\\nDifferences:\\n- Projection: the golden query returns only the name column; the candidate returns id, name, country, and amount_donated.\\n- Filtering: the candidate adds WHERE country = 'Kenya', restricting results to Kenya. The golden query has no WHERE and returns top donors across all countries.\\n- Although both use ORDER BY amount_donated DESC LIMIT 3, the WHERE causes the candidate to produce a different (and smaller/subset) result set. Because of the different filter and different selected columns, the candidate will not generally return the same results as the golden query.\\n\\nTherefore they are not similar.\"}, {'idx': 6, 'reason': 'mismatch', 'pred_sql': 'SELECT DISTINCT bs.stop_name, sr.route_number FROM madrid.bus_stops bs LEFT JOIN madrid.stop_routes sr ON bs.id = sr.stop_id ORDER BY bs.stop_name, sr.route_number;', 'feedback': \"The candidate uses a LEFT JOIN instead of the INNER JOIN in the golden query. That means the candidate will include bus stops that have no matching route (route_number = NULL), whereas the golden query returns only stops that have routes. The ORDER BY in the candidate only affects result ordering and doesn't make the queries equivalent. Because the join behavior differs and can change the result set, the queries are not functionally the same.\"}, {'idx': 8, 'reason': 'mismatch', 'pred_sql': \"SELECT omp.project, SUM(sb.biomass) AS total_biomass FROM OceanMappingProjects AS omp JOIN SpeciesBiomass AS sb ON omp.species = sb.species WHERE omp.project IN ('OceanMappingProjectA','OceanMappingProjectB') GROUP BY omp.project;\", 'feedback': \"The golden query returns a single scalar: the combined SUM of biomass for species in both projects ('OceanMappingProjectA' and 'OceanMappingProjectB'). The candidate returns per-project sums (one row per project with project name and SUM), not a single combined total. The outputs differ in aggregation and shape (single total vs. grouped totals), so they are not functionally equivalent to the golden SQL.\"}, {'idx': 14, 'reason': 'mismatch', 'pred_sql': \"SELECT investor_type, SUM(amount) AS total_amount FROM renewable_energy_projects WHERE sector='Renewable Energy' AND region='Africa' AND investor_type IN ('Public','Private') GROUP BY investor_type UNION ALL SELECT 'Total' AS investor_type, SUM(amount) FROM renewable_energy_projects WHERE sector='Renewable Energy' AND region='Africa' AND investor_type IN ('Public','Private');\", 'feedback': \"The golden SQL returns a single scalar: the SUM(amount) for all rows where sector='Renewable Energy' and region='Africa'. \\n\\nThe candidate SQL:\\n- Filters only investor_type IN ('Public','Private') (golden does not filter by investor_type).\\n- Returns multiple rows: one SUM per investor_type (grouped) plus an extra 'Total' row with the combined SUM.\\n- Produces a different result shape (multiple rows and an investor_type column) vs. a single scalar.\\n\\nBecause the candidate applies an additional filter and changes the output shape (and would omit other investor_types if present), it is not functionally equivalent to the golden query.\"}, {'idx': 18, 'reason': 'mismatch', 'pred_sql': 'SELECT AVG(budget) AS average_budget FROM company_tech;', 'feedback': \"The golden query restricts the average to specific company names ('TechCo', 'EthicalAI', 'GoodTech') and excludes non-positive budgets (budget > 0). The candidate query averages the entire table without any WHERE clause, so it does not enforce those name or budget filters. Even if the current data contains only those three rows, the candidate is not functionally equivalent because it would produce different results if other companies or non-positive budgets were present. Therefore they are not functionally similar.\"}, {'idx': 20, 'reason': 'mismatch', 'pred_sql': \"SELECT s.store_id, s.store_name, ROUND(AVG(p.price),2) AS average_price FROM Stores s JOIN Inventory i ON s.store_id = i.store_id JOIN Products p ON i.product_id = p.product_id WHERE p.is_free_range = TRUE AND LOWER(p.product_name) LIKE '%egg%' AND i.quantity > 0 GROUP BY s.store_id, s.store_name;\", 'feedback': \"The two queries share the intent (average price of free-range egg products per store) and both filter on p.is_free_range = TRUE and join the same tables, but they differ in ways that change results:\\n\\n- Egg identification: golden uses p.product_category = 'egg' while candidate uses LOWER(p.product_name) LIKE '%egg%'. These are different predicates and can match different sets of products.\\n- Quantity filter: candidate adds i.quantity > 0, excluding inventory rows with zero quantity; golden does not. This can change averages.\\n- Grouping/selection: golden groups by store_name only; candidate groups by store_id and store_name and also returns store_id. If store_name is not unique across stores, the golden query would aggregate across same names while candidate would not.\\n- Minor formatting: candidate rounds AVG and aliases differently — these do not affect core logic but are additional differences.\\n\\nBecause of the different product-matching predicate and the extra quantity filter (and grouping difference), the candidate is not functionally equivalent to the golden query.\"}, {'idx': 21, 'reason': 'mismatch', 'pred_sql': \"SELECT m.menu_id, m.name, COALESCE(m.co2_emission,0) + COALESCE(ing.total_ingredient_emission,0) AS total_co2_emission, COALESCE(ing.origin_breakdown,'') AS origin_breakdown FROM menu_items m LEFT JOIN ( SELECT x.menu_id, SUM(x.emission_by_origin) AS total_ingredient_emission, GROUP_CONCAT(CONCAT(x.origin,':',ROUND(x.emission_by_origin,3)) SEPARATOR '; ') AS origin_breakdown FROM ( SELECT r.menu_id, i.origin, SUM(r.quantity * i.co2_emission_per_kg) AS emission_by_origin FROM recipe r JOIN ingredients i ON r.ingredient_id = i.ingredient_id GROUP BY r.menu_id, i.origin ) x GROUP BY x.menu_id ) ing ON m.menu_id = ing.menu_id;\", 'feedback': 'The candidate and golden queries both compute the sum of ingredient emissions (SUM(r.quantity * i.co2_emission_per_kg)) per menu item — the candidate even aggregates by ingredient origin first and then sums, which is equivalent for the ingredient contribution. However, the candidate query differs in important ways:\\n\\n- It adds m.co2_emission to the computed ingredient total (COALESCE(m.co2_emission,0) + ...). The gold query does not include any menu_items.co2_emission column in the total.\\n- The candidate uses a LEFT JOIN to include menu items without recipe rows and returns extra columns (menu_id and an origin_breakdown via GROUP_CONCAT), while the gold query returns only name and total_co2_emission and uses inner joins.\\nBecause the candidate changes the total by including m.co2_emission and returns different output, it is not functionally equivalent to the golden SQL.'}, {'idx': 25, 'reason': 'mismatch', 'pred_sql': \"SELECT strftime('%Y-%m',SaleDate) AS month, COUNT(*) AS total_sales_count, NULL AS total_monthly_revenue FROM Sales WHERE Category='Organic' GROUP BY month ORDER BY month;\", 'feedback': \"The two queries are not functionally similar.\\n\\n- The golden SQL returns a single scalar by summing a boolean expression that effectively counts rows for Category='Organic' within January 2022 (it applies a date filter inside SUM).\\n- The candidate SQL returns one row per month (strftime grouped by month) with a COUNT of rows per month and an explicit NULL column labeled total_monthly_revenue. It does not filter to January 2022 and does not compute any revenue (the revenue column is NULL).\\n- They differ in result shape (single scalar vs. multiple rows), filtering (golden restricts to a specific month; candidate does not), and intent (golden counts for a month; candidate lists monthly counts and omits revenue).\\n\\nTherefore they are not equivalent.\"}, {'idx': 26, 'reason': 'mismatch', 'pred_sql': \"DELETE FROM satellite_images WHERE farm_id = 501 AND timestamp >= '2022-05-03' AND timestamp < '2022-05-04';\", 'feedback': \"The golden query deletes a single row with an exact timestamp '2022-05-03 14:30:00'. The candidate query deletes all rows for farm_id = 501 whose timestamp falls anywhere on 2022-05-03 (i.e., the entire day). That is a broader operation and may remove multiple images (or different image times) rather than the single specific timestamp in the golden SQL. They are not functionally equivalent unless you know for certain there is only one image on that date; without that guarantee the candidate is not the same as the golden query.\"}, {'idx': 30, 'reason': 'mismatch', 'pred_sql': \"SELECT GrantName, SUM(Amount) AS TotalAmount FROM Grants WHERE Sector = 'Education' GROUP BY GrantName ORDER BY TotalAmount DESC LIMIT 3;\", 'feedback': 'The two queries are not functionally similar.\\n\\nDifferences:\\n- The golden SQL selects individual grant rows in the Education sector ordered by Amount (using ROW_NUMBER and LIMIT 3), so it returns the top 3 single grant records by Amount.\\n- The candidate SQL aggregates by GrantName (SUM(Amount)) and returns the top 3 recipients by their total received amount in Education. That changes the semantics when a recipient has multiple grants: the candidate can return a recipient whose individual grants are smaller but total larger, whereas the golden SQL would not aggregate and only returns the largest individual grants.\\n- The returned columns differ (golden returns Sector, Amount, Rank; candidate returns TotalAmount), and the ordering/partitioning logic is different.\\n\\nBecause of these semantic and result differences, they are not equivalent.'}, {'idx': 31, 'reason': 'mismatch', 'pred_sql': \"SELECT SUM(tb.token_balance) AS total_tokens FROM smart_contracts sc JOIN token_balances tb ON sc.id = tb.smart_contract_id WHERE sc.name = 'AAVE';\", 'feedback': \"The candidate SQL joins smart_contracts to token_balances and returns SUM(token_balance) as total_tokens (an aggregated single-row result). The golden SQL selects token_balance directly from token_balances using a subquery to find the contract id (returning the raw balance rows, not an aggregate). While both target the same contract 'AAVE' and will yield the same numeric value in this specific dataset (only one token_balances row), they are not functionally equivalent in general: the candidate returns an aggregate sum (one row) and the golden returns raw row(s) (potentially multiple rows) and uses a different projection/column name. Therefore they are not similar.\"}, {'idx': 32, 'reason': 'mismatch', 'pred_sql': \"SELECT airline, COUNT(*) AS flights_last_month FROM FlightOperations WHERE operated_date >= date_trunc('month', current_date) - INTERVAL '1 month' AND operated_date < date_trunc('month', current_date) GROUP BY airline ORDER BY flights_last_month DESC;\", 'feedback': 'The two queries use different date ranges:\\n\\n- Golden SQL: WHERE operated_date >= DATEADD(month, -1, GETDATE()) — selects rows from one month ago up to now (a rolling/relative \"last month\" period ending at the current datetime).\\n- Candidate SQL: WHERE operated_date >= date_trunc(\\'month\\', current_date) - INTERVAL \\'1 month\\' AND operated_date < date_trunc(\\'month\\', current_date) — selects only the entire previous calendar month (from the first day of last month up to but not including the first day of the current month).\\n\\nBecause the time windows differ (rolling past-30/31-days vs. fixed previous calendar month), the results can be different. The candidate also adds an ORDER BY and an alias, which do not make them equivalent to the golden query.\\n\\nTherefore they are not functionally similar.'}, {'idx': 34, 'reason': 'mismatch', 'pred_sql': \"SELECT accommodation_date, accommodation_type FROM accommodation WHERE accommodation_type = 'Extended Testing Time' ORDER BY accommodation_date ASC LIMIT 1;\", 'feedback': \"The candidate query is not functionally similar to the golden query.\\n\\nDifferences:\\n- The golden SQL restricts to students with disability = 'Learning Disability' (via student table) and computes the minimum accommodation_date grouped by accommodation_type (earliest date per accommodation type for those students).\\n- The candidate SQL filters only for accommodation_type = 'Extended Testing Time' (hard-coded), ignores any student disability filter, and returns only a single row (the earliest date for that one accommodation type across all students).\\n- The candidate does not perform grouping/aggregation per accommodation_type nor apply the required student-level filter. Column order/alias also differ, but the semantic differences above are the key reasons they are not equivalent.\\n\\nTherefore, not similar.\"}, {'idx': 35, 'reason': 'mismatch', 'pred_sql': \"SELECT COUNT(*) AS habitats_over_50000_hectares FROM wildlife_habitats WHERE LOWER(country) = 'south africa' AND area > 50000;\", 'feedback': \"The two queries are not functionally equivalent.\\n\\n- The golden SQL groups by country and uses HAVING SUM(area) > 50000. It checks whether the total area of all habitats in South Africa exceeds 50000, and if so returns the count of all South Africa habitats (COUNT(*) for that group). If the sum is not > 50000 the query yields no rows.\\n- The candidate SQL applies a per-row filter AND area > 50000, so it counts only habitats whose individual area is greater than 50000. This is a different condition than the golden query (per-habitat vs. total across the country).\\n- Additionally, the candidate uses LOWER(country) = 'south africa' (case-insensitive matching), while the golden uses country = 'South Africa' (case-sensitive depending on DB). That is a minor difference but does not reconcile the main logical difference.\\n\\nBecause the core aggregation/ HAVING logic is different from the per-row filter, the queries produce different results in general.\"}, {'idx': 36, 'reason': 'mismatch', 'pred_sql': 'SELECT id, name, type FROM shariah_compliant_products;', 'feedback': 'Both queries select from the same view (so they return the same set of rows/products), but the golden query returns only the product name while the candidate returns additional columns (id and type). The user asked to \"List all Shariah-compliant financial products\" (names), so the candidate returns extra/unrequested fields and is not functionally equivalent to the golden query.'}, {'idx': 37, 'reason': 'mismatch', 'pred_sql': 'SELECT id, site_name, COUNT(*) AS artifact_count FROM european_sites GROUP BY id, site_name HAVING COUNT(*) > 15;', 'feedback': 'The candidate is not functionally equivalent to the golden query.\\n\\nReasons:\\n- The golden query groups only by site_name and returns site_name values where the count of artifacts per site_name is > 15, which matches the prompt.\\n- The candidate groups by id and site_name. If id is not the site identifier (or is unique per row), grouping by id changes the aggregation granularity and will produce different results (likely no groups > 15). Even if id is a site identifier, grouping by both id and site_name is redundant but still different in projection.\\n- The candidate also returns id and the artifact_count column, not just site_name as the prompt requests.\\n\\nBecause grouping by id can change which groups are counted and the output columns differ, the queries are not functionally similar in general.'}, {'idx': 40, 'reason': 'mismatch', 'pred_sql': 'SELECT ConditionID, MIN(Age) AS MinAge, MAX(Age) AS MaxAge FROM PatientAges GROUP BY ConditionID ORDER BY ConditionID;', 'feedback': 'The golden query returns the condition name (Conditions.Condition) and aggregates MIN and MAX ages per condition by joining PatientAges to the Conditions table. The candidate query aggregates MIN and MAX ages per ConditionID from PatientAges but does not join to the Conditions table and thus returns ConditionID rather than the condition name. Because the selected grouping column (Condition name vs ConditionID) and the join to get names differ, the two queries are not functionally equivalent to the user\\'s requested output (which asks \"by condition\" implying condition names). Therefore they are not similar.'}, {'idx': 42, 'reason': 'mismatch', 'pred_sql': \"SELECT * FROM defense_diplomacy WHERE ((country1 = 'China' AND country2 = 'India') OR (country1 = 'India' AND country2 = 'China')) AND year BETWEEN 2005 AND 2010;\", 'feedback': \"The two queries differ in how the year filter is applied because of parentheses/operator precedence.\\n\\n- Golden SQL: WHERE (country1 = 'China' AND country2 = 'India') OR (country1 = 'India' AND country2 = 'China') AND year BETWEEN 2005 AND 2010;\\n  Due to SQL precedence (AND before OR), this is equivalent to:\\n  (country1 = 'China' AND country2 = 'India')\\n  OR\\n  ((country1 = 'India' AND country2 = 'China') AND year BETWEEN 2005 AND 2010)\\n  So the year condition only applies to the second clause. Rows with country1='China' AND country2='India' would be returned regardless of year.\\n\\n- Candidate SQL: WHERE ((country1 = 'China' AND country2 = 'India') OR (country1 = 'India' AND country2 = 'China')) AND year BETWEEN 2005 AND 2010;\\n  Here the parentheses force the OR to be evaluated first, and the year BETWEEN ... applies to both directions. This matches the natural-language requirement.\\n\\nTherefore the candidate and golden queries are not functionally equivalent (they can return different result sets, e.g., a China→India row from year 2000 would be returned by the golden query but not by the candidate).\"}, {'idx': 43, 'reason': 'mismatch', 'pred_sql': \"SELECT COALESCE(SUM(f.amount),0) AS total_funding FROM funding f WHERE f.company_id IN (SELECT DISTINCT company_id FROM founders WHERE LOWER(gender) = 'female');\", 'feedback': 'The two queries differ in behavior:\\n\\n- The golden SQL sums funding rows after joining funding to founders directly: JOIN funding ON company_id and WHERE founders.gender = \\'female\\'. If a company has multiple female founders, the same funding row will be repeated once per matching founder, causing the sum to be duplicated/overcounted.\\n- The candidate SQL sums funding for companies whose company_id appears in a subquery of founders with gender = \\'female\\' (using LOWER to allow case-insensitive matching and DISTINCT implicitly via IN). This counts each funding row only once per company and avoids the overcounting. It also uses COALESCE to return 0 when there are no matches.\\n- The candidate therefore can produce different numeric results from the golden query (and is in fact more correct for \"total funding received by female founders\" interpreted as funding for companies that have female founders). Because of the potential overcounting and the case-insensitivity/COALESCE differences, they are not functionally equivalent.'}]}\n"
     ]
    }
   ],
   "source": [
    "og_program = dspy.ChainOfThought(ProblemDef)\n",
    "og_program.load(\"./dspy_program/program.json\")\n",
    "og_metrics = evaluate_program(og_program, test_split, limit=500, max_workers=32)\n",
    "print(f\"Original Program: {og_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2fdea35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Program: {'accuracy': 0.642, 'correct': 321, 'total': 500, 'avg_latency_s': 0.9665514680820052, 'failures': [{'idx': 0, 'reason': 'mismatch', 'pred_sql': 'sql\\nSELECT country, COUNT(*) AS num_factories FROM FairTradeFactories GROUP BY country HAVING COUNT(*) = (SELECT MAX(cnt) FROM (SELECT COUNT(*) AS cnt FROM FairTradeFactories GROUP BY country) AS sub);', 'feedback': 'The golden query returns the top 5 countries by factory count (ordered descending, limited to 5). The candidate query returns only the country or countries whose count equals the overall maximum (i.e., the countries tied for the single highest count). These behaviors differ (candidate yields only the top tied maxima, not the top 5), so they are not functionally equivalent.'}, {'idx': 5, 'reason': 'mismatch', 'pred_sql': 'sql\\nSELECT o.mission_area, COUNT(d.donation_id) AS total_donations FROM Organizations o LEFT JOIN Donations d ON o.org_id = d.org_id GROUP BY o.mission_area;', 'feedback': 'The two queries are not functionally similar.\\n\\n- The golden query counts Organizations rows per mission_area (COUNT(*) FROM Organizations GROUP BY mission_area) — i.e., number of organizations in each mission_area.\\n- The candidate query attempts to count donation rows by joining to a Donations table (COUNT(d.donation_id) with a LEFT JOIN). That yields the number of donations per mission_area, not the number of organizations, and requires a Donations table (which is not present in the given context). Semantically they aggregate different things and would produce different results (and the candidate may error given the missing table). Therefore they are not equivalent.'}, {'idx': 6, 'reason': 'mismatch', 'pred_sql': 'sql\\nSELECT c.country FROM (SELECT DISTINCT country FROM national_security_incidents) c LEFT JOIN (SELECT country, SUM(incident_count) AS recent_total FROM national_security_incidents WHERE year BETWEEN (SELECT MAX(year) FROM national_security_incidents) - 2 AND (SELECT MAX(year) FROM national_security_incidents) GROUP BY country) r ON c.country = r.country WHERE COALESCE(r.recent_total, 0) = 0;', 'feedback': 'The two queries are not functionally equivalent.\\n\\nKey differences:\\n- Time window: the golden SQL uses a fixed filter \"year >= 2020\" (static last 3 years assumption), while the candidate computes the last 3 years dynamically using (MAX(year) - 2) to MAX(year). They may target different year ranges.\\n- Aggregation logic: the golden query selects countries that have at least one row in the period with incident_count = 0 (it returns countries with any zero-valued row in the period). The candidate sums incident_count across the 3-year window and requires the sum to be 0 (it returns countries with no incidents in any of the last 3 years). These produce different results when a country has a mix of zero and nonzero incident rows in the period.\\n- Presence of rows: the candidate also correctly includes countries that have no rows in the recent period (treated as zero via COALESCE). The golden query will not return countries lacking rows for year >= 2020.\\n\\nBecause of these semantic differences, they are not functionally similar.'}, {'idx': 9, 'reason': 'mismatch', 'pred_sql': 'sql\\nSELECT site, year FROM UNESCO_Heritage_Sites WHERE year >= EXTRACT(YEAR FROM CURRENT_DATE) - 4 ORDER BY year DESC;', 'feedback': \"The two queries are not functionally equivalent. Both select site and year and use equivalent functions to get the current year (YEAR(CURRENT_DATE) vs EXTRACT(YEAR FROM CURRENT_DATE)), and the candidate's ORDER BY only affects result ordering. However the candidate subtracts 4 from the current year while the golden SQL subtracts 5, so they use different year thresholds (off-by-one). That changes which rows are returned (different time window), so they are not the same.\"}, {'idx': 10, 'reason': 'mismatch', 'pred_sql': \"sql\\nSELECT COUNT(*) AS total_initiatives FROM ethical_ai_initiatives WHERE region IN ('Asia','Europe');\", 'feedback': 'The candidate SQL returns the count of initiatives in Asia and Europe (COUNT(*)), which matches the natural language prompt asking for the \"total number of ethical AI initiatives.\" The golden SQL returns the sum of the funds (SUM(funds)) for those regions, which instead computes total money, not the number of initiatives. These produce different results (e.g., there are 4 initiatives but SUM(funds)=250000.00), so they are not functionally equivalent.'}, {'idx': 13, 'reason': 'mismatch', 'pred_sql': 'sql\\nSELECT COALESCE(100.0 * SUM(CASE WHEN success THEN 1 ELSE 0 END) / NULLIF(COUNT(*), 0), 0) AS success_rate_percent FROM DiverseStartups WHERE year >= (SELECT MAX(year) FROM DiverseStartups) - 2;', 'feedback': 'Both queries compute a percent of successes among rows in a recent-year window, but they use different time-window logic and thus can produce different results:\\n\\n- Golden SQL filters WHERE year >= YEAR(CURRENT_DATE) - 3 (uses the current calendar year minus 3 as the cutoff) and divides count of successes by the count of all rows using that same cutoff. It can also error on division by zero.\\n- Candidate SQL filters WHERE year >= (SELECT MAX(year) FROM DiverseStartups) - 2 (uses the dataset\\'s maximum year minus 2 as the cutoff, i.e. a data-driven \"last 3 years\") and computes 100 * SUM(success) / COUNT(*) with COALESCE/NULLIF to avoid division-by-zero.\\n\\nBecause one uses CURRENT_DATE - 3 and the other uses MAX(year) - 2 (different offsets and different reference points), they are not functionally equivalent and can yield different results for the same data. The candidate also handles division-by-zero differently.'}, {'idx': 14, 'reason': 'mismatch', 'pred_sql': 'sql SELECT COALESCE(SUM(revenue), 0) AS total_revenue FROM hotel_revenue WHERE is_eco_friendly = true;', 'feedback': \"The candidate SQL does not include the required filter for city = 'New York'. The golden query sums revenue only for eco-friendly hotels in New York; the candidate sums revenue for all eco-friendly hotels (no city restriction) and also wraps SUM in COALESCE to return 0 if there are no rows. Because the city filter is missing (and the schema provided has no city column), the candidate is not functionally equivalent to the golden query.\"}, {'idx': 17, 'reason': 'mismatch', 'pred_sql': \"sql\\nSELECT CASE WHEN (SELECT COUNT(DISTINCT student_id) FROM Students)=0 THEN NULL ELSE ROUND(100.0 * (SELECT COUNT(DISTINCT student_id) FROM Accommodations WHERE LOWER(accommodation_type) LIKE '%mobility%')/(SELECT COUNT(DISTINCT student_id) FROM Students), 2) END AS percent_mobility;\", 'feedback': 'The two queries are not functionally equivalent.\\n\\nKey differences:\\n- Golden SQL identifies students with the disability by checking DisabilityTypes.disability_type = \\'Mobility Impairment\\' and then counts how many of those students appear in Accommodations (i.e., received any accommodation). It uses the DisabilityTypes table to define \"mobility impairment\".\\n- Candidate SQL ignores DisabilityTypes entirely and instead looks for accommodation records whose accommodation_type text contains \"mobility\" (LOWER(accommodation_type) LIKE \\'%mobility%\\'). This counts students who received an accommodation described as \"mobility...\" regardless of whether they are recorded as having a mobility impairment, and it will miss students with a mobility impairment who received accommodations not labeled with \"mobility\".\\n- Other minor differences: candidate rounds the percentage and returns NULL when there are zero students; golden does not.\\n\\nBecause the candidate uses a different condition/source to identify \"mobility impairments\" (based on accommodation_type text rather than the DisabilityTypes table), the result sets can differ, so they are not functionally similar.'}, {'idx': 19, 'reason': 'mismatch', 'pred_sql': 'sql\\nDELETE FROM hotel_reviews WHERE rating < 4 OR rating IS NULL;', 'feedback': 'The golden query deletes rows where rating < 4. The candidate deletes rows where rating < 4 OR rating IS NULL. The difference is how NULL ratings are handled: the candidate also removes rows with NULL rating, while the golden query leaves them untouched. With the provided data (no NULL ratings) both queries would have the same effect, but they are not functionally equivalent for all possible data. Because they can produce different results when NULLs are present, they are not considered functionally similar to the golden SQL.'}, {'idx': 20, 'reason': 'mismatch', 'pred_sql': 'SELECT country, COUNT(*) AS site_count FROM heritagesites GROUP BY country ORDER BY COUNT(*) DESC, country ASC LIMIT 3;', 'feedback': \"The two queries both intend to return countries and their site counts ordered by count, but they are not functionally equivalent.\\n\\nDifferences:\\n- The golden query uses a window function (COUNT(...) OVER (PARTITION BY country)) without GROUP BY, so it returns one row for each original site with the country repeated and the country-count repeated on each of that country's rows. Limiting to the first 3 rows may therefore return the same country multiple times (if that country has multiple sites).\\n- The candidate query uses GROUP BY country, returning one row per country with the aggregated count. It returns the top 3 distinct countries by site count (ties broken by country ASC due to the ORDER BY clause).\\n- Tie-breaking and row uniqueness differ (golden has no deterministic tie-breaker and may produce duplicates; candidate deterministically picks three distinct countries).\\n\\nBecause the golden query can produce duplicate country rows and thus may not return the top 3 distinct countries, the two queries are not functionally similar for the user request.\"}, {'idx': 22, 'reason': 'mismatch', 'pred_sql': \"sql\\nSELECT DISTINCT CASE WHEN LOWER(country) = 'united states' THEN allied_country WHEN LOWER(allied_country) = 'united states' THEN country END AS allied_country FROM military_alliances WHERE LOWER(country) = 'united states' OR LOWER(allied_country) = 'united states';\", 'feedback': \"The golden query selects allied_country only from rows where country = 'United States'.\\n\\nThe candidate query:\\n- Is case-insensitive and checks both columns (country OR allied_country) for 'united states'.\\n- Returns the other side of the pair (country when allied_country = 'United States', allied_country when country = 'United States').\\n- Uses DISTINCT (removes duplicates).\\n\\nBecause the candidate handles rows where United States appears in allied_country (and the golden does not), and because DISTINCT/case-insensitivity change potential results, the two queries can produce different result sets. Example: a row (country='Japan', allied_country='United States') would be returned by the candidate (as 'Japan') but not by the golden query.\\n\\nTherefore they are not functionally equivalent.\"}, {'idx': 25, 'reason': 'mismatch', 'pred_sql': \"sql\\nSELECT COUNT(DISTINCT c.customer_id) AS num_customers FROM Customers c JOIN Purchases p ON c.customer_id = p.customer_id WHERE c.customer_country IN ('China','Japan','South Korea');\", 'feedback': 'The two queries differ in important ways:\\n\\n- Country filtering: the golden SQL filters WHERE c.customer_country = \\'Asia\\' (a literal country value that does not match the data), while the candidate filters WHERE c.customer_country IN (\\'China\\',\\'Japan\\',\\'South Korea\\') — a different and more realistic filter for \"in Asia\".\\n- Counting semantics: the golden query uses COUNT(*) on the join (counts joined rows / purchases), whereas the candidate uses COUNT(DISTINCT c.customer_id) (counts unique customers). The natural language asks for \"How many customers...\", which implies distinct customers; the golden does not ensure uniqueness.\\n- Both queries omit any condition about \"sustainable fabrics\" because no fabric/sustainability info exists in the schema, so neither addresses that part of the prompt.\\n\\nBecause the country filter and the counting semantics differ (and thus produce different results), the candidate SQL is not functionally equivalent to the golden SQL.'}, {'idx': 28, 'reason': 'mismatch', 'pred_sql': \"sql\\nSELECT COALESCE(SUM(co2_emissions), 0) AS total_co2_emissions FROM company WHERE LOWER(region) = 'eu' AND LOWER(company_name) LIKE '%cosmetic%';\", 'feedback': 'The candidate query is not functionally equivalent to the golden query. Differences:\\n- The candidate adds AND LOWER(company_name) LIKE \\'%cosmetic%\\', restricting to rows whose name contains \"cosmetic\"; the golden query has no such filter.\\n- The candidate uses LOWER(region) = \\'eu\\' (case-insensitive) and COALESCE(SUM(...), 0) to return 0 instead of NULL; these are minor behavioral differences but do not reconcile the missing cosmetic filter.\\n\\nBecause the candidate applies an additional filter that changes which rows are summed, the results can differ from the golden query.'}, {'idx': 29, 'reason': 'mismatch', 'pred_sql': \"sql SELECT p.name AS project_name, p.budget, e.name AS engineer_name FROM public_works_projects p LEFT JOIN (SELECT project_id, MIN(engineer_id) AS min_eid FROM project_engineers GROUP BY project_id) m ON p.project_id = m.project_id LEFT JOIN project_engineers e ON e.project_id = p.project_id AND e.engineer_id = m.min_eid WHERE p.state = 'CA' ORDER BY p.budget DESC;\", 'feedback': 'The candidate query is not functionally equivalent to the golden query.\\n\\nKey differences:\\n- The golden query uses an INNER JOIN, returning only projects in CA that have matching engineer rows (and will return one row per engineer if multiple exist).\\n- The candidate query uses a LEFT JOIN to a subquery that picks MIN(engineer_id) per project, then joins to that single engineer row. That causes two behavioral changes:\\n  - It returns at most one engineer per project (the one with the smallest engineer_id) instead of returning all engineers for a project.\\n  - It will include CA projects that have no engineers (engineer_name NULL) because of the LEFT JOIN, whereas the INNER JOIN golden query would exclude such projects.\\n- Column naming/aliases differ only cosmetically and do not affect semantics.\\n\\nBecause of the differences in join behavior and which engineers are returned, the candidate SQL is not functionally similar to the golden SQL.'}, {'idx': 36, 'reason': 'mismatch', 'pred_sql': 'sql\\nSELECT CASE WHEN COALESCE(SUM(population),0)=0 THEN NULL ELSE SUM(monthly_consumption)/SUM(population) END AS avg_monthly_consumption_per_capita FROM urban_areas;', 'feedback': 'They are not equivalent. The golden query computes the unweighted average of per-city per-capita values (AVG(monthly_consumption / population)), i.e. it gives each city equal weight. The candidate computes the overall per-capita value as SUM(monthly_consumption) / SUM(population), which is a population-weighted average (total consumption divided by total population). These produce different results in general (they only match when all city populations are equal).\\n\\nExample with the provided data:\\n- New York per-capita = 1,200,000,000 / 8,500,000 ≈ 141.1764706\\n- Los Angeles per-capita = 600,000,000 / 4,000,000 = 150\\n- Golden (AVG of per-city rates) = (141.1764706 + 150) / 2 ≈ 145.5882353\\n- Candidate (total consumption / total population) = 1,800,000,000 / 12,500,000 = 144.0\\n\\nAlso, the candidate includes handling for SUM(population)=0 to avoid divide-by-zero; the golden query lacks that handling. Therefore they are functionally different.'}, {'idx': 38, 'reason': 'mismatch', 'pred_sql': 'sql\\nSELECT DISTINCT m1.actor FROM movies m1 JOIN movies m2 ON m1.id = m2.id AND m2.actor = m1.director;', 'feedback': 'The two queries are not equivalent.\\n\\n- Golden: SELECT DISTINCT actor FROM movies WHERE director IN (SELECT actor FROM movies);\\n  - This returns actors from any row whose director appears as an actor anywhere in the table (not necessarily in the same movie).\\n- Candidate: SELECT DISTINCT m1.actor FROM movies m1 JOIN movies m2 ON m1.id = m2.id AND m2.actor = m1.director;\\n  - This returns actors from movies where the director is also listed as an actor for the same movie id (i.e., director acted in that same movie).\\n\\nCounterexample:\\n- Rows:\\n  - (id=1, title=\\'A\\', actor=\\'Alice\\', director=\\'D\\')\\n  - (id=2, title=\\'B\\', actor=\\'D\\', director=\\'X\\')\\n- Golden: D appears in actors (row id=2), so golden returns \\'Alice\\' (actors from rows where director=\\'D\\').\\n- Candidate: For id=1, there is no row with id=1 and actor=\\'D\\', so candidate does NOT return \\'Alice\\'.\\n\\nThus the candidate enforces \"same movie\" while the golden checks only \"director appears as actor somewhere\", so they are functionally different.'}, {'idx': 39, 'reason': 'mismatch', 'pred_sql': \"sql SELECT EXTRACT(HOUR FROM revenue_time) AS hour, SUM(revenue) AS total_revenue FROM Movie_Revenue WHERE LOWER(movie_title) = LOWER('MOVIE_TITLE') GROUP BY EXTRACT(HOUR FROM revenue_time) ORDER BY hour;\", 'feedback': 'The candidate is not functionally equivalent to the golden SQL.\\n\\nDifferences:\\n- Movie filter: the golden query filters for the specific movie \\'Spider-Man: No Way Home\\'. The candidate filters WHERE LOWER(movie_title) = LOWER(\\'MOVIE_TITLE\\'), which uses the literal string \\'MOVIE_TITLE\\' (a placeholder) and therefore will not select the intended movie.\\n- Grouping: the golden groups by revenue_time directly; the candidate groups by EXTRACT(HOUR FROM revenue_time). While extracting the hour may be appropriate for \"hourly revenue\", it is a different grouping level than grouping by the full time value and thus can produce different results.\\n- Output columns and ordering: the candidate returns hour and orders results; the golden returns revenue_time and does not order. These are semantic differences in output.\\n\\nBecause of the incorrect movie filter and different grouping, the candidate SQL is not functionally similar to the golden query.'}, {'idx': 40, 'reason': 'mismatch', 'pred_sql': \"sql\\nDELETE FROM projects WHERE continent = 'Australia' AND end_date < '2015-01-01' AND (LOWER(name) LIKE '%coal%' OR LOWER(name) LIKE '%ore%');\", 'feedback': 'The golden query deletes every project in Australia with end_date < \\'2015-01-01\\'. The candidate adds an extra condition restricting deletions to project names containing \"coal\" or \"ore\" (case-insensitive). That makes the candidate more restrictive and not equivalent in general — it would fail to delete Australian projects completed before 2015-01-01 if their names don\\'t contain \"coal\" or \"ore\". (Note: for the provided sample data both would end up deleting the same row, but they are not functionally the same.)'}, {'idx': 46, 'reason': 'mismatch', 'pred_sql': \"sql\\nSELECT COUNT(*) AS successful_count FROM Satellite_Deployments WHERE manufacturer = 'SpaceX' AND success = TRUE AND deployment_date BETWEEN CURRENT_DATE - INTERVAL '5 years' AND CURRENT_DATE;\", 'feedback': \"The two queries are not functionally equivalent.\\n\\n- They share the same manufacturer and success filters, but they use different time filters:\\n  - Golden: YEAR(deployment_date) >= 2017 — a fixed cutoff that includes all rows from 2017 onward.\\n  - Candidate: deployment_date BETWEEN CURRENT_DATE - INTERVAL '5 years' AND CURRENT_DATE — a dynamic 5-year window relative to the current date.\\n- These produce different results in general (and on the provided data). Using the current date (2025-10-14) as an example:\\n  - Golden would count the two successful SpaceX deployments in 2018 and 2019 (COUNT = 2).\\n  - Candidate would consider only deployments since 2020-10-14; the 2018 and 2019 rows fall outside that window and the 2023 row is not successful, so it would return 0.\\n- Also note minor differences in boundary handling (YEAR(...) >= 2017 vs inclusive BETWEEN) and dependence on the system date for the candidate.\\n\\nBecause of these semantic differences, they are not functionally similar.\"}, {'idx': 48, 'reason': 'mismatch', 'pred_sql': \"sql SELECT AVG(avg_expr) AS avg_top5_expression FROM (SELECT gene_id, gene_name, AVG(expression) AS avg_expr FROM genetic.gene_expression WHERE LOWER(tissue) = 'cardiovascular' GROUP BY gene_id, gene_name ORDER BY avg_expr DESC LIMIT 5) AS top5;\", 'feedback': \"The two queries are not functionally equivalent.\\n\\n- Golden SQL: filters rows with tissue = 'Cardiovascular', orders rows by expression, takes the top 5 rows (individual expression records), and averages those expression values.\\n- Candidate SQL: filters for tissue (case-insensitive), then groups by gene_id and gene_name to compute an average expression per gene, orders genes by that per-gene average, takes the top 5 genes, and returns the average of those per-gene averages.\\n\\nThese differ when a gene has multiple expression rows: the golden query can select multiple rows for the same gene and average those row values directly, while the candidate first collapses rows per gene (AVG per gene) and then averages the top genes' averages. That produces different results in general. (The sample data happens to have only one cardiovascular row per gene, so they would match on this dataset, but they are not functionally similar.)\"}]}\n"
     ]
    }
   ],
   "source": [
    "opt_program = dspy.ChainOfThought(ProblemDef)\n",
    "opt_program.load(\"./optimized_program/program.json\")\n",
    "opt_metrics = evaluate_program(opt_program, test_split, limit=500, max_workers=32)\n",
    "print(f\"Original Program: {opt_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2e48098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Program: {'accuracy': 0.606, 'correct': 303, 'total': 500, 'avg_latency_s': 1.1225918439160014, 'failures': [{'idx': 1, 'reason': 'mismatch', 'pred_sql': \"sql SELECT DISTINCT o.Id, o.Name, o.Sector FROM Organizations o JOIN Donations d ON d.OrganizationId = o.Id JOIN Donors dr ON dr.Id = d.DonorId JOIN Countries c ON c.Id = dr.Id WHERE c.Name = 'India';\", 'feedback': 'They are not functionally similar.\\n\\nDifferences:\\n- Join condition between Donors and Countries:\\n  - Golden: JOIN Countries ON Donors.Name = Countries.Name and filters Countries.Continent = \\'Asia\\'.\\n  - Candidate: JOIN Countries ON Countries.Id = Donors.Id and filters Countries.Name = \\'India\\'.\\n  These are different relationships (matching donor name to country name vs matching donor id to country id).\\n- Filter difference: golden filters by Continent = \\'Asia\\', candidate filters by Country name = \\'India\\'.\\n- Selected columns: golden returns Organizations.Name only; candidate returns DISTINCT o.Id, o.Name, o.Sector.\\n- Because of the different joins/filters they will produce different results on the given data. Example with the provided rows: donor Id=2 matches country Id=2 (India), so the candidate will return organization Id 5 (\"Asha for Education\"), while the golden query (donor name ≠ country name) returns no rows.\\n\\nTherefore the candidate SQL is not functionally equivalent to the golden SQL.'}, {'idx': 12, 'reason': 'mismatch', 'pred_sql': \"sql SELECT 'AlgoFairness_Table1' AS table_name UNION ALL SELECT 'AlgoFairness_Table2' UNION ALL SELECT 'AlgoFairness_Table3';\", 'feedback': \"The candidate query is a hard-coded UNION that returns the three table names regardless of metadata, schema, or whether 'algorithm_id' is a primary key. The golden query reads from information_schema.columns to find tables that have a column named 'algorithm_id' in a given schema (although it still does not explicitly check primary key constraints). Functionally they are not equivalent: the candidate does not consult metadata or schema and will always return those three names, while the golden SQL dynamically queries metadata (and filters by schema). Therefore they are not similar.\"}, {'idx': 16, 'reason': 'mismatch', 'pred_sql': \"sql\\nINSERT INTO renewable_energy_production (id, source, state, production_gwh) VALUES (1, 'Wind', 'Texas', 3500.0), (2, 'Solar', 'Texas', 1200.0), (3, 'Wind', 'California', 2800.0), (4, 'Solar', 'California', 4200.0);\", 'feedback': \"The candidate inserts four rows for wind and solar in Texas and California (so it matches the sources/states), but it is not functionally equivalent to the golden SQL because:\\n- The production_gwh values differ (golden: 50, 75, 60, 80; candidate: 3500.0, 1200.0, 2800.0, 4200.0). These produce different data and are not interchangeable.\\n- The candidate supplies explicit id values while the golden query does not; the presence/values of the primary key matter for the resulting table state.\\n- Minor string case differences ('Wind'/'Solar' vs 'wind'/'solar') also change the exact stored values.\\n\\nBecause the inserted row values are not the same, the two queries are not functionally similar.\"}, {'idx': 20, 'reason': 'mismatch', 'pred_sql': \"sql\\nSELECT MIN(depth) AS min_depth FROM deep_sea_expeditions WHERE LOWER(location) LIKE '%atlantic%';\", 'feedback': 'The golden query filters rows where location = \\'Atlantic Ocean\\' exactly. The candidate uses LOWER(location) LIKE \\'%atlantic%\\', which is case-insensitive and matches any location string containing \"atlantic\" (e.g., \"North Atlantic\", \"South Atlantic Ocean\", \"Atlantic Rim\"), not strictly \"Atlantic Ocean\". Because the candidate\\'s predicate is broader, it can return a different (typically smaller) minimum depth in many datasets. The SELECT differences (aliasing) are harmless, but the WHERE difference makes them not functionally equivalent.'}, {'idx': 24, 'reason': 'mismatch', 'pred_sql': 'sql SELECT a.name AS agency, ROUND(AVG(e.salary),2) AS average_salary FROM agency a LEFT JOIN employee e ON a.name = e.agency GROUP BY a.name ORDER BY a.name;', 'feedback': 'The golden query computes AVG(salary) grouped only over the employee table, returning one row per agency that actually has employees. The candidate query LEFT JOINs agency to employee, so it will also return agencies that have no employees (with NULL average_salary). The candidate additionally rounds the average and orders the results and renames columns. Because the candidate can return extra rows (agencies with no employees) and NULLs where the golden query would have no row, the two queries are not functionally equivalent.'}, {'idx': 29, 'reason': 'mismatch', 'pred_sql': 'sql\\nSELECT id, name, daily_tx_volume FROM dapps ORDER BY daily_tx_volume DESC LIMIT 5;', 'feedback': \"The candidate SQL is not functionally similar to the golden SQL.\\n\\nKey differences:\\n- The golden query filters by region = 'South America'; the candidate has no WHERE clause and thus returns results across all regions.\\n- The golden aggregates transactions per app (SUM(daily_tx_volume)) and computes a rank over that sum; the candidate simply selects the raw daily_tx_volume (no aggregation) and does not compute rank.\\n- The golden groups by name to produce total transaction volume per dapp; the candidate returns individual rows (id, name, daily_tx_volume) and simply limits to the top 5 by daily_tx_volume overall.\\n- Column outputs differ: golden returns name, total_tx_volume, and rank; candidate returns id, name, daily_tx_volume.\\n\\nBecause of these substantive semantic differences, the two queries are not equivalent.\"}, {'idx': 30, 'reason': 'mismatch', 'pred_sql': \"sql SELECT COUNT(DISTINCT ip_address) AS total_unique_ips FROM attack_ips WHERE attack_type IN ('Ransomware','Phishing') AND last_updated >= DATETIME((SELECT MAX(last_updated) FROM attack_ips), '-6 months') AND occurrence_count = 1;\", 'feedback': 'The queries are not functionally equivalent.\\n\\nMain differences:\\n1. How \"no repeat offenders\" is enforced\\n   - Golden: GROUP BY ip_address HAVING COUNT(*) = 1 ensures an IP appears exactly once in the filtered time window (counts rows per IP).\\n   - Candidate: WHERE occurrence_count = 1 filters rows to those whose occurrence_count column equals 1, then COUNT(DISTINCT ip_address) counts unique IPs among those rows. This can include an IP that has multiple rows in the window (a repeat offender) as long as at least one of its rows has occurrence_count = 1 — that should have been excluded by the golden query.\\n\\n   Example: if IP X has two rows in the period, one with occurrence_count=1 and another with occurrence_count=2:\\n   - Golden: COUNT(*) = 2 -> excluded.\\n   - Candidate: the row with occurrence_count=1 remains so X is counted -> incorrect.\\n\\n2. Time-window computation\\n   - Golden: last_updated >= DATEADD(month, -6, GETDATE()) uses current date minus 6 months.\\n   - Candidate: last_updated >= DATETIME((SELECT MAX(last_updated) FROM attack_ips), \\'-6 months\\') uses the table\\'s max last_updated minus 6 months. These yield different cutoffs and are not equivalent.\\n\\n3. Result shape\\n   - Golden returns ip_address rows (could be adapted to COUNT), candidate returns the total count directly. The shape difference is minor relative to semantics, but combined with the other differences the candidate does not implement the same logic.\\n\\nBecause of these semantic differences (particularly the incorrect use of occurrence_count instead of counting rows per IP, and different date anchor), the candidate query is not functionally similar to the golden query.'}, {'idx': 32, 'reason': 'mismatch', 'pred_sql': 'sql\\nSELECT c.name AS country, COUNT(*) AS projects_2019 FROM projects p JOIN countries c ON p.country = c.name WHERE p.year = 2019 GROUP BY c.name ORDER BY projects_2019 DESC LIMIT 3;', 'feedback': \"The candidate SQL counts projects only for year = 2019 (WHERE p.year = 2019) and returns the top 3 by that year's project count. The golden SQL counts projects across the entire projects table (no year filter) and returns the top 3 overall. Because the candidate applies a 2019 filter and the golden does not, they are not functionally equivalent, even though both join to the countries table and limit to 3 rows.\"}, {'idx': 33, 'reason': 'mismatch', 'pred_sql': \"sql SELECT program_name, ROUND(COALESCE(CASE WHEN end_date < '2022-01-01' OR start_date > '2022-12-31' THEN 0.0 WHEN (end_date - start_date + 1) <= 0 THEN 0.0 ELSE total_expenses * ((CASE WHEN end_date < '2022-12-31' THEN end_date ELSE '2022-12-31' END) - (CASE WHEN start_date > '2022-01-01' THEN start_date ELSE '2022-01-01' END) + 1) / (end_date - start_date + 1) END, 0.0), 2) AS expenses_2022 FROM programs ORDER BY expenses_2022 DESC;\", 'feedback': 'The two queries are not functionally equivalent.\\n\\n- The golden query selects programs that overlap 2022 and returns the table column total_expenses unchanged for those programs (i.e., it lists the program\\'s full total_expenses when the program intersects 2022).\\n- The candidate query computes a prorated expense amount for 2022 by calculating the overlap days between the program period and 2022, scaling total_expenses by that fraction, rounding the result, and returning expenses_2022. It also sets non-overlapping programs to 0.0 rather than excluding them (and contains a stray \"sql\" token at the start).\\n\\nBecause the golden SQL returns the raw total_expenses for overlapping programs while the candidate returns a prorated/rounded expense for 2022 (and may include zero rows), they do not produce the same results for the same data.'}, {'idx': 34, 'reason': 'mismatch', 'pred_sql': 'sql\\nDELETE FROM Donors WHERE COALESCE(total_donations, 0) <= 5000;', 'feedback': 'The candidate SQL is not functionally similar. The user asked to delete donors who have not donated more than $5,000 in total between 2018 and 2022 — this requires aggregating donations from a Donations table and filtering by donation_year BETWEEN 2018 AND 2022. The golden query does that (it computes per-donor sums for 2018–2022 and deletes donors whose sum is not >5000).\\n\\nThe candidate query instead deletes based on the Donors.total_donations column (and treats NULL as 0), with no aggregation over the Donations table and no year filter. Therefore it can produce different results and does not implement the required year-restricted sum. Consequently they are not equivalent.'}, {'idx': 36, 'reason': 'mismatch', 'pred_sql': 'sql\\nSELECT c.country\\nFROM (SELECT DISTINCT country FROM national_security_incidents) c\\nLEFT JOIN (\\n  SELECT country, COALESCE(SUM(incident_count), 0) AS incidents_last3\\n  FROM national_security_incidents\\n  WHERE year BETWEEN (SELECT MAX(year) FROM national_security_incidents) - 2 AND (SELECT MAX(year) FROM national_security_incidents)\\n  GROUP BY country\\n) s ON c.country = s.country\\nWHERE COALESCE(s.incidents_last3, 0) = 0;', 'feedback': 'The two queries are not functionally equivalent.\\n\\n- Golden SQL: WHERE year >= 2020 AND incident_count = 0 GROUP BY country\\n  - Hard-codes the cutoff (>=2020) instead of using the dataset\\'s most recent year.\\n  - Selects any country that has at least one row in that period with incident_count = 0. It does not ensure the country had zero incidents across all rows in the 3-year span (a country could have a 0 row and also positive-incident rows and still be returned).\\n  - Also will not return countries that have no rows in that period.\\n\\n- Candidate SQL:\\n  - Dynamically computes the last 3 years using MAX(year) and sums incident_count per country over that span.\\n  - Left-joins against all distinct countries and treats missing aggregates as 0, so it returns countries whose total incidents in the 3-year window is zero (including countries with no records).\\n  - This correctly enforces \"no incidents in the last 3 years\" across the entire window.\\n\\nBecause the golden query checks only for existence of a zero row (and uses a fixed year) while the candidate ensures the sum over the full 3-year window is zero (and handles missing data), they are not functionally similar.'}, {'idx': 38, 'reason': 'mismatch', 'pred_sql': \"sql\\nSELECT AVG(s.speed) AS average_speed FROM speeds s JOIN vessels v ON v.id = s.vessel_id WHERE LOWER(v.region) LIKE '%arctic%' AND s.speed_date BETWEEN '2020-07-01' AND '2020-07-31';\", 'feedback': 'The two queries differ in the region filter. The golden query requires region = \\'Arctic Circle\\' (exact match). The candidate uses LOWER(v.region) LIKE \\'%arctic%\\' which is a case-insensitive substring match and will also match strings other than exactly \"Arctic Circle\" (e.g., \"Arctic Ocean\", \"Northern Arctic\", or different casing). Everything else (join, date range, aggregation) is the same, but the region predicate is broader in the candidate, so they are not functionally equivalent.'}, {'idx': 39, 'reason': 'mismatch', 'pred_sql': \"sql SELECT p.player_name, p.assists FROM players p INNER JOIN teams t ON p.team_name = t.team_name WHERE t.team_name = 'Celtics' AND t.season_start_year = 2018 AND t.season_end_year = 2019 AND p.assists = (SELECT MAX(p2.assists) FROM players p2 INNER JOIN teams t2 ON p2.team_name = t2.team_name WHERE t2.team_name = 'Celtics' AND t2.season_start_year = 2018 AND t2.season_end_year = 2019);\", 'feedback': 'The candidate query returns only the player(s) who have the maximum assists for the Celtics in 2018–2019 by joining to the teams table and comparing p.assists to the MAX from the same filtered set. The golden query selects player_name and MAX(assists) grouped BY player_name for the same filters; that returns each player with their own (per-player) max assists rather than selecting the overall top assist value (it does not restrict to the highest-assist player). Also the golden SQL references season_start_year/season_end_year in the players table (which actually exist only in teams), while the candidate correctly joins to teams. Because the candidate implements a different (and in fact more correct) result — returning only the top assist player(s) — the two queries are not functionally equivalent.'}, {'idx': 42, 'reason': 'mismatch', 'pred_sql': 'sql\\nSELECT COUNT(DISTINCT exhibits.work_id) AS works_in_1950 FROM exhibits JOIN exhibitions ON exhibits.exhibition_id = exhibitions.exhibition_id WHERE exhibitions.year = 1950;', 'feedback': 'The two queries both join exhibits to exhibitions and filter to year = 1950, but they count different things. The golden SQL uses COUNT(*) (counts exhibit rows), while the candidate uses COUNT(DISTINCT work_id) (counts unique works). These are not functionally equivalent in general — they only match when each work appears at most once in the exhibits rows for 1950. Therefore they are not the same.'}, {'idx': 46, 'reason': 'mismatch', 'pred_sql': \"sql\\nSELECT COALESCE(SUM(p.price * s.quantity), 0) AS total_revenue FROM product p JOIN sales s ON p.product_id = s.product_id WHERE LOWER(p.name) LIKE '%cosmetic%' AND p.cruelty_free = FALSE;\", 'feedback': 'The candidate SQL adds an extra filter \"LOWER(p.name) LIKE \\'%cosmetic%\\'\" which restricts results to products whose name contains the word \"cosmetic\". The golden query sums revenue for all products that are not cruelty-free (no name-based filter). The candidate also wraps the sum in COALESCE(..., 0) to return 0 instead of NULL when there are no matching rows — a minor difference. Because the candidate imposes an additional filtering condition not present in the gold query, the two are not functionally equivalent.'}, {'idx': 51, 'reason': 'mismatch', 'pred_sql': \"sql INSERT INTO startups (id, name, industry, founding_date, raised_funding, founder_race) VALUES (1, 'Health Startup 5', NULL, '2022-03-15', 7500000, 'Middle Eastern');\", 'feedback': 'The two statements are not functionally equivalent.\\n\\n- Industry value: the golden query inserts \\'Healthcare\\' for industry; the candidate inserts NULL. That changes the stored data and does not match the prompt\\'s implied industry.\\n- id column: the golden omits id (likely letting it default/autoincrement); the candidate explicitly inserts id = 1, which may be incorrect or produce key conflicts.\\n- Syntax/noise: the candidate string begins with an extraneous \"sql \" prefix which is not valid SQL.\\n- Minor difference: raised_funding formatting (7500000 vs 7500000.00) is semantically the same, but the industry and id differences are decisive.\\n\\nBecause of the industry and id differences (and the extraneous prefix), these queries are not functionally similar.'}, {'idx': 52, 'reason': 'mismatch', 'pred_sql': 'sql SELECT a.name AS animal, COALESCE(SUM(ap.population), 0) AS total_population FROM animal a LEFT JOIN animal_population ap ON a.id = ap.animal_id GROUP BY a.id, a.name;', 'feedback': 'The candidate uses a LEFT JOIN and COALESCE(SUM(...), 0) and groups by a.id, a.name, so it will include every animal and show 0 for animals with no rows in animal_population. The golden query uses an (inner) JOIN and SUM(...) grouping by a.name, so it will exclude animals that have no population rows. Because the candidate includes animals with zero total and the golden does not, they are not functionally equivalent for the given prompt (\"List all animals and their total population\"), which implies returning all animals. Therefore they differ in behavior (especially for animals with no population rows).'}, {'idx': 53, 'reason': 'mismatch', 'pred_sql': \"sql SELECT COUNT(*) AS us_supplier_count FROM suppliers WHERE LOWER(location) IN ('alabama','alaska','arizona','arkansas','california','colorado','connecticut','delaware','district of columbia','florida','georgia','hawaii','idaho','illinois','indiana','iowa','kansas','kentucky','louisiana','maine','maryland','massachusetts','michigan','minnesota','mississippi','missouri','montana','nebraska','nevada','new hampshire','new jersey','new mexico','new york','north carolina','north dakota','ohio','oklahoma','oregon','pennsylvania','rhode island','south carolina','south dakota','tennessee','texas','utah','vermont','virginia','washington','west virginia','wisconsin','wyoming','united states','usa','u.s.a.','u.s.','us');\", 'feedback': \"The golden query only counts rows where location = 'California'. The candidate checks a long list of US state names and variants ('usa', 'united states', abbreviations, etc.) and would count suppliers located in any of those values. Although both return the same result on the provided data (location = 'California'), they are not functionally equivalent: the candidate is broader and would count many more rows that the golden query would not. Therefore they are not similar.\"}, {'idx': 54, 'reason': 'mismatch', 'pred_sql': \"sql INSERT INTO Fabric (id, name, source_country, sustainability_score) VALUES ((SELECT COALESCE(MAX(id), 0) + 1 FROM Fabric), 'Organic Hemp', 'Unknown', 90);\", 'feedback': 'The candidate and golden queries both insert a row with name \\'Organic Hemp\\', but they differ in important ways:\\n\\n- id: golden inserts id = 10 explicitly. Candidate uses (SELECT COALESCE(MAX(id),0)+1 FROM Fabric) to generate an id dynamically — not guaranteed to be 10 and may differ depending on table contents.\\n- source_country: golden uses \\'China\\'; candidate uses \\'Unknown\\' — different data.\\n- sustainability_score: golden uses 85; candidate uses 90 — different data.\\n- Syntax issue: the candidate begins with the token \"sql \" before INSERT, which would produce a syntax error unless removed.\\n\\nBecause the inserted rows would not be the same (different id, source_country, and sustainability_score) and the candidate has a leading syntax error, the candidate is not functionally equivalent to the golden SQL.'}, {'idx': 58, 'reason': 'mismatch', 'pred_sql': \"sql\\nSELECT COUNT(*) AS sustainable_count FROM Accommodations WHERE country IN ('USA','Canada') AND sustainability_rating >= 4;\", 'feedback': 'The two queries are not equivalent. The golden SQL filters WHERE country IN (\\'North America\\'), which only matches rows whose country value is the literal \\'North America\\' (none in the provided data). The candidate SQL filters WHERE country IN (\\'USA\\',\\'Canada\\'), which matches the two rows in the table and correctly reflects \"North America\" as USA and Canada. With the given data the golden query would return 0 while the candidate returns 2, so they are not functionally similar.'}]}\n"
     ]
    }
   ],
   "source": [
    "openai_opt_program = dspy.ChainOfThought(ProblemDef)\n",
    "openai_opt_program.load(\"./openai_optimized_program/program.json\")\n",
    "openai_opt_metrics = evaluate_program(openai_opt_program, test_split, limit=500, max_workers=32)\n",
    "print(f\"Original Program: {openai_opt_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcc020d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def save_metrics(metrics, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4, sort_keys=True) \n",
    "\n",
    "save_metrics(og_metrics, \"/dspy_program/5-mini.json\")\n",
    "save_metrics(opt_metrics,\"/optimized_program/5-mini.json\")\n",
    "save_metrics(openai_metrics,\"/openai_optimized_program/5-mini.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
