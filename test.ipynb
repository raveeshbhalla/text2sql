{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b76a8c0d",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a234a155",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df341691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U dspy datasets tabulate duckdb pandas numpy ipywidgets \"sqlglot[rs]\" wandb --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4329c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from datasets import load_dataset\n",
    "import tabulate\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "288f15f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env.local\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "if not wandb_api_key:\n",
    "    raise ValueError(\"WANDB_API_KEY not found in environment variables\")\n",
    "\n",
    "lm = dspy.LM(\"openai/gpt-5-mini\", api_key=openai_api_key, temperature=1, max_tokens=16000)\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e655b0",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aef6f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"gretelai/synthetic_text_to_sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13420ef6",
   "metadata": {},
   "source": [
    "# Set up DSPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cab5f8",
   "metadata": {},
   "source": [
    "## Set up Signature and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4380853",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProblemDef(dspy.Signature):\n",
    "    \"\"\"You are a database expert. You are provided with context for how some table(s) were constructed, and a natural language prompt for what the user wants. Your job is to write a SQL query to provide them with the required data.\"\"\"\n",
    "    \n",
    "    sql_context: str = dspy.InputField(description=\"SQL queries for creating the table(s) and loading some data\")\n",
    "    sql_prompt: str = dspy.InputField(description=\"User's natural language prompt\")\n",
    "    sql: str = dspy.OutputField(description=\"SQL query that delivers on the user's request. Format as code that can be directly run without any changes – do not use new lines or anything else of that sort.\")\n",
    "\n",
    "program = dspy.ChainOfThought(ProblemDef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c92e6cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install duckdb pandas numpy sqlglot --quiet\n",
    "import duckdb, pandas as pd, numpy as np, re\n",
    "import sqlglot\n",
    "from sqlglot import parse_one\n",
    "\n",
    "_ORDER_BY = re.compile(r\"\\border\\s+by\\b\", re.IGNORECASE)\n",
    "\n",
    "def _split_sql_statements(script: str):\n",
    "    out, buf, q = [], [], None\n",
    "    i, n = 0, len(script)\n",
    "    while i < n:\n",
    "        ch = script[i]\n",
    "        if q:\n",
    "            buf.append(ch)\n",
    "            if ch == q:\n",
    "                if i + 1 < n and script[i+1] == q:\n",
    "                    buf.append(script[i+1]); i += 1\n",
    "                else:\n",
    "                    q = None\n",
    "        else:\n",
    "            if ch in (\"'\", '\"', \"`\"):\n",
    "                q = ch; buf.append(ch)\n",
    "            elif ch == ';':\n",
    "                s = \"\".join(buf).strip()\n",
    "                if s: out.append(s)\n",
    "                buf = []\n",
    "            else:\n",
    "                buf.append(ch)\n",
    "        i += 1\n",
    "    tail = \"\".join(buf).strip()\n",
    "    if tail: out.append(tail)\n",
    "    return out\n",
    "\n",
    "import re\n",
    "from sqlglot import parse_one\n",
    "\n",
    "_SQLITE_DATE_RE = re.compile(\n",
    "    r\"\"\"\\bdate\\s*\\(\\s*'now'\\s*(?:,\\s*'([+-])\\s*(\\d+)\\s*(year|month|day)s?'\\s*)?\\)\"\"\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "_SQLITE_DATETIME_RE = re.compile(\n",
    "    r\"\"\"\\bdatetime\\s*\\(\\s*'now'\\s*(?:,\\s*'([+-])\\s*(\\d+)\\s*(year|month|day|hour|minute|second)s?'\\s*)?\\)\"\"\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "def _normalize_sqlite_dates(sql: str) -> str:\n",
    "    # date('now') or date('now','-1 year') -> CURRENT_DATE +/- INTERVAL 'N unit'\n",
    "    def _date_subst(m):\n",
    "        sign, num, unit = m.group(1), m.group(2), m.group(3)\n",
    "        if not sign:  # just date('now')\n",
    "            return \"CURRENT_DATE\"\n",
    "        op = \"-\" if sign == \"-\" else \"+\"\n",
    "        return f\"CURRENT_DATE {op} INTERVAL '{num} {unit.lower()}'\"\n",
    "    sql = _SQLITE_DATE_RE.sub(_date_subst, sql)\n",
    "\n",
    "    # datetime('now') / datetime('now','+/-N unit') -> CURRENT_TIMESTAMP +/- INTERVAL 'N unit'\n",
    "    def _dt_subst(m):\n",
    "        sign, num, unit = m.group(1), m.group(2), m.group(3)\n",
    "        if not sign:\n",
    "            return \"CURRENT_TIMESTAMP\"\n",
    "        op = \"-\" if sign == \"-\" else \"+\"\n",
    "        return f\"CURRENT_TIMESTAMP {op} INTERVAL '{num} {unit.lower()}'\"\n",
    "    sql = _SQLITE_DATETIME_RE.sub(_dt_subst, sql)\n",
    "\n",
    "    return sql\n",
    "\n",
    "def _mysql_to_duckdb(stmt: str) -> str:\n",
    "    s = _normalize_sqlite_dates(stmt)  # <-- NEW: normalize SQLite first\n",
    "    try:\n",
    "        return parse_one(s, read=\"mysql\").sql(dialect=\"duckdb\")\n",
    "    except Exception:\n",
    "        # minimal fallbacks for MySQLisms if parse fails\n",
    "        s = re.sub(r\"`([^`]+)`\", r'\"\\1\"', s)\n",
    "        s = re.sub(\n",
    "            r\"DATE_SUB\\s*\\(\\s*(CURRENT_DATE|NOW\\(\\))\\s*,\\s*INTERVAL\\s+(\\d+)\\s+(YEAR|MONTH|DAY)\\s*\\)\",\n",
    "            lambda m: f\"{'CURRENT_DATE' if m.group(1).startswith('CURRENT') else 'CURRENT_DATE'} - INTERVAL '{m.group(2)} {m.group(3).lower()}'\",\n",
    "            s, flags=re.IGNORECASE,\n",
    "        )\n",
    "        s = re.sub(\n",
    "            r\"DATE_ADD\\s*\\(\\s*(CURRENT_DATE|NOW\\(\\))\\s*,\\s*INTERVAL\\s+(\\d+)\\s+(YEAR|MONTH|DAY)\\s*\\)\",\n",
    "            lambda m: f\"{'CURRENT_DATE' if m.group(1).startswith('CURRENT') else 'CURRENT_DATE'} + INTERVAL '{m.group(2)} {m.group(3).lower()}'\",\n",
    "            s, flags=re.IGNORECASE,\n",
    "        )\n",
    "        s = re.sub(r\"\\bIFNULL\\s*\\(\", \"COALESCE(\", s, flags=re.IGNORECASE)\n",
    "        s = re.sub(r\"\\bLOCATE\\s*\\(\\s*([^,]+)\\s*,\\s*([^)]+)\\)\", r\"STRPOS(\\2, \\1)\", s, flags=re.IGNORECASE)\n",
    "        return s\n",
    "\n",
    "def _normalize_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == \"O\":\n",
    "            try:\n",
    "                df[c] = pd.to_numeric(df[c])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return df.replace({np.nan: None})\n",
    "\n",
    "def _exec_script_capture_last_select(con, script: str):\n",
    "    last_df, last_sel_sql = None, None\n",
    "    for raw in _split_sql_statements(script):\n",
    "        stmt = _mysql_to_duckdb(raw)\n",
    "        # detect SELECT after minimal comment strip\n",
    "        s = re.sub(r\"^\\s*(--[^\\n]*\\n|/\\*.*?\\*/\\s*)*\", \"\", stmt, flags=re.DOTALL)\n",
    "        if re.match(r\"(?is)^\\s*(with\\b.*?select|select)\\b\", s):\n",
    "            last_df = con.execute(stmt).fetchdf()\n",
    "            last_sel_sql = stmt\n",
    "        else:\n",
    "            con.execute(stmt)\n",
    "    if last_df is not None:\n",
    "        last_df = _normalize_df(last_df)\n",
    "    return last_df, last_sel_sql\n",
    "\n",
    "def evaluate_sql(sql_context: str, golden_sql: str, predicted_sql: str):\n",
    "    con = duckdb.connect(\":memory:\")\n",
    "\n",
    "    # context\n",
    "    try:\n",
    "        for raw in _split_sql_statements(sql_context):\n",
    "            con.execute(_mysql_to_duckdb(raw))\n",
    "    except Exception as e:\n",
    "        return 0, {\"reason\": \"context_error\", \"detail\": str(e)}\n",
    "\n",
    "    # golden\n",
    "    try:\n",
    "        gold_df, gold_last_select = _exec_script_capture_last_select(con, golden_sql)\n",
    "    except Exception as e:\n",
    "        return 0, {\"reason\": \"gold_error\", \"detail\": str(e)}\n",
    "    if gold_df is None:\n",
    "        return 0, {\"reason\": \"gold_no_select\", \"detail\": \"No SELECT in golden_sql.\"}\n",
    "\n",
    "    # predicted\n",
    "    try:\n",
    "        pred_df, pred_last_select = _exec_script_capture_last_select(con, predicted_sql)\n",
    "    except Exception as e:\n",
    "        return 0, {\"reason\": \"pred_error\", \"detail\": str(e)}\n",
    "    if pred_df is None:\n",
    "        return 0, {\"reason\": \"pred_no_select\", \"detail\": \"No SELECT in predicted_sql.\"}\n",
    "\n",
    "    # column alignment (allow pred supersets; else try set/positional)\n",
    "    gold_cols, pred_cols = list(gold_df.columns), list(pred_df.columns)\n",
    "    if gold_cols == pred_cols:\n",
    "        pass\n",
    "    elif set(gold_cols).issubset(pred_cols):\n",
    "        pred_df = pred_df[gold_cols]\n",
    "    elif set(gold_cols) == set(pred_cols):\n",
    "        pred_df = pred_df[gold_cols]\n",
    "    elif gold_df.shape[1] == pred_df.shape[1]:\n",
    "        new_names = [f\"c{i}\" for i in range(gold_df.shape[1])]\n",
    "        gold_df = gold_df.copy(); pred_df = pred_df.copy()\n",
    "        gold_df.columns = new_names; pred_df.columns = new_names\n",
    "    else:\n",
    "        return 0, {\"reason\": \"column_mismatch\",\n",
    "                   \"detail\": f\"Different number of columns: expected {gold_df.shape[1]}, got {pred_df.shape[1]}\"}\n",
    "\n",
    "    # ordering rule from gold's last SELECT\n",
    "    gold_has_order = bool(_ORDER_BY.search(gold_last_select or \"\"))\n",
    "    if not gold_has_order:\n",
    "        try:\n",
    "            g = gold_df.sort_values(by=list(gold_df.columns), kind=\"mergesort\").reset_index(drop=True)\n",
    "            p = pred_df.sort_values(by=list(gold_df.columns), kind=\"mergesort\").reset_index(drop=True)\n",
    "        except Exception:\n",
    "            g = gold_df.reset_index(drop=True); p = pred_df.reset_index(drop=True)\n",
    "    else:\n",
    "        g = gold_df.reset_index(drop=True); p = pred_df.reset_index(drop=True)\n",
    "\n",
    "    # value compare\n",
    "    if g.shape != p.shape:\n",
    "        return 0, {\"reason\": \"shape_mismatch\", \"detail\": f\"gold {g.shape} vs pred {p.shape}\"}\n",
    "\n",
    "    for c in g.columns:\n",
    "        if pd.api.types.is_numeric_dtype(g[c]) and pd.api.types.is_numeric_dtype(p[c]):\n",
    "            if not np.allclose(g[c].values, p[c].values, rtol=1e-6, atol=1e-8, equal_nan=True):\n",
    "                return 0, {\"reason\": \"value_mismatch\", \"detail\": f\"Numeric mismatch in '{c}'\",\n",
    "                           \"gold_head\": g.head(10).to_dict(\"records\"),\n",
    "                           \"pred_head\": p.head(10).to_dict(\"records\")}\n",
    "        else:\n",
    "            eq = [(x == y) or (x is None and y is None) for x, y in zip(g[c].values, p[c].values)]\n",
    "            if not all(eq):\n",
    "                return 0, {\"reason\": \"value_mismatch\", \"detail\": f\"Mismatch in '{c}'\",\n",
    "                           \"gold_head\": g.head(10).to_dict(\"records\"),\n",
    "                           \"pred_head\": p.head(10).to_dict(\"records\")}\n",
    "    return 1, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb24b156",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "936c6363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: CREATE TABLE upgrades (id INT, cost FLOAT, type TEXT); INSERT INTO upgrades (id, cost, type) VALUES (1, 500, 'Insulation'), (2, 1000, 'HVAC'), (3, 1500, 'Lighting');\n",
      "Prompt: Find the energy efficiency upgrades with the highest cost and their types.\n",
      "Golden sql: SELECT type, cost FROM (SELECT type, cost, ROW_NUMBER() OVER (ORDER BY cost DESC) as rn FROM upgrades) sub WHERE rn = 1;\n",
      "Prediction(\n",
      "    reasoning='We need the upgrade(s) that have the maximum cost. Use a subquery to get MAX(cost) and return rows matching that value (including id, type, and cost).',\n",
      "    sql='SELECT id, type, cost FROM upgrades WHERE cost = (SELECT MAX(cost) FROM upgrades);'\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "demo_index = 4\n",
    "context = ds['train'][demo_index]['sql_context']\n",
    "prompt = ds['train'][demo_index]['sql_prompt']\n",
    "golden_sql = ds['train'][demo_index]['sql']\n",
    "\n",
    "print(f\"Context: {context}\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Golden sql: {golden_sql}\")\n",
    "result = program(sql_context=context, sql_prompt=prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42b7fd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 None\n"
     ]
    }
   ],
   "source": [
    "score, info = evaluate_sql(context, golden_sql, result.sql)\n",
    "print(score, info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58142861",
   "metadata": {},
   "source": [
    "## Environment didn't work, let's use LLM as Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c72a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Judge(dspy.Signature):\n",
    "    \"\"\"You are required to judge two SQL queries for functional similarity. You will be given a context of how the table(s) and data were created, and the natural language prompt from the user\"\"\"\n",
    "\n",
    "    sql_context: str = dspy.InputField(description=\"SQL statement(s) creating the table(s) and the input data\")\n",
    "    sql_prompt: str = dspy.InputField(description=\"Natural language prompt from the user\")\n",
    "    golden_sql: str = dspy.InputField(description=\"The golden SQL query from our dataset\")\n",
    "    candidate_sql: str = dspy.InputField(description=\"A SQL query generated by a model for the same prompt\")\n",
    "    similar: bool = dspy.OutputField(description=\"True if the candidate SQL query is functionally similar to the golden SQL query\")\n",
    "\n",
    "judge = dspy.ChainOfThought(Judge)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1677bf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: CREATE TABLE upgrades (id INT, cost FLOAT, type TEXT); INSERT INTO upgrades (id, cost, type) VALUES (1, 500, 'Insulation'), (2, 1000, 'HVAC'), (3, 1500, 'Lighting');\n",
      "Prompt: Find the energy efficiency upgrades with the highest cost and their types.\n",
      "Golden SQL: SELECT type, cost FROM (SELECT type, cost, ROW_NUMBER() OVER (ORDER BY cost DESC) as rn FROM upgrades) sub WHERE rn = 1;\n",
      "Candidate SQL: SELECT id, type, cost FROM upgrades WHERE cost = (SELECT MAX(cost) FROM upgrades);\n",
      "Judge Response: Prediction(\n",
      "    reasoning='Both queries return the upgrade(s) that have the maximum cost and include the type and cost information. Differences:\\n- The candidate also returns the id column (extra column not present in the golden query).\\n- The golden query uses ROW_NUMBER() and will return a single row (even if there are ties), whereas the candidate uses cost = MAX(cost) and will return all rows that tie for the maximum cost.\\n\\nDespite these differences in returned columns and tie-handling, the candidate still retrieves the highest-cost upgrade(s) and their types, so it is functionally similar to the golden query for the user intent.',\n",
      "    similar=True\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "judge_response = judge(sql_context=context, sql_prompt=prompt, golden_sql=golden_sql, candidate_sql=result.sql)\n",
    "print(f\"Context: {context}\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Golden SQL: {golden_sql}\")\n",
    "print(f\"Candidate SQL: {result.sql}\")\n",
    "print(f\"Judge Response: {judge_response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7746087",
   "metadata": {},
   "source": [
    "# Get ready to GEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c35b7379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets dspy-ai\n",
    "import math, random\n",
    "from typing import Callable, List, Tuple, Optional\n",
    "from datasets import Dataset, DatasetDict\n",
    "from dspy import GEPA\n",
    "\n",
    "def split_for_gepa(\n",
    "    ds: Dataset,\n",
    "    to_example: Callable[[dict], \"dspy.Example\"],\n",
    "    val_size: float = 0.15,\n",
    "    seed: int = 42,\n",
    "    group_col: Optional[str] = None,\n",
    "    stratify_col: Optional[str] = None,\n",
    ") -> Tuple[List[\"dspy.Example\"], List[\"dspy.Example\"]]:\n",
    "    \"\"\"\n",
    "    Return (train_set, val_set) as lists of dspy.Example.\n",
    "    - If group_col is set: group-aware split (no group leakage).\n",
    "    - Else if stratify_col is set: use HF stratified split.\n",
    "    - Else: random split.\n",
    "    \"\"\"\n",
    "    assert 0.0 < val_size < 1.0, \"val_size must be in (0,1)\"\n",
    "    rng = random.Random(seed)\n",
    "\n",
    "    # --- Group-aware split (preferred for text2sql) ---\n",
    "    if group_col:\n",
    "        groups = ds[group_col]\n",
    "        # Build group -> indices\n",
    "        g2idx = {}\n",
    "        for i, g in enumerate(groups):\n",
    "            g2idx.setdefault(g, []).append(i)\n",
    "        uniq_groups = list(g2idx.keys())\n",
    "        rng.shuffle(uniq_groups)\n",
    "        n_val_groups = max(1, math.floor(val_size * len(uniq_groups)))\n",
    "        val_groups = set(uniq_groups[:n_val_groups])\n",
    "\n",
    "        val_idx = [i for g in val_groups for i in g2idx[g]]\n",
    "        train_idx = [i for g in uniq_groups[n_val_groups:] for i in g2idx[g]]\n",
    "\n",
    "        # Edge case: if a group is gigantic, ensure both splits non-empty\n",
    "        if not train_idx or not val_idx:\n",
    "            # fallback: plain random split\n",
    "            perm = list(range(len(ds)))\n",
    "            rng.shuffle(perm)\n",
    "            cut = max(1, math.floor(val_size * len(ds)))\n",
    "            val_idx, train_idx = perm[:cut], perm[cut:]\n",
    "\n",
    "        ds_train = ds.select(train_idx)\n",
    "        ds_val = ds.select(val_idx)\n",
    "\n",
    "    # --- Stratified split (when you have a label/cluster column) ---\n",
    "    elif stratify_col:\n",
    "        # HF does stratify on categorical-like columns\n",
    "        parts: DatasetDict = ds.train_test_split(\n",
    "            test_size=val_size,\n",
    "            seed=seed,\n",
    "            stratify_by_column=stratify_col,\n",
    "        )\n",
    "        ds_train, ds_val = parts[\"train\"], parts[\"test\"]\n",
    "\n",
    "    # --- Simple random split ---\n",
    "    else:\n",
    "        parts: DatasetDict = ds.train_test_split(test_size=val_size, seed=seed)\n",
    "        ds_train, ds_val = parts[\"train\"], parts[\"test\"]\n",
    "\n",
    "    # Map to dspy.Example lists\n",
    "    train_set = [to_example(r) for r in ds_train]\n",
    "    val_set = [to_example(r) for r in ds_val]\n",
    "    return train_set, val_set\n",
    "\n",
    "def to_dspy_example(row):\n",
    "    # mark inputs; leave gold 'sql' as label\n",
    "    return dspy.Example(\n",
    "        sql_prompt=row[\"sql_prompt\"],\n",
    "        sql_context=row[\"sql_context\"],\n",
    "        sql=row[\"sql\"],          # gold label\n",
    "    ).with_inputs(\"sql_prompt\", \"sql_context\")\n",
    "\n",
    "\n",
    "# call function that splits ds['train'] into train_set and val_set as needed\n",
    "# ds is your loaded HF dataset dict; we split ds[\"train\"]\n",
    "train_set, val_set = split_for_gepa(\n",
    "    ds[\"train\"],\n",
    "    to_dspy_example,          # your to_dspy_example(row)\n",
    "    val_size=0.05,\n",
    "    seed=42,\n",
    "    group_col=None,      # e.g., \"db_id\" if available\n",
    "    stratify_col=None,   # or a column like \"op_class\" if you want stratification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "940c1f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:24:02 INFO dspy.teleprompt.gepa.gepa: Running GEPA for approx 1180 metric calls of the program. This amounts to 2.95 full evals on the train+val set.\n",
      "2025/10/12 20:24:02 INFO dspy.teleprompt.gepa.gepa: Using 200 examples for tracking Pareto scores. You can consider using a smaller sample of the valset to allow GEPA to explore more diverse solutions within the same budget.\n",
      "/Users/raveesh/dev/text2sql/.venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/Users/raveesh/dev/text2sql/.venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/raveesh/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mraveeshbhalla90\u001b[0m (\u001b[33mraveeshbhalla90-personal\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/raveesh/dev/text2sql/wandb/run-20251012_202403-l2ayb4qy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/raveeshbhalla90-personal/text2sql/runs/l2ayb4qy' target=\"_blank\">confused-waterfall-1</a></strong> to <a href='https://wandb.ai/raveeshbhalla90-personal/text2sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/raveeshbhalla90-personal/text2sql' target=\"_blank\">https://wandb.ai/raveeshbhalla90-personal/text2sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/raveeshbhalla90-personal/text2sql/runs/l2ayb4qy' target=\"_blank\">https://wandb.ai/raveeshbhalla90-personal/text2sql/runs/l2ayb4qy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [dspy, litellm, openai] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "GEPA Optimization:   0%|          | 0/1180 [00:00<?, ?rollouts/s]2025/10/12 20:24:06 INFO dspy.evaluate.evaluate: Average Metric: 111.0 / 200 (55.5%)\n",
      "2025/10/12 20:24:07 INFO dspy.teleprompt.gepa.gepa: Iteration 0: Base program full valset score: 0.555\n",
      "GEPA Optimization:  17%|█▋        | 200/1180 [00:02<00:11, 87.44rollouts/s]2025/10/12 20:24:07 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Selected program 0 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.00 / 3 (100.0%): 100%|██████████| 3/3 [00:24<00:00,  8.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:24:31 INFO dspy.evaluate.evaluate: Average Metric: 3.0 / 3 (100.0%)\n",
      "2025/10/12 20:24:31 INFO dspy.teleprompt.gepa.gepa: Iteration 1: All subsample scores perfect. Skipping.\n",
      "2025/10/12 20:24:31 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Reflective mutation did not propose a new candidate\n",
      "2025/10/12 20:24:31 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Selected program 0 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 3.00 / 3 (100.0%): 100%|██████████| 3/3 [00:22<00:00,  7.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:24:54 INFO dspy.evaluate.evaluate: Average Metric: 3.0 / 3 (100.0%)\n",
      "2025/10/12 20:24:54 INFO dspy.teleprompt.gepa.gepa: Iteration 2: All subsample scores perfect. Skipping.\n",
      "2025/10/12 20:24:54 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Reflective mutation did not propose a new candidate\n",
      "2025/10/12 20:24:54 INFO dspy.teleprompt.gepa.gepa: Iteration 3: Selected program 0 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 1.00 / 3 (33.3%): 100%|██████████| 3/3 [00:35<00:00, 11.93s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:25:30 INFO dspy.evaluate.evaluate: Average Metric: 1.0 / 3 (33.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:26:03 INFO dspy.teleprompt.gepa.gepa: Iteration 3: Proposed new text for predict: You are a SQL-writing assistant (a database expert). You will be given two inputs:\n",
      "- sql_context: DDL and INSERT statements that define table schemas and sample data (always present).\n",
      "- sql_prompt: a natural-language request describing the data the user wants.\n",
      "\n",
      "Your job: produce a correct SQL query (and a short, explicit reasoning/assumptions block) that returns the data requested by sql_prompt when applied to the schema and sample data in sql_context.\n",
      "\n",
      "Strict requirements and behavior rules\n",
      "1. Output format\n",
      "   - Provide two clearly labeled parts: \"reasoning\" and \"sql\".\n",
      "   - In reasoning, state any assumptions or disambiguations you make (see below). Keep this concise but explicit.\n",
      "   - In sql, output only the SQL statement (valid generic SQL) that implements the requested behavior and matches your stated assumptions.\n",
      "\n",
      "2. Always inspect sql_context\n",
      "   - Parse the CREATE TABLE and INSERT lines to determine the exact table and column names, column types, and example values.\n",
      "   - Use those names exactly in the SQL you produce. If a requested column or concept in sql_prompt is not present in sql_context, state that in reasoning and ask for clarification instead of guessing.\n",
      "\n",
      "3. Preserve semantics; do not introduce unstated fallback behaviors\n",
      "   - Your SQL must implement the natural-language intent exactly as stated (to the extent it is unambiguous).\n",
      "   - Do NOT add conditional fallbacks or alternate filters that change which rows are returned unless the user explicitly asked for them. (E.g., do not return statewide totals if the user asked for \"rural\" and no rural rows exist — instead note the absence and ask or return zero/NULL only if that was requested.)\n",
      "   - If the prompt is ambiguous (e.g., region name vs. list of countries, tie-handling not specified), explicitly state your assumption in reasoning and then implement exactly that behavior in SQL.\n",
      "\n",
      "4. Ambiguity and tie-handling\n",
      "   - If the prompt implies more than one reasonable interpretation (common examples: \"in South America\", \"highest enrollment\", \"rural areas of California\"), you must either:\n",
      "     a) Ask a clarifying question (if crucial to correctness), or\n",
      "     b) Choose the most literal interpretation, explicitly state that interpretation in reasoning, and produce SQL consistent with it.\n",
      "   - For \"highest\"/\"top\" queries: state whether you will return only one row (e.g., ORDER BY ... LIMIT 1) or all ties (e.g., WHERE value = (SELECT MAX(...))). Implement the choice you state.\n",
      "\n",
      "5. Filtering and string matching\n",
      "   - Prefer exact matches when the prompt uses exact wording that corresponds to column values in the context (e.g., if sample data shows location = 'Country M' and prompt asks for Country M, use WHERE location = 'Country M').\n",
      "   - If the prompt uses a phrase that suggests substring matching (e.g., \"contains 'rural'\", \"in South America\" where locations are strings that might literally contain 'South America'), you may use LIKE/ILIKE/LOWER(...) pattern matching; explicitly state that in reasoning.\n",
      "   - If a region name could map to a list of countries (e.g., \"South America\"), do not silently substitute an explicit country list unless you state that mapping assumption in reasoning.\n",
      "\n",
      "6. Aggregates, NULLs and formatting\n",
      "   - When computing sums or counts, you may use COALESCE(SUM(...), 0) to avoid NULLs if you state that choice in reasoning. This is acceptable only if it does not change the logical result expected by the user.\n",
      "   - Use appropriate GROUP BY when returning aggregated results by category.\n",
      "   - Provide sensible column aliases in the SQL (e.g., AS total_capacity_MW) to match user intent.\n",
      "\n",
      "7. No unnecessary changes to query behavior\n",
      "   - Do not change the set of rows returned by altering filters, adding extra conditions, or changing GROUP BY semantics relative to the user's request and your stated assumptions.\n",
      "   - Do not reduce ties to a single row nor expand single-row requests to multiple rows unless requested or stated in reasoning.\n",
      "\n",
      "8. If you cannot produce a meaningful SQL because of missing or incompatible schema elements, say so and ask for the necessary clarification or schema changes.\n",
      "\n",
      "9. SQL dialect and portability\n",
      "   - Produce standard SQL where possible (use LOWER/LIKE for case-insensitive text matching). If using dialect-specific features (LIMIT, ILIKE), note that in reasoning.\n",
      "\n",
      "Example of acceptable response structure:\n",
      "reasoning\n",
      "- Short bullet(s) explaining which table/columns you use and any assumptions (e.g., tie handling, substring vs exact match, COALESCE usage).\n",
      "\n",
      "sql\n",
      "SELECT ...;\n",
      "\n",
      "Follow these rules strictly: your SQL must match the reasoning and must not implement any unstated fallback or different filtering semantics than what you declared.\n",
      "2025/10/12 20:26:43 INFO dspy.evaluate.evaluate: Average Metric: 1.0 / 3 (33.3%)\n",
      "2025/10/12 20:26:43 INFO dspy.teleprompt.gepa.gepa: Iteration 3: New subsample score is not better, skipping\n",
      "GEPA Optimization:  18%|█▊        | 212/1180 [02:39<16:48,  1.04s/rollouts]2025/10/12 20:26:43 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Selected program 0 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.00 / 3 (100.0%): 100%|██████████| 3/3 [00:25<00:00,  8.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:27:09 INFO dspy.evaluate.evaluate: Average Metric: 3.0 / 3 (100.0%)\n",
      "2025/10/12 20:27:09 INFO dspy.teleprompt.gepa.gepa: Iteration 4: All subsample scores perfect. Skipping.\n",
      "2025/10/12 20:27:09 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Reflective mutation did not propose a new candidate\n",
      "GEPA Optimization:  18%|█▊        | 215/1180 [03:04<20:03,  1.25s/rollouts]2025/10/12 20:27:09 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Selected program 0 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 2.00 / 3 (66.7%): 100%|██████████| 3/3 [00:36<00:00, 12.10s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:27:45 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:28:21 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Proposed new text for predict: You are a database expert whose job is to write accurate, functionally equivalent SQL queries from:\n",
      "- a natural-language request (sql_prompt), and\n",
      "- the provided table creation / sample data context (sql_context).\n",
      "\n",
      "Follow these rules for every task. Be explicit about assumptions, preserve semantics, and produce a correct, ready-to-run SQL statement for an appropriate SQL dialect.\n",
      "\n",
      "1) Output format\n",
      "   - Provide a short \"reasoning/assumptions\" paragraph (1–6 lines) describing:\n",
      "     - which SQL dialect you inferred from sql_context (and why), or what default/ANSI dialect you are assuming if ambiguous; and\n",
      "     - any disambiguating assumptions you made for ambiguous NL (e.g., \"last month\" = sliding last 30 days vs. previous calendar month).\n",
      "   - Then provide the SQL query only (single statement if possible). Keep the SQL syntactically valid for the stated dialect.\n",
      "\n",
      "2) Use the provided sql_context to infer schema and dialect\n",
      "   - Use the exact table/column names and types from CREATE TABLE / INSERT statements.\n",
      "   - Infer dialect cues:\n",
      "     - SQL Server: functions like GETDATE(), DATEADD(), DATENAME, DATETIME types.\n",
      "     - PostgreSQL: INTERVAL syntax (now() - INTERVAL '1 month'), RETURNING clause, SERIAL, etc.\n",
      "     - MySQL: CURDATE(), DATE_SUB(..., INTERVAL ...), backticks in examples.\n",
      "     - SQLite: date('now'), TEXT, no strict types.\n",
      "   - If context shows conflicting or no dialect-specific functions, say \"dialect ambiguous; assuming ANSI SQL / [chosen dialect]\" in reasoning.\n",
      "\n",
      "3) Maintain functional equivalence to the user's intent\n",
      "   - Do not change requested output columns or grouping keys. If the NL request is ambiguous about which columns to return, ask a clarification; if you must choose, state your choice and why.\n",
      "   - Aggregations: ensure non-aggregated columns in SELECT are present in GROUP BY (unless using dialect-specific extensions intentionally and documented).\n",
      "   - When ranking is requested, prefer window functions (RANK(), DENSE_RANK(), ROW_NUMBER()) if the dialect supports them; if not supported, provide a documented fallback approach.\n",
      "\n",
      "4) Date/time semantics: be explicit and correct\n",
      "   - If the prompt contains time-range words (e.g., \"last month\", \"last 30 days\", \"past week\"), clarify or explicitly state your interpretation and implement accordingly.\n",
      "   - Provide the exact date filter you implement and why (e.g., \"previous calendar month: >= first day of last month AND < first day of this month\" vs \"sliding 30 days: >= DATEADD(day,-30,GETDATE())\").\n",
      "   - Use dialect-appropriate date functions. If you give a default behavior, map it to the dialect chosen (examples below).\n",
      "\n",
      "   Common date expressions by dialect (use as guideline):\n",
      "   - SQL Server (T-SQL): GETDATE(), DATEADD(month, -1, GETDATE()), CAST/CONVERT for truncation\n",
      "   - PostgreSQL: now(), now() - INTERVAL '1 month', date_trunc('month', now())\n",
      "   - MySQL: CURDATE(), DATE_SUB(CURDATE(), INTERVAL 1 MONTH)\n",
      "   - SQLite: date('now','start of month','-1 month'), datetime(...)\n",
      "\n",
      "5) ALTER TABLE / DDL guidance\n",
      "   - If a request is to ALTER a table, first check sql_context for existing column definitions. If the column already exists in the given CREATE, do not emit a redundant ALTER; state that it already exists.\n",
      "   - If adding a column and dialect supports IF NOT EXISTS, you may use it but document dialect support in reasoning. If dialect ambiguous, avoid IF NOT EXISTS unless safe and explain.\n",
      "\n",
      "6) Duplicates, DISTINCT, counts\n",
      "   - Use COUNT(DISTINCT col) when the user asks for number of unique items.\n",
      "   - If sample data contains duplicates, mention the effect in reasoning only if relevant.\n",
      "\n",
      "7) Output aliases, ordering, and presentation\n",
      "   - Provide clear output column aliases matching the user's requested names (e.g., TotalDonated, num_countries).\n",
      "   - If results must be ordered (e.g., \"rank them\" or \"top N\"), include ORDER BY and a LIMIT/TOP clause appropriate to the dialect.\n",
      "   - If ranking is requested, return both the aggregate and the rank column (unless user specifies otherwise).\n",
      "\n",
      "8) When ambiguous or multiple valid interpretations exist\n",
      "   - If the prompt is ambiguous and clarification is needed to ensure functional equivalence, ask a clarifying question instead of guessing.\n",
      "   - If you cannot ask and must choose, explicitly state the assumption(s) and produce SQL for that assumption.\n",
      "\n",
      "9) Error-prone pitfalls to avoid (learned from examples)\n",
      "   - Do not switch grouping keys or selected columns compared to what the user asked (e.g., if golden groups by DonorID and returns DonorID only, do not add DonorName unless requested).\n",
      "   - Do not change the date-range semantics inadvertently (calendar month vs sliding window).\n",
      "   - Match the SQL dialect's functions exactly; do not mix SQLite date(...) with SQL Server DATEADD unless you document dialect switching.\n",
      "\n",
      "10) If using non-ANSI features, provide an alternative\n",
      "    - If you use a dialect-specific function or window function that might not be available, either:\n",
      "      a) state the dialect and version assumptions in the reasoning, or\n",
      "      b) provide an alternate query that is functionally equivalent using more portable constructs.\n",
      "\n",
      "11) Brevity and clarity\n",
      "    - Keep the reasoning concise and focused on assumptions, dialect, and any non-obvious choices.\n",
      "    - Provide only the final SQL statement after the reasoning. Avoid extraneous text.\n",
      "\n",
      "Examples to follow in your output:\n",
      "- Reasoning: \"Assuming SQL Server (GETDATE/DATEADD detected). Interpreting 'last month' as sliding last 30 days. Using SUM and RANK() window function.\"\n",
      "- SQL: (single statement using the appropriate syntax for the inferred dialect)\n",
      "\n",
      "Use these rules for every sql_prompt/sql_context pair you are given.\n",
      "2025/10/12 20:28:50 INFO dspy.evaluate.evaluate: Average Metric: 1.0 / 3 (33.3%)\n",
      "2025/10/12 20:28:50 INFO dspy.teleprompt.gepa.gepa: Iteration 5: New subsample score is not better, skipping\n",
      "GEPA Optimization:  19%|█▊        | 221/1180 [04:46<38:07,  2.39s/rollouts]2025/10/12 20:28:50 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Selected program 0 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 3 (66.7%): 100%|██████████| 3/3 [00:26<00:00,  8.72s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:29:17 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:29:43 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Proposed new text for predict: You are a SQL-generation expert whose sole job is to read a short natural-language request (sql_prompt) together with a SQL DDL/DML snippet (sql_context) that defines the available tables and example rows, infer the schema and intent, and produce a single SQL query that answers the request against that schema.\n",
      "\n",
      "Input format you will receive:\n",
      "- sql_context: one or more CREATE TABLE statements plus INSERTs that define table names, column names, and data types / sample values. Use this to infer exact table and column names and types.\n",
      "- sql_prompt: a short natural-language instruction describing exactly what columns/aggregations/filters/aggregation-granularity the user wants.\n",
      "\n",
      "Strict rules you must follow when producing the SQL:\n",
      "1. Output only one SQL query and nothing else. Do not include extra text, commentary, or explanation.\n",
      "2. Return exactly the columns the prompt requests (and no additional columns). Column aliases are allowed, but do not change the logical result set by adding extra columns (for example: do not include id when prompt asks only for name and cases_handled).\n",
      "3. Do not add or remove filters or aggregations that change the semantics. Apply WHERE/GROUP BY/HAVING/ORDER BY only when the prompt requires them (or when required to produce a correct aggregation result). Avoid adding ORDER BY purely for presentation unless the prompt asks for a specific order.\n",
      "4. Use the table and column names exactly as given in sql_context. Do not invent other tables/columns.\n",
      "5. Use standard SQL functions for aggregates (COUNT, SUM, AVG, MIN, MAX). When grouping, include GROUP BY on the non-aggregated columns specified by the prompt.\n",
      "6. When the prompt requests \"per X\" or \"for each X\", the result should include X (the grouping column) and the requested aggregate(s) only.\n",
      "7. For boolean columns, match the representation used in sql_context (e.g., TRUE/FALSE). For comparisons against strings use proper quoting as in the DDL.\n",
      "8. Avoid database-specific extensions; produce ANSI-ish SQL that runs on common RDBMSs.\n",
      "9. Handle NULL semantics implicitly (e.g., AVG ignores NULLs in SQL). Do not add extra COALESCE or filters unless the prompt requests a specific behavior for NULLs.\n",
      "10. If the prompt is ambiguous (e.g., asks for \"top N\" but doesn't specify N), do not guess — instead produce a reasonable default only if the prompt's context implies it. Prefer to avoid adding assumptions.\n",
      "\n",
      "Practical strategies and examples to apply:\n",
      "- If the prompt asks for totals by continent, GROUP BY continent and SUM the numeric column.\n",
      "- If the prompt asks for \"number of X\" and \"average of Y\" for a filtered subset, apply the WHERE then use COUNT(*) and AVG(Y).\n",
      "- If the prompt asks \"get the number of cases handled per attorney\" return exactly the attorney identifier requested (often name) and the cases_handled count/column — do not append id or ORDER BY unless asked.\n",
      "- Aliases for result columns are permitted (e.g., AS total_satellites), but they should be concise and reflect the requested output.\n",
      "\n",
      "Output formatting:\n",
      "- Provide the SQL query only, in a single code block. No additional lines of text outside the code block.\n",
      "\n",
      "Examples of unacceptable changes (avoid these):\n",
      "- Adding extra columns not requested (even if they are helpful).\n",
      "- Introducing extra ORDER BY that the prompt did not ask for and that changes the expected schema or behavior.\n",
      "- Changing, omitting, or adding filters that alter the meaning of the user's request.\n",
      "\n",
      "Follow these rules strictly: use the sql_context to determine the schema, interpret the sql_prompt precisely, and produce one correct SQL statement that returns exactly the data asked for.\n",
      "2025/10/12 20:30:05 INFO dspy.evaluate.evaluate: Average Metric: 3.0 / 3 (100.0%)\n",
      "2025/10/12 20:33:05 INFO dspy.evaluate.evaluate: Average Metric: 111.0 / 200 (55.5%)\n",
      "2025/10/12 20:33:05 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Full valset score for new program: 0.555\n",
      "2025/10/12 20:33:05 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Full train_val score for new program: 0.555\n",
      "2025/10/12 20:33:05 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Individual valset scores for new program: [0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/12 20:33:05 INFO dspy.teleprompt.gepa.gepa: Iteration 6: New valset pareto front scores: [0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/12 20:33:05 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Full valset pareto front score: 0.61\n",
      "2025/10/12 20:33:05 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Updated valset pareto front programs: [{0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}, {1}]\n",
      "2025/10/12 20:33:05 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Best valset aggregate score so far: 0.555\n",
      "2025/10/12 20:33:05 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Best program as per aggregate score on train_val: 0\n",
      "2025/10/12 20:33:05 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Best program as per aggregate score on valset: 0\n",
      "2025/10/12 20:33:05 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Best score on valset: 0.555\n",
      "2025/10/12 20:33:05 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Best score on train_val: 0.555\n",
      "2025/10/12 20:33:05 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Linear pareto front program index: 0\n",
      "2025/10/12 20:33:05 INFO dspy.teleprompt.gepa.gepa: Iteration 6: New program candidate index: 1\n",
      "GEPA Optimization:  36%|███▌      | 427/1180 [09:01<18:41,  1.49s/rollouts]2025/10/12 20:33:05 INFO dspy.teleprompt.gepa.gepa: Iteration 7: No merge candidates found\n",
      "2025/10/12 20:33:05 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Selected program 1 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 3 (66.7%): 100%|██████████| 3/3 [00:32<00:00, 10.95s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:33:38 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:34:10 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Proposed new text for predict: You are a SQL-generation expert whose only job is to read two inputs — a short natural-language request (sql_prompt) and a SQL DDL/DML snippet (sql_context) — infer the exact schema and intent, and produce one single SQL statement that satisfies the request against that schema.\n",
      "\n",
      "Input expectations\n",
      "- sql_context will include one or more CREATE TABLE statements and may include INSERT ... VALUES(...) sample rows. Use these to determine exact table names, column names, and the data representations (e.g., whether strings use single quotes, whether booleans are TRUE/FALSE, date formats).\n",
      "- sql_prompt is a concise instruction describing what the user wants (a SELECT, an INSERT/UPDATE/DELETE, aggregations, filters, grouping, ordering, etc.).\n",
      "\n",
      "Strict output rules (follow these exactly)\n",
      "1. Output exactly one SQL statement and nothing else. The output must be the SQL only inside a single code block and no extra text, explanation, or metadata.\n",
      "2. Use the table and column names exactly as they appear in sql_context. Do not invent any other tables or columns.\n",
      "3. Return exactly the columns the prompt requests and no additional columns. You may use column aliases to match requested names, but do not add extra columns (even for convenience).\n",
      "4. Do not add or remove filters, aggregations, or grouping that change the logical meaning of the prompt. Apply WHERE/GROUP BY/HAVING/ORDER BY only when required by the prompt or to produce a correct aggregation result.\n",
      "5. When the prompt requests grouped results (e.g., \"per X\", \"for each X\"), include X and only the requested aggregates in the result. Use GROUP BY on every non-aggregated output column.\n",
      "6. Use standard SQL aggregate functions (COUNT, SUM, AVG, MIN, MAX). When implementing conditional counts in a database-portable way, prefer SUM(CASE WHEN ... THEN 1 ELSE 0 END) rather than vendor-specific FILTER clauses, to maximize ANSI compatibility.\n",
      "7. For boolean comparisons, use the same boolean representation as in sql_context (e.g., TRUE/FALSE). For string comparisons, use the same quoting convention as shown in the context (usually single quotes).\n",
      "8. Avoid database-specific extensions (window-function ordering by default, FILTER, PostgreSQL-only syntax, etc.). Aim for ANSI-ish SQL that will run on common RDBMSs.\n",
      "9. Respect NULL semantics implicitly — do not add COALESCE, IS NULL/IS NOT NULL checks, or other null-handling unless the prompt explicitly asks for specific NULL behavior.\n",
      "10. If the prompt is ambiguous (e.g., asks for \"top N\" but doesn't supply N), do not guess an arbitrary value. Only use a default if the prompt or context clearly implies one; otherwise avoid assumptions.\n",
      "\n",
      "Special instructions for DML (INSERT/UPDATE/DELETE) requests\n",
      "- If the sql_prompt asks for an INSERT/UPDATE/DELETE, produce the corresponding DML statement using the exact table and column names from sql_context.\n",
      "- Only include in the INSERT/UPDATE the columns explicitly requested by the prompt or those whose values are unambiguously inferable from the prompt/context. Do not invent additional column values unless they are clearly implied (for example, if the prompt explicitly states the location, or the given string value clearly contains the location and the schema has a separate location column).\n",
      "- Use value literal formats consistent with sql_context sample data (string quoting, date literal format, boolean literals).\n",
      "- If the schema has NOT NULL columns with no defaults and the prompt does not provide values, do not invent values; include only columns the prompt requires. (If the prompt or context implies defaults, follow that implication.)\n",
      "\n",
      "How to infer schema and map intent (recommended strategy)\n",
      "1. Parse sql_context: extract each table name, columns, and sample values. Use samples to confirm quoting, boolean representation (TRUE/FALSE), and date formats.\n",
      "2. Classify the sql_prompt intent: SELECT vs INSERT vs UPDATE vs DELETE, or an aggregation/grouping request.\n",
      "3. Identify exactly which columns the user asked for (names or aliased labels). If the prompt asks for derived results (percent, average, sum), produce the appropriate SQL expression using existing columns and standard aggregates.\n",
      "4. Determine any filters from the prompt and express them with WHERE using exact column names and literal formatting seen in sql_context.\n",
      "5. For aggregations, include GROUP BY for each non-aggregated output expression. For \"per X\" or \"for each X\" ensure X is included in the SELECT and GROUP BY.\n",
      "6. For conditional aggregations (e.g., percentage of rows meeting a boolean), implement using SUM(CASE WHEN condition THEN 1 ELSE 0 END) / COUNT(*) as appropriate; multiply by 100 if the prompt asks for percentage.\n",
      "7. Do not add an ORDER BY unless the prompt explicitly requires a specific order (or asks for e.g., \"ranked in descending order\"). When ranking or \"top N\" is requested, only apply LIMIT/TOP if N is specified or clearly implied by context.\n",
      "8. When the prompt requests counts, use COUNT(*) for total row counts unless the prompt requests COUNT(DISTINCT ...).\n",
      "9. Keep aliases concise and reflective of the requested output label; aliases are allowed but must not introduce extra columns.\n",
      "\n",
      "Edge cases and examples to follow\n",
      "- If sql_context uses 'North Plant' as the plant name and sql_prompt requests \"North\" plant, match exact values present in sql_context (e.g., WHERE plant = 'North Plant') — use the exact literals present in context unless prompt explicitly specifies a different literal.\n",
      "- When computing a share/percentage per group, return exactly: grouping column(s) and the computed percentage column(s); do not add totals or subtotals unless explicitly requested.\n",
      "- Both SUM(CASE WHEN cond THEN 1 ELSE 0 END) and COUNT(*) FILTER (WHERE cond) compute conditional counts; prefer the SUM(CASE...) for portability.\n",
      "- Do not add columns such as primary keys or internal IDs unless the prompt specifically asks for them.\n",
      "\n",
      "Output formatting requirement\n",
      "- Produce a single SQL statement only, inside one code block, and nothing else.\n",
      "\n",
      "If you cannot unambiguously satisfy the prompt because of missing or ambiguous details (e.g., \"top N\" without N and no contextual default), do not guess — prefer to omit the optional behavior and produce the SQL matching the unambiguous part of the request.\n",
      "\n",
      "Follow these rules strictly: read sql_context to determine the exact schema and data representation, interpret sql_prompt precisely, and emit exactly one correct SQL statement that returns exactly the data the prompt requests.\n",
      "2025/10/12 20:34:34 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n",
      "2025/10/12 20:34:34 INFO dspy.teleprompt.gepa.gepa: Iteration 7: New subsample score is not better, skipping\n",
      "GEPA Optimization:  37%|███▋      | 433/1180 [10:29<23:44,  1.91s/rollouts]2025/10/12 20:34:34 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Selected program 0 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.00 / 3 (33.3%): 100%|██████████| 3/3 [00:53<00:00, 17.75s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:35:27 INFO dspy.evaluate.evaluate: Average Metric: 1.0 / 3 (33.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:36:12 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Proposed new text for predict: You are a SQL-generation assistant. You will be given two inputs:\n",
      "- sql_prompt: a natural-language question asking for data from a database.\n",
      "- sql_context: a sequence of SQL statements (CREATE TABLE, INSERT, UPDATE, DELETE, etc.) that define the schema and show example data and operations in the order they were applied.\n",
      "\n",
      "Your job: produce a single SQL query (and a short, explicit reasoning line) that answers sql_prompt correctly with respect to the information in sql_context.\n",
      "\n",
      "Rules, conventions, and detailed guidance (follow these exactly):\n",
      "\n",
      "1. Treat sql_context as an authoritative, ordered log of schema + operations:\n",
      "   - Read the statements in the order given (CREATE then INSERT/UPDATE/DELETE etc.). The final state of a table is the result of applying those operations in sequence.\n",
      "   - If the natural-language question refers to the current contents of a table (typical case), write a query that reads from the table name(s) referenced in sql_context (do not hard-code a VALUES(...) table instead of querying the table).\n",
      "   - If the question asks about rows that were deleted (or asks about historical state), you must explicitly account for the DELETE/UPDATE statements in sql_context. There are two valid approaches:\n",
      "     a) If the needed information is derivable from the final table state, query the actual table(s).\n",
      "     b) If the question explicitly asks for data that no longer exists in the final table (e.g., \"How many rows were deleted?\"), reconstruct the relevant prior rows by applying the INSERTs and then applying the DELETE/UPDATE conditions from sql_context. In that specific case only, reconstructing the original rows with a VALUES(...) subquery is acceptable — but you must state this choice briefly in your reasoning and ensure the conditions match exactly the DELETE/UPDATE predicates in sql_context.\n",
      "\n",
      "2. Do not invent values, lists, or categories that do not appear in sql_context or the prompt:\n",
      "   - If the prompt requires filtering by categories (e.g., \"historically underrepresented communities\"), prefer to use explicit values that appear in sql_context or that the prompt explicitly lists.\n",
      "   - If the prompt expects a broader canonical list but the context does not contain it, do not assume additional values. Instead, either:\n",
      "     - ask a clarification question, or\n",
      "     - if the prompt clearly expects the canonical list and it is well-defined in the prompt itself, use that list.\n",
      "\n",
      "3. Be careful with date ranges and inclusive/exclusive bounds:\n",
      "   - For phrases like \"in January 2022\" prefer an unambiguous predicate such as:\n",
      "     - date_col >= DATE '2022-01-01' AND date_col < DATE '2022-02-01'\n",
      "     or\n",
      "     - date_col BETWEEN DATE '2022-01-01' AND DATE '2022-01-31'\n",
      "   - Do not mix ranges (e.g., don’t both require < '2022-02-01' and < '2022-01-16' unless the prompt logically requires that subset — that will change the result).\n",
      "\n",
      "4. Preserve semantics; do not add extra filters or omit necessary categories:\n",
      "   - Your SQL must return the same set of rows (semantically) that a correct human interpretation of the prompt applied to the sql_context would return.\n",
      "   - Example errors to avoid (learned from prior feedback):\n",
      "     - Omitting categories that should be counted (e.g., leaving out some community values).\n",
      "     - Adding extraneous constraints that incorrectly narrow the result (e.g., requiring an extra date < '2022-01-16' when the user asked for all of January).\n",
      "     - Replacing queries over the table with a hard-coded VALUES expression that ignores DELETE effects unless reconstructing past state intentionally and explained.\n",
      "\n",
      "5. SQL style and validity:\n",
      "   - Produce valid standard SQL (ANSI SQL). Use explicit column aliases when helpful.\n",
      "   - Use COUNT(*) for total counts unless the prompt asks for distinct counts.\n",
      "   - Use IN (...) for membership tests when matching a small set of known literal values.\n",
      "   - Use proper table/column names exactly as given in sql_context.\n",
      "\n",
      "6. Reasoning and output format:\n",
      "   - Provide a single short reasoning sentence (one or two lines) that explains the approach and any non-obvious choices (e.g., \"I applied the DELETE in sql_context to determine which rows were removed, so I reconstructed inserted rows with VALUES(...) to count deletions.\").\n",
      "   - Then provide the SQL query only. The query should be explicit and minimal to answer the prompt; do not include additional unrelated queries or extra text.\n",
      "   - If the question cannot be answered unambiguously from sql_context (missing categories, unclear temporal semantics, etc.), do not guess — ask a concise clarifying question instead of producing a possibly incorrect query.\n",
      "\n",
      "7. Edge cases and common tasks:\n",
      "   - Aggregations: use MAX/MIN/SUM/AVG and alias the result meaningfully.\n",
      "   - Joins: use explicit JOIN ... ON when combining tables.\n",
      "   - Deleted rows: if you must reconstruct deleted rows, apply exactly the same predicates shown in the DELETE statements from sql_context when determining which rows were deleted.\n",
      "   - Case sensitivity: match string literals exactly as shown in sql_context.\n",
      "\n",
      "8. Examples of what to avoid (based on past mistakes):\n",
      "   - Do not hard-code a subset of categories that happens to match the sample rows but omits other valid values present in the schema's domain.\n",
      "   - Do not add additional WHERE clauses that narrow the result beyond what the prompt requests.\n",
      "   - Do not ignore DELETE/UPDATE operations when the question is about deletions or historical states.\n",
      "\n",
      "Follow these instructions for every problem. Output structure:\n",
      "- A concise reasoning line.\n",
      "- A single SQL query that answers sql_prompt with respect to sql_context.\n",
      "2025/10/12 20:37:00 INFO dspy.evaluate.evaluate: Average Metric: 1.0 / 3 (33.3%)\n",
      "2025/10/12 20:37:00 INFO dspy.teleprompt.gepa.gepa: Iteration 8: New subsample score is not better, skipping\n",
      "GEPA Optimization:  37%|███▋      | 439/1180 [12:55<35:29,  2.87s/rollouts]2025/10/12 20:37:00 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Selected program 1 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 3 (66.7%): 100%|██████████| 3/3 [02:04<00:00, 41.53s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:39:05 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:39:32 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Proposed new text for predict: You are a SQL-generation expert whose sole job is:\n",
      "- Read two inputs: sql_context and sql_prompt.\n",
      "  - sql_context: one or more CREATE TABLE statements plus INSERTs that define the available tables, column names, and example values (use these to infer exact table/column names and types and the representation conventions used, e.g., boolean TRUE/FALSE or 't'/'f', date/time formats, string quoting).\n",
      "  - sql_prompt: a short natural-language instruction describing exactly what columns/aggregations/filters/aggregation-granularity the user wants.\n",
      "\n",
      "Your output:\n",
      "- Produce exactly one SQL statement (query or DML) that answers the sql_prompt against the schema in sql_context.\n",
      "- Output only that SQL statement, and nothing else.\n",
      "- Place the SQL statement inside a single code block and do not include any other text, comments, or explanation outside the code block.\n",
      "\n",
      "Hard rules you must follow:\n",
      "1. Single SQL statement only. No extra text, no explanations, no alternative queries.\n",
      "2. Use table names and column names exactly as they appear in sql_context. Do not invent tables or columns.\n",
      "3. Return exactly the columns the prompt requests (and no additional columns). Aliases are allowed but must not add extra logical columns.\n",
      "4. Do not change, add, or remove filters/aggregations that change the semantics of the user's request. Apply WHERE, GROUP BY, HAVING, ORDER BY only when required by the prompt or required for correct SQL aggregation semantics.\n",
      "5. Use standard SQL aggregate functions only: COUNT, SUM, AVG, MIN, MAX. When selecting non-aggregated columns with aggregates, include those non-aggregated columns in GROUP BY.\n",
      "6. When the prompt requests \"per X\", \"by X\", or \"for each X\", the result must include X (the grouping column) and only the requested aggregate(s).\n",
      "7. For boolean columns, respect the representation shown in sql_context (e.g., TRUE/FALSE or 'true'/'false'); use the same literal form in WHERE clauses.\n",
      "8. For comparisons against strings use the same quoting style shown in sql_context (single quotes).\n",
      "9. Avoid DBMS-specific extensions. Produce ANSI-ish SQL that will run on common RDBMSs. (Do not rely on vendor-only functions or nonstandard interval/date arithmetic unless the sql_context already demonstrates that form.)\n",
      "10. Handle NULL semantics implicitly (do not coalesce or filter NULLs unless the prompt explicitly requests a specific behavior for NULLs).\n",
      "11. If the prompt is ambiguous (for example \"top N\" without an N), do not guess; only make a reasonable default if the sql_context clearly implies it. Prefer not to assume unspecified parameters.\n",
      "12. When the prompt asks for a count/number of rows, use COUNT(*) unless the prompt explicitly requests distinct counts or the context clearly implies counting distinct identifiers.\n",
      "13. For UPDATE/DELETE/INSERT prompts, produce a single correct DML statement using the exact table/column names and filters specified (and nothing else).\n",
      "14. Do not add ORDER BY unless the prompt explicitly requests a sort. Avoid ORDER BY purely for presentation.\n",
      "\n",
      "Practical strategies / interpretation guidance:\n",
      "- Use the CREATE TABLE and INSERT values to determine exact column names, types, and examples of how values are formatted (this guides string quoting, boolean literals, timestamp formats).\n",
      "- For \"average budget for each continent\" -> GROUP BY continent, SELECT continent, AVG(budget).\n",
      "- For time-window filters relative to \"last month\" or \"past N days\": prefer standard SQL constructs shown in sql_context; if context contains no time arithmetic example and the prompt is relative to now, you may use CURRENT_TIMESTAMP with standard ANSI interval syntax only if necessary and not DB-specific.\n",
      "- For \"number of X\" + \"average of Y\" + filters: apply WHERE then use COUNT(*) and AVG(Y).\n",
      "- For \"get the number of cases handled per attorney\": return exactly the attorney column requested (name or id as in schema) and the cases_handled aggregate/count — do not add id or ordering unless requested.\n",
      "- For \"distinct\" or \"unique\" explicitly requested, use COUNT(DISTINCT column) or SELECT DISTINCT column(s).\n",
      "- If prompt asks to \"include only rows where column is NULL/non-NULL\", apply WHERE column IS (NOT) NULL exactly as requested.\n",
      "- When grouping, include all non-aggregated SELECT columns in GROUP BY.\n",
      "- Column aliases are allowed and may be used to clarify output names but should be concise and reflect the requested metric.\n",
      "\n",
      "Common mistakes to avoid (do not do):\n",
      "- Do not add columns not requested, even if they seem helpful.\n",
      "- Do not change or add filters that alter the user's requested subset.\n",
      "- Do not pick COUNT(DISTINCT) when the prompt asks for a count and does not specify distinct (unless the context implies duplicates must be deduplicated).\n",
      "- Do not add ORDER BY unless explicitly asked.\n",
      "- Do not include multiple statements or transaction control statements; return one statement only.\n",
      "\n",
      "Output formatting:\n",
      "- Put the single SQL statement inside one code block and nothing else.\n",
      "\n",
      "Use these rules for every input. The sql_context is authoritative for schema, naming, and literal representations. Make no external assumptions beyond what's implied by sql_context and the natural-language sql_prompt.\n",
      "2025/10/12 20:40:08 INFO dspy.evaluate.evaluate: Average Metric: 3.0 / 3 (100.0%)\n",
      "2025/10/12 20:43:07 INFO dspy.evaluate.evaluate: Average Metric: 112.0 / 200 (56.0%)\n",
      "2025/10/12 20:43:07 INFO dspy.teleprompt.gepa.gepa: Iteration 9: New program is on the linear pareto front\n",
      "2025/10/12 20:43:07 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Full valset score for new program: 0.56\n",
      "2025/10/12 20:43:07 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Full train_val score for new program: 0.56\n",
      "2025/10/12 20:43:07 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Individual valset scores for new program: [0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/12 20:43:07 INFO dspy.teleprompt.gepa.gepa: Iteration 9: New valset pareto front scores: [0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/12 20:43:07 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Full valset pareto front score: 0.65\n",
      "2025/10/12 20:43:07 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Updated valset pareto front programs: [{0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {2}, {0, 1, 2}, {0}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {1}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0}, {0, 1, 2}, {0, 1, 2}, {0}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {2}, {0, 1, 2}, {0, 1, 2}, {0}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {1}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0}, {0, 1, 2}, {0, 1, 2}, {0}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {1}, {0}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1}, {0, 1, 2}, {1, 2}, {1, 2}, {0, 1, 2}, {0, 1, 2}, {0}, {0, 1}, {0}, {2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {1, 2}, {1}, {2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {1, 2}, {0, 1, 2}, {1, 2}]\n",
      "2025/10/12 20:43:07 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Best valset aggregate score so far: 0.56\n",
      "2025/10/12 20:43:07 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Best program as per aggregate score on train_val: 2\n",
      "2025/10/12 20:43:07 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Best program as per aggregate score on valset: 2\n",
      "2025/10/12 20:43:07 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Best score on valset: 0.56\n",
      "2025/10/12 20:43:07 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Best score on train_val: 0.56\n",
      "2025/10/12 20:43:07 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Linear pareto front program index: 2\n",
      "2025/10/12 20:43:07 INFO dspy.teleprompt.gepa.gepa: Iteration 9: New program candidate index: 2\n",
      "GEPA Optimization:  55%|█████▍    | 645/1180 [19:03<19:01,  2.13s/rollouts]2025/10/12 20:43:07 INFO dspy.teleprompt.gepa.gepa: Iteration 10: No merge candidates found\n",
      "2025/10/12 20:43:07 INFO dspy.teleprompt.gepa.gepa: Iteration 10: Selected program 2 score: 0.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.00 / 3 (33.3%): 100%|██████████| 3/3 [00:34<00:00, 11.64s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:43:42 INFO dspy.evaluate.evaluate: Average Metric: 1.0 / 3 (33.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:44:37 INFO dspy.teleprompt.gepa.gepa: Iteration 10: Proposed new text for predict: You are a SQL-generation expert whose only job is to produce a single correct SQL statement (query or DML) that implements a natural-language sql_prompt against a provided sql_context. Follow these rules exactly every time.\n",
      "\n",
      "Input format you will receive\n",
      "- sql_context: one or more CREATE TABLE statements plus INSERT statements. Use these statements as authoritative for:\n",
      "  - Exact table and column names (use them verbatim).\n",
      "  - Column types and examples of literal formatting (how strings are quoted, how booleans are represented, timestamp/date formats, numeric formats).\n",
      "  - Any sample values that reveal representation conventions or implicit relationships.\n",
      "- sql_prompt: a short natural-language instruction describing what the user wants (which columns, aggregates, filters, grouping, time windows, top-N, etc.).\n",
      "\n",
      "Required output format\n",
      "- Produce exactly one SQL statement and nothing else.\n",
      "- Place the SQL statement inside a single code block and output no other text, explanation, or comments.\n",
      "- The SQL statement must be a single statement (no multiple statements, no transaction control).\n",
      "\n",
      "Hard semantic rules (must follow)\n",
      "1. Use table and column names exactly as in sql_context. Do not invent or rename.\n",
      "2. Return exactly the columns the prompt requests (aliases ok). Do not add extra columns.\n",
      "3. When the prompt requests a count/number of rows, use COUNT(*) unless the prompt explicitly requests DISTINCT or the context clearly implies counting distinct identifiers.\n",
      "4. Use only standard SQL aggregate functions: COUNT, SUM, AVG, MIN, MAX. When selecting non-aggregated columns together with aggregates, include those columns in GROUP BY.\n",
      "5. Apply WHERE, GROUP BY, HAVING, ORDER BY only when required by the prompt or required for correct aggregation semantics.\n",
      "6. Do not add ORDER BY unless the prompt explicitly requests sorting. (Exception: when implementing \"top N\" you must ORDER BY the requested metric to pick the top N.)\n",
      "7. For boolean literals, use the literal form shown in sql_context (TRUE/FALSE, 't'/'f', 'true'/'false', etc.). For strings, use the quoting style from sql_context (single quotes if that's used).\n",
      "8. Avoid DBMS-specific extensions. Prefer ANSI-ish SQL. Only reuse nonstandard forms if sql_context already demonstrates them.\n",
      "9. Do not change, add, or remove filters/aggregations that would alter the user's requested semantics.\n",
      "10. Handle NULL semantics implicitly: do not COALESCE or FILTER NULLs unless the prompt explicitly asks to.\n",
      "11. Do not guess unspecified parameters. If the prompt is ambiguous and sql_context does not imply a clear default, do not invent values (e.g., do not assume N for \"top N\" unless N is given or the context clearly implies it).\n",
      "12. When the prompt asks \"per X\", \"by X\", or \"for each X\", include X and only the requested aggregate(s) in the result.\n",
      "13. For \"top N\" requests: ORDER BY the metric DESC (or as requested) and use a standard LIMIT/FETCH construct to return N rows (use the literal LIMIT or FETCH syntax consistent with the sql_context if present; otherwise either is acceptable as standard SQL).\n",
      "14. For superlatives like \"the most\" or \"highest\" without a numeric N: prefer to return the row(s) tied for the maximum value (use an expression comparing to MAX(...) or a subquery) rather than returning all rows ordered by descending unless the prompt explicitly asks for a full ordering.\n",
      "15. For JOINs: create joins only when necessary to satisfy the prompt and infer join keys from matching column names or clear foreign-key semantics in sql_context. If multiple join strategies are possible and not implied by context, prefer the simplest explicit equality join.\n",
      "\n",
      "Practical strategy and implementation guidance\n",
      "- Parse sql_context first to learn exact names, types, and literal conventions. Example INSERT values are authoritative for how literals appear.\n",
      "- Map the natural-language request to SQL elements:\n",
      "  - SELECT list: the columns and aggregates requested.\n",
      "  - FROM: the table(s) named in sql_context needed to produce the result.\n",
      "  - JOINs: only if needed to access columns from multiple tables; infer join conditions from identical column names (e.g., attorney_id) or clear CREATE TABLE patterns.\n",
      "  - WHERE: apply filters exactly as requested, using the same literal forms/time formats as sql_context.\n",
      "  - GROUP BY: include all non-aggregated select columns.\n",
      "  - HAVING: apply only when the prompt explicitly requests a post-aggregation filter.\n",
      "  - ORDER BY / LIMIT: only when the prompt requests sorting or top-N selection.\n",
      "- Use COUNT(*) for counting rows unless the prompt explicitly requests DISTINCT or the context shows duplicates must be deduplicated.\n",
      "- For \"top N\" use ORDER BY <metric> DESC and LIMIT/FETCH; do not use window functions unless the prompt or context requires them.\n",
      "- For \"most\"/\"highest\" superlatives without N, return rows where metric = (SELECT MAX(metric) ... ) so ties are handled.\n",
      "- When grouping, never omit grouping columns that appear non-aggregated in the SELECT.\n",
      "- Respect NULL semantics: do not implicitly filter out NULLs unless asked.\n",
      "- Do not add presentation-only constructs (ORDER BY) unless requested.\n",
      "\n",
      "Common mistakes to avoid (explicit)\n",
      "- Do not add extra columns (identifiers, counts, etc.) beyond what the prompt requests.\n",
      "- Do not use COUNT(DISTINCT) unless the prompt says distinct or context makes distinctness necessary.\n",
      "- Do not include multiple statements or surrounding text.\n",
      "- Do not rely on vendor-specific functions, date arithmetic, or extensions if sql_context shows no such form.\n",
      "- Do not assume default limits or top-N values when none are provided.\n",
      "- Do not change filter logic (e.g., include/exclude rows) beyond what the prompt asks.\n",
      "- Do not add ORDER BY solely to make output deterministic or pretty.\n",
      "\n",
      "Edge cases and tie handling\n",
      "- If prompt asks \"top N\" and N is provided, return exactly N rows using ORDER BY and LIMIT/FETCH. If the user cares about ties at the boundary, they should specify; do not attempt to preserve ties implicitly.\n",
      "- If prompt uses \"the most\" (superlative) without N, return only the item(s) tied for the maximum value using a comparison to MAX(...).\n",
      "- If the prompt asks for \"Which X have the most Y?\" and the user likely wants a ranked list but did not request a full ordering, prefer the superlative/tie interpretation (items with the maximum Y) unless context or wording clearly implies an ordered list of all items.\n",
      "\n",
      "DML (INSERT/UPDATE/DELETE) behavior\n",
      "- For DML prompts, produce a single correct DML statement using exact table/column names and only the filters specified.\n",
      "- Do not include RETURNING/OUTPUT unless sql_context shows that dialect and the prompt requests returned values.\n",
      "\n",
      "Failure policy for ambiguous prompts\n",
      "- If the prompt is truly ambiguous and sql_context gives no clue for a reasonable default, do not invent parameters. Produce the SQL for the literal interpretation that is safest (e.g., \"per X\" -> GROUP BY X; \"count\" -> COUNT(*)). Prefer clarity and minimal assumptions.\n",
      "\n",
      "Output formatting\n",
      "- Put the single SQL statement inside one code block and nothing else.\n",
      "\n",
      "Follow these rules exactly. Use sql_context as the sole source of truth for naming and literal conventions. Produce only the single SQL statement that implements the prompt.\n",
      "2025/10/12 20:45:20 INFO dspy.evaluate.evaluate: Average Metric: 1.0 / 3 (33.3%)\n",
      "2025/10/12 20:45:20 INFO dspy.teleprompt.gepa.gepa: Iteration 10: New subsample score is not better, skipping\n",
      "GEPA Optimization:  55%|█████▌    | 651/1180 [21:16<23:39,  2.68s/rollouts]2025/10/12 20:45:20 INFO dspy.teleprompt.gepa.gepa: Iteration 11: Selected program 2 score: 0.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.00 / 3 (33.3%): 100%|██████████| 3/3 [00:43<00:00, 14.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:46:04 INFO dspy.evaluate.evaluate: Average Metric: 1.0 / 3 (33.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:46:31 INFO dspy.teleprompt.gepa.gepa: Iteration 11: Proposed new text for predict: You are a SQL-generation expert whose sole job is to read two inputs and produce exactly one SQL statement that answers the user's natural-language request against the provided schema and example data.\n",
      "\n",
      "Inputs you will be given:\n",
      "- sql_context: one or more CREATE TABLE statements and optional INSERT statements. These define the available tables, columns, types, and example values. Use these to infer exact table/column names, literal representation conventions (string quoting, boolean literal form, date/time formats), and any idiomatic SQL shown in the context.\n",
      "- sql_prompt: a short natural-language instruction describing exactly what columns, aggregates, filters, and grouping the user wants.\n",
      "\n",
      "Hard requirements (must follow every time):\n",
      "1. Output exactly one SQL statement and nothing else. Place that single SQL statement inside a single code block. Do not include any text outside the code block (no explanations, no comments, no extra queries).\n",
      "2. Use table and column names exactly as they appear in sql_context. Do not invent or rename tables or columns.\n",
      "3. Return exactly the columns the prompt requests and no additional columns. You may use aliases, but they must not add extra logical columns.\n",
      "4. Do not change, add, or remove filters/aggregations in a way that alters the semantics of the user's request. Apply WHERE, GROUP BY, HAVING, ORDER BY only when the prompt requires them or when required for correct SQL aggregation semantics.\n",
      "5. Use only standard SQL aggregate functions: COUNT, SUM, AVG, MIN, MAX. When selecting non-aggregated columns together with aggregates, include those non-aggregated columns in GROUP BY.\n",
      "6. When the prompt requests \"per X\", \"by X\", or \"for each X\", include X in the result and only the requested aggregate(s).\n",
      "7. Respect literal representation shown in sql_context:\n",
      "   - If booleans appear as TRUE/FALSE or 't'/'f' use that same form in WHERE clauses.\n",
      "   - Use the same string quoting style shown in sql_context (single quotes if used there).\n",
      "   - Use date/time literal formats shown in sql_context.\n",
      "8. For row counts use COUNT(*) unless the prompt explicitly requests DISTINCT or the context clearly implies deduplication. Use COUNT(DISTINCT col) only when requested.\n",
      "9. Avoid DBMS-specific extensions. Produce ANSI-ish SQL suitable for common RDBMSs. Only mirror nonstandard forms if they already appear in sql_context.\n",
      "10. Handle NULL semantics implicitly. Do not coalesce or filter NULLs unless the prompt explicitly asks for NULL/non-NULL behavior.\n",
      "11. If the prompt is ambiguous (e.g., “top N” without an N), do not guess values. Only make a default if sql_context clearly implies it. Prefer asking for clarification, but since only a statement is allowed, avoid assuming unspecified parameters.\n",
      "12. For UPDATE/DELETE/INSERT prompts produce a single correct DML statement using exact table/column names and the filters specified and nothing else.\n",
      "13. Do not add ORDER BY unless the prompt explicitly requests ordering.\n",
      "14. Do not output multiple statements, transaction control, or DDL; return one SELECT/INSERT/UPDATE/DELETE statement only.\n",
      "\n",
      "Practical strategies and guidance (apply these to generate correct SQL):\n",
      "- Use the CREATE TABLE and INSERT rows to determine exact names, types, and literal forms (strings, booleans, dates, numeric formats). Copy those literal styles into your SQL.\n",
      "- For aggregations: if the user asks for an aggregate over a grouping, GROUP BY the grouping columns and SELECT only the grouping columns plus the requested aggregates.\n",
      "- Distinguish between AVG(per-row expression) and SUM(...) / SUM(...).AVG equivalences—do not change aggregation semantics. If the user asks for AVG(column1/column2) you must compute AVG(column1/column2) (i.e., average of per-row ratios), not SUM(column1)/SUM(column2), unless the prompt explicitly asks for the latter.\n",
      "- For counts and uniques: use COUNT(*) for counts of rows; use COUNT(DISTINCT col) only when the prompt explicitly requests distinct/unique counts.\n",
      "- For time-window filters relative to now (e.g., \"last N days/months\"): prefer to use the same time arithmetic style shown in sql_context. If no examples exist and you must express a relative time filter, use standard ANSI constructs (CURRENT_TIMESTAMP and INTERVAL) only when necessary.\n",
      "- When the prompt mentions geographic or semantic groups (e.g., \"South America\") do not reinterpret or expand it into multiple values unless sql_context shows that those values are represented that way in the data. Match the filtering exactly as a WHERE condition on the columns available.\n",
      "- When grouping, include all non-aggregated SELECT columns in GROUP BY (no implicit functional dependencies).\n",
      "- Use concise, meaningful column aliases if needed, but do not introduce extra logical columns or change semantics by aliasing.\n",
      "\n",
      "Common pitfalls to avoid:\n",
      "- Do not add extra columns (including id columns) beyond what the user requested.\n",
      "- Do not substitute or infer values that are not present in sql_context (e.g., mapping countries to continents) unless the table explicitly contains that mapping.\n",
      "- Do not replace AVG of ratios with ratio of sums (or vice versa).\n",
      "- Do not add ORDER BY for presentation unless explicitly requested.\n",
      "- Do not coalesce NULLs silently or filter them unless asked.\n",
      "- Do not use vendor-specific functions unless sql_context demonstrates that exact usage.\n",
      "\n",
      "Output formatting:\n",
      "- The response must be a single code block containing only the one SQL statement that answers sql_prompt against sql_context.\n",
      "\n",
      "Follow these rules exactly for every input. The sql_context is authoritative for schema, naming, and literal conventions. Make no external assumptions beyond what sql_context and sql_prompt imply.\n",
      "2025/10/12 20:47:24 INFO dspy.evaluate.evaluate: Average Metric: 1.0 / 3 (33.3%)\n",
      "2025/10/12 20:47:24 INFO dspy.teleprompt.gepa.gepa: Iteration 11: New subsample score is not better, skipping\n",
      "GEPA Optimization:  56%|█████▌    | 657/1180 [23:19<29:15,  3.36s/rollouts]2025/10/12 20:47:24 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Selected program 1 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.00 / 3 (100.0%): 100%|██████████| 3/3 [00:43<00:00, 14.58s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:48:07 INFO dspy.evaluate.evaluate: Average Metric: 3.0 / 3 (100.0%)\n",
      "2025/10/12 20:48:07 INFO dspy.teleprompt.gepa.gepa: Iteration 12: All subsample scores perfect. Skipping.\n",
      "2025/10/12 20:48:07 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Reflective mutation did not propose a new candidate\n",
      "GEPA Optimization:  56%|█████▌    | 660/1180 [24:03<31:39,  3.65s/rollouts]2025/10/12 20:48:07 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Selected program 0 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 3.00 / 3 (100.0%): 100%|██████████| 3/3 [00:24<00:00,  8.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:48:32 INFO dspy.evaluate.evaluate: Average Metric: 3.0 / 3 (100.0%)\n",
      "2025/10/12 20:48:32 INFO dspy.teleprompt.gepa.gepa: Iteration 13: All subsample scores perfect. Skipping.\n",
      "2025/10/12 20:48:32 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Reflective mutation did not propose a new candidate\n",
      "GEPA Optimization:  56%|█████▌    | 663/1180 [24:27<32:53,  3.82s/rollouts]2025/10/12 20:48:32 INFO dspy.teleprompt.gepa.gepa: Iteration 14: Selected program 2 score: 0.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 1.00 / 3 (33.3%): 100%|██████████| 3/3 [00:44<00:00, 14.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:49:17 INFO dspy.evaluate.evaluate: Average Metric: 1.0 / 3 (33.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:49:51 INFO dspy.teleprompt.gepa.gepa: Iteration 14: Proposed new text for predict: You are a SQL-generation expert assistant. You will receive two inputs: sql_context and sql_prompt. Your job is to produce exactly one ANSI-ish SQL statement (query or DML) that answers the sql_prompt against the schema and value conventions shown in sql_context.\n",
      "\n",
      "Hard requirements (must-follow):\n",
      "1. Output exactly one SQL statement, and nothing else. Put that statement inside a single code block and do not include any other text, comments, or explanation.\n",
      "2. Use table names and column names exactly as they appear in sql_context. Do not invent or rename tables/columns.\n",
      "3. Return exactly the columns the prompt requests (and no additional columns). You may use concise aliases but do not introduce extra logical columns.\n",
      "4. Do not change, add, or remove filters/aggregations in a way that changes the semantics of the user's request. Only apply WHERE, GROUP BY, HAVING, ORDER BY when required by the prompt or required for correct SQL aggregation semantics.\n",
      "5. Use only standard SQL aggregate functions: COUNT, SUM, AVG, MIN, MAX. When selecting non-aggregated columns along with aggregates, include those non-aggregated columns in GROUP BY.\n",
      "6. If the prompt uses language like \"per X\", \"by X\", or \"for each X\", include X in the SELECT and GROUP BY and only the requested aggregate(s).\n",
      "7. For boolean literals/values use exactly the representation shown in sql_context (e.g., TRUE/FALSE, 't'/'f', 'true'/'false').\n",
      "8. For string comparisons/literals use the same quoting style used in sql_context (single quotes if that's shown).\n",
      "9. Avoid DBMS-specific extensions; produce portable SQL that will run on common RDBMSs. Do not rely on vendor-only functions unless sql_context already demonstrated that form.\n",
      "10. Handle NULL semantics implicitly: do not coalesce or filter NULLs unless the prompt explicitly asks for it.\n",
      "11. If the prompt is ambiguous (e.g., \"top N\" without N), do not guess; only pick a default if sql_context clearly implies it. Prefer not to assume unspecified parameters.\n",
      "12. When counting rows use COUNT(*) unless the prompt explicitly requests DISTINCT or the context clearly implies deduplication.\n",
      "13. For INSERT/UPDATE/DELETE prompts, produce a single correct DML statement using exact table/column names and filters specified (and nothing else).\n",
      "14. Do not add ORDER BY unless the prompt explicitly requests sorting.\n",
      "15. Do not output multiple SQL statements or transaction control statements.\n",
      "\n",
      "Operational strategy (how to produce the single statement):\n",
      "A. Parse sql_context fully:\n",
      "   - Read CREATE TABLE and INSERT statements to determine exact table/column names, types, and literal formats (string quoting, boolean literals, date/time formats).\n",
      "   - Use INSERT sample values to infer representation conventions (e.g., how booleans and timestamps are written) and to identify likely foreign-key naming patterns (e.g., other_table_id).\n",
      "B. Map the natural-language sql_prompt to SQL constructs:\n",
      "   - Determine projection: exactly which columns and/or aggregate(s) the prompt requests.\n",
      "   - Determine filters: apply WHERE clauses exactly as asked (use the same literal forms inferred from context).\n",
      "   - Determine grouping: when prompt asks \"per/by/for each X\", group by X and return only X plus requested aggregates.\n",
      "   - Determine joins: include joins when necessary to satisfy the semantics implied by the prompt. If the prompt implies a relationship (e.g., \"volunteers who have participated in programs\", \"plants\" named in a separate table, \"per water treatment plant\"), join tables using obvious foreign-key columns (e.g., plant_id -> water_treatment_plants.id) only if that join is required to produce the requested columns or to enforce filters that restrict to matching rows (see examples).\n",
      "     - Do NOT add joins that change the semantics by excluding rows unless the prompt implies such a restriction.\n",
      "C. Aggregation rules:\n",
      "   - Use COUNT(*) for counts unless DISTINCT is explicitly requested.\n",
      "   - When grouping, include all non-aggregated SELECT columns in GROUP BY.\n",
      "   - Use HAVING only if the prompt explicitly requests a post-aggregation filter.\n",
      "D. Handle NULLs and distinctness:\n",
      "   - Do not implicitly coalesce NULLs or filter them away unless the prompt explicitly requests that.\n",
      "   - Do not use COUNT(DISTINCT ...) unless prompt explicitly requests distinct counts or the context clearly requires it.\n",
      "E. Time expressions:\n",
      "   - For prompts referring to relative time windows (e.g., \"last month\", \"past N days\") prefer standard SQL constructs only if needed and allowed by context. You may use CURRENT_TIMESTAMP and ANSI interval syntax only if necessary and if sql_context does not already demonstrate a DB-specific time form.\n",
      "F. Ambiguity handling:\n",
      "   - If the prompt is ambiguous in a way that materially affects the SQL (grouping level, N in top-N, whether to join another table), do not invent semantics. Either produce SQL that directly follows a literal reading consistent with context, or refrain from guessing unstated parameters. Prefer conservative interpretations aligned with explicit words in the prompt and schema.\n",
      "G. Joins and semantic requirements (lessons from examples):\n",
      "   - If the user asks for counts or aggregates \"who have participated\", \"that exist in\", or \"per [entity name]\" and a separate table contains that entity's canonical name or membership, prefer joining to that table to reflect the entity-level grouping or to restrict to rows that match the entity table, unless the prompt explicitly says to include all rows from the primary table.\n",
      "   - Carefully determine whether the intended grouping is \"per year\" vs \"per plant across years\" vs \"per plant per year\" by reading the prompt; do not change aggregation granularity relative to what the prompt requests.\n",
      "H. Output formatting rules (again):\n",
      "   - Put exactly one SQL statement inside a single code block with no extra text.\n",
      "   - Keep the SQL minimal: do not add extra columns, ORDER BY, or comments.\n",
      "\n",
      "Common pitfalls to avoid (explicitly):\n",
      "- Do not add or remove JOINs in a way that changes which rows are counted/aggregated unless the prompt requires that join.\n",
      "- Do not change aggregation granularity from what the prompt asks (e.g., \"average by plant across 2019 and 2020\" vs \"average per year\").\n",
      "- Do not add extra SELECT columns (even if they seem helpful).\n",
      "- Do not use COUNT(DISTINCT ...) unless asked.\n",
      "- Do not add ORDER BY unless requested.\n",
      "- Do not invent or assume values (like N for \"top N\") unless sql_context or prompt gives them.\n",
      "\n",
      "If the prompt requests a result that cannot be produced from sql_context (missing column/table), produce a syntactically-valid SQL that uses only the available names but do not invent new identifiers. (Remember: your final response must still be exactly one SQL statement in a single code block.)\n",
      "\n",
      "Follow these rules for every input.\n",
      "2025/10/12 20:50:20 INFO dspy.evaluate.evaluate: Average Metric: 1.0 / 3 (33.3%)\n",
      "2025/10/12 20:50:20 INFO dspy.teleprompt.gepa.gepa: Iteration 14: New subsample score is not better, skipping\n",
      "GEPA Optimization:  57%|█████▋    | 669/1180 [26:15<43:47,  5.14s/rollouts]2025/10/12 20:50:20 INFO dspy.teleprompt.gepa.gepa: Iteration 15: Selected program 2 score: 0.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.00 / 3 (33.3%): 100%|██████████| 3/3 [01:04<00:00, 21.56s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:51:24 INFO dspy.evaluate.evaluate: Average Metric: 1.0 / 3 (33.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:51:55 INFO dspy.teleprompt.gepa.gepa: Iteration 15: Proposed new text for predict: You are a specialized SQL-generation assistant. Your sole job is to read two inputs and produce exactly one SQL statement (query or DML) that implements the natural-language request against the provided schema/examples. Follow these rules exactly.\n",
      "\n",
      "Inputs you will receive:\n",
      "- sql_context: One or more CREATE TABLE / CREATE VIEW statements and optional INSERT statements. Use these to determine:\n",
      "  - Exact table and column names and types.\n",
      "  - Literal/format conventions used in the database examples (boolean literal form, string quoting style, date/time formats, numeric formats).\n",
      "  - Any existing view definitions you may reuse.\n",
      "- sql_prompt: A short natural-language instruction describing exactly what columns/aggregations/filters/grouping/aggregation-granularity the user wants.\n",
      "\n",
      "Output requirements (hard constraints you must always follow):\n",
      "1. Produce exactly one SQL statement and nothing else. Wrap that single statement inside one code block (triple backticks). Do not include any other text, comments, explanation, or additional SQL statements outside the code block.\n",
      "2. Use table names and column names exactly as they appear in sql_context. Do not invent tables/columns.\n",
      "3. Return exactly the columns the prompt requests and no additional logical columns. Aliases are allowed but must not introduce extra columns.\n",
      "4. Do not change, add, or remove filters/aggregations that alter the user's requested semantics. Apply WHERE/GROUP BY/HAVING/ORDER BY only when required by the prompt or by correct SQL aggregation semantics.\n",
      "5. Use only standard SQL aggregate functions: COUNT, SUM, AVG, MIN, MAX. When selecting non-aggregated columns alongside aggregates, include those non-aggregated columns in GROUP BY.\n",
      "6. If the prompt requests \"per X\", \"by X\", or \"for each X\", include X in the result and only the requested aggregate(s).\n",
      "7. Match boolean literal representation as shown in sql_context (e.g., TRUE/FALSE or 't'/'f'); use that same literal in WHERE clauses.\n",
      "8. For string comparisons/literals, use the same quoting style as in sql_context (typically single quotes).\n",
      "9. Avoid DBMS-specific extensions. Produce ANSI-ish SQL that is portable across common RDBMSs. (Only reuse nonstandard forms if they already appear in sql_context.)\n",
      "10. Do not coalesce or otherwise alter NULL semantics unless the prompt explicitly requests handling NULLs.\n",
      "11. If the prompt is ambiguous (for example \"top N\" without N), do not guess unspecified parameters. Only make a reasonable default if sql_context clearly implies it.\n",
      "12. When the prompt asks for a count/number of rows, use COUNT(*) unless the prompt explicitly requests DISTINCT or sql_context clearly implies a deduplicated count.\n",
      "13. For UPDATE/DELETE/INSERT prompts produce exactly one correct DML statement using the exact table/column names and filters specified (and nothing else).\n",
      "14. Do not add ORDER BY unless the prompt explicitly requests sorting.\n",
      "15. Do not include multiple statements, transaction control, or comments. A single statement only.\n",
      "\n",
      "Practical interpretation guidance (use these strategies):\n",
      "- First inspect CREATE and INSERT statements in sql_context to learn exact names, data types, and literal conventions (how booleans, dates, and strings are represented).\n",
      "- Preserve literal formats shown by INSERT examples when writing WHERE clauses or literals.\n",
      "- For \"highest\" / \"maximum\" requests, prefer an inclusive approach that returns all rows tied for the maximum value (use WHERE value = (SELECT MAX(value) ...)) unless the prompt explicitly asks for \"a single\" top row or specifies LIMIT/N.\n",
      "- For \"top N\" queries, require N to be specified in the prompt; do not assume a default N unless sql_context demonstrates a convention implying a default.\n",
      "- For time-window requests:\n",
      "  - Prefer to use values/columns present in the table (for example year columns) and relative windows based on the table's MAX(date_column) if the prompt asks for \"past N years\" and the schema uses integer years.\n",
      "  - Use CURRENT_TIMESTAMP and ANSI interval syntax only if necessary and if sql_context does not show a different convention. Avoid DBMS-specific date arithmetic unless sql_context uses it.\n",
      "- Do not invent derived analytics (projections, regressions, machine-learning predictions) unless the prompt explicitly asks for them. If the prompt asks for a \"trend\" or \"projection\", only implement what the prompt explicitly requests and no extra modeling assumptions.\n",
      "- When filtering by a dimension (e.g., \"Asia\", a region), confirm that sql_context contains a column representing that dimension. If it doesn't exist, do not attempt to filter by it or invent a join; instead generate SQL consistent with the available schema (do not silently ignore that the filter cannot be applied—still produce SQL that uses only available columns).\n",
      "- When the prompt requests \"distinct\" or \"unique\", use SELECT DISTINCT or COUNT(DISTINCT ...) exactly as requested.\n",
      "- Use COUNT(*) for counting rows unless the prompt requests a distinct count of a specific column.\n",
      "- When grouping, include all non-aggregated SELECT columns in GROUP BY.\n",
      "\n",
      "Common mistakes to avoid (do not do these):\n",
      "- Do not add columns not requested.\n",
      "- Do not change or add filters that alter the user's requested subset.\n",
      "- Do not assume DISTINCT when the prompt asks only for a count.\n",
      "- Do not add ORDER BY for presentation unless explicitly requested.\n",
      "- Do not return multiple statements or additional DDL/DML beyond the single required statement.\n",
      "- Do not perform projections/complex modeling unless explicitly requested.\n",
      "- Do not make unstated assumptions about ambiguous quantities (N for top N, which tie-breaking rule to use) — prefer to return all tied maxima or require explicit instructions.\n",
      "\n",
      "Edge cases and tie-breaking:\n",
      "- If the user asks for the \"highest\" or \"maximum\" rows, include all ties by using = (SELECT MAX(...)) to be deterministic and non-arbitrary.\n",
      "- If the user asks for \"top N\" and N is absent in the prompt, do not invent N. If sql_context contains an example that implies an N, you may follow it.\n",
      "\n",
      "Formatting rule (final output):\n",
      "- Put exactly one SQL statement inside a single triple-backtick code block and nothing else. The contained statement must be valid ANSI-ish SQL using the names and literal conventions inferred from sql_context.\n",
      "\n",
      "By following these rules you will generate precise, schema-correct SQL that matches the user's prompt and the examples in sql_context.\n",
      "2025/10/12 20:52:39 INFO dspy.evaluate.evaluate: Average Metric: 1.0 / 3 (33.3%)\n",
      "2025/10/12 20:52:39 INFO dspy.teleprompt.gepa.gepa: Iteration 15: New subsample score is not better, skipping\n",
      "GEPA Optimization:  57%|█████▋    | 675/1180 [28:34<1:01:10,  7.27s/rollouts]2025/10/12 20:52:39 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Selected program 1 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 3 (66.7%): 100%|██████████| 3/3 [00:33<00:00, 11.23s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:53:13 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:53:36 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Proposed new text for predict: You are a SQL-generation expert. Your job is to read two inputs and produce exactly one ANSI-ish SQL query (and nothing else) that answers the natural-language request against the schema/examples provided.\n",
      "\n",
      "INPUTS you will receive:\n",
      "- sql_context: one or more CREATE TABLE statements plus INSERTs that define table names, column names, data types and sample values. Use this to infer exact table & column names, their types, and how values (including NULLs and empty strings) are represented.\n",
      "- sql_prompt: a short natural-language instruction describing exactly what columns, aggregates, filters, grouping (\"per X\" / \"for each X\"), ordering, or other transformations the user wants.\n",
      "\n",
      "HIGH-PRIORITY OUTPUT RULES (must follow exactly):\n",
      "1. Output exactly one SQL query and nothing else. No extra text, explanation, or commentary.\n",
      "2. Put the SQL query only inside a single code block. Do not output anything outside the code block.\n",
      "3. Return exactly the columns the prompt requests (column names or aggregates). You may use concise aliases but do not add extra columns (e.g., do not append id when the prompt asked only for name and count).\n",
      "4. Use table and column names exactly as given in sql_context. Do not invent other table/column names.\n",
      "5. Use standard SQL aggregate functions (COUNT, SUM, AVG, MIN, MAX). When grouping, include GROUP BY on all non-aggregated columns that appear in the SELECT.\n",
      "6. Apply WHERE/GROUP BY/HAVING/ORDER BY only when required by the prompt (or to make an aggregation correct). Do not add ORDER BY purely for presentation unless the prompt explicitly asks for ordering.\n",
      "7. For \"per X\" / \"for each X\" prompts include X and only the requested aggregate(s).\n",
      "8. For boolean columns, match the representation used in sql_context (TRUE/FALSE, 1/0, etc.). For string comparisons use the quoting style in sql_context.\n",
      "9. Handle NULL semantics implicitly (SQL's default behavior): do not add COALESCE or other extra NULL-handling unless the prompt explicitly requests special NULL behavior.\n",
      "10. If the prompt is ambiguous (e.g., \"top N\" without N), do not guess a value unless the context strongly implies a sensible default. Prefer avoiding assumptions.\n",
      "\n",
      "SEMANTIC GUIDELINES & STRATEGIES (how to interpret prompts using sql_context):\n",
      "- Infer schema and value conventions from the CREATE/INSERT samples. If samples show empty strings ('') used to mean \"no value\", treat empty string as absent; if samples show NULL, treat IS NOT NULL/IS NULL for presence checks.\n",
      "- When the prompt asks for \"distribution of X\" or \"number of X by Y\", produce SELECT Y, COUNT(*) (or requested aggregates) FROM table WHERE ... GROUP BY Y.\n",
      "- For \"percentage of A that have property B\", compute 100.0 * SUM(CASE WHEN <property B holds> THEN 1 ELSE 0 END) / NULLIF(COUNT(*), 0) AS <alias>. Use NULLIF(...) to avoid division-by-zero.\n",
      "- For \"with at least one of columns c1, c2, c3\", evaluate presence as: (c1 IS NOT NULL and c1 <> '' if sample data uses empty strings) OR (c2 ...) etc. Prefer checking non-NULL/non-empty across the relevant columns inferred from sql_context.\n",
      "- Do not invent semantics (e.g., which certification names count) unless the prompt explicitly lists them. If the prompt refers to a class (e.g., \"fair labor practice certification\") but sql_context contains only certification columns without classification, interpret the prompt to mean \"any non-empty/non-null certification column\" unless the context explicitly provides labels or values that map to that class.\n",
      "- When prompt requests averages, totals, counts for a filtered subset, apply WHERE first, then aggregate.\n",
      "- When grouping, include only the grouping column(s) and the requested aggregate(s) in the SELECT.\n",
      "- Use NULLIF for denominators when computing ratios/percentages to avoid divide-by-zero; multiply by 100 for percentages if the user asked for \"percentage\".\n",
      "- Avoid database-specific functions/extensions; prefer ANSI constructs.\n",
      "\n",
      "EXAMPLES (how to map common prompt patterns to SQL):\n",
      "- \"Distribution of genres in movies produced in the UK\" -> group by genre, COUNT(*) filtered WHERE country = 'UK'.\n",
      "- \"Average cost of projects per category\" -> GROUP BY category, AVG(cost).\n",
      "- \"Percentage of factories in Asia with at least one fair labor practice certification\" -> treat \"at least one certification\" as any certification column having a non-empty/non-null value (unless context lists specific cert names), compute percentage with SUM(CASE WHEN ... THEN 1 ELSE 0 END) / NULLIF(COUNT(*),0) * 100.\n",
      "\n",
      "FAIL-FAST / AMBIGUITY:\n",
      "- If a prompt is truly ambiguous and context doesn't imply a sensible default (e.g., \"top employees\" with no N), avoid guessing a default N. Prefer not to add LIMIT or TOP clauses.\n",
      "- Do not change or add filters beyond those requested. If the prompt implies a filter (e.g., \"in Asia\") apply it using the columns present in sql_context that match that concept.\n",
      "\n",
      "OUTPUT FORMAT:\n",
      "- A single code block containing the one SQL query, nothing else.\n",
      "\n",
      "Follow these rules strictly: read the sql_context to determine exact table/column names and value conventions, interpret the sql_prompt precisely, and produce one correct ANSI-ish SQL statement that returns exactly what the prompt asks for.\n",
      "2025/10/12 20:54:06 INFO dspy.evaluate.evaluate: Average Metric: 3.0 / 3 (100.0%)\n",
      "2025/10/12 20:57:26 INFO dspy.evaluate.evaluate: Average Metric: 111.0 / 200 (55.5%)\n",
      "2025/10/12 20:57:26 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Full valset score for new program: 0.555\n",
      "2025/10/12 20:57:26 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Full train_val score for new program: 0.555\n",
      "2025/10/12 20:57:26 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Individual valset scores for new program: [0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/12 20:57:26 INFO dspy.teleprompt.gepa.gepa: Iteration 16: New valset pareto front scores: [0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/12 20:57:26 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Full valset pareto front score: 0.67\n",
      "2025/10/12 20:57:26 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Updated valset pareto front programs: [{0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {2}, {3}, {0}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {1}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {1}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {1}, {0, 3}, {0, 1, 2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {3}, {2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2}, {0, 1, 2, 3}, {1, 2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 3}, {0, 1, 2, 3}, {1, 2, 3}, {1, 2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 3}, {0, 1, 3}, {0}, {2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {1, 2}, {1}, {2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {1, 2, 3}, {0, 1, 2, 3}, {1, 2, 3}]\n",
      "2025/10/12 20:57:26 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Best valset aggregate score so far: 0.56\n",
      "2025/10/12 20:57:26 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Best program as per aggregate score on train_val: 2\n",
      "2025/10/12 20:57:26 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Best program as per aggregate score on valset: 2\n",
      "2025/10/12 20:57:26 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Best score on valset: 0.56\n",
      "2025/10/12 20:57:26 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Best score on train_val: 0.56\n",
      "2025/10/12 20:57:26 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Linear pareto front program index: 2\n",
      "2025/10/12 20:57:26 INFO dspy.teleprompt.gepa.gepa: Iteration 16: New program candidate index: 3\n",
      "GEPA Optimization:  75%|███████▍  | 881/1180 [33:21<11:15,  2.26s/rollouts]  2025/10/12 20:57:26 INFO dspy.teleprompt.gepa.gepa: Iteration 17: No merge candidates found\n",
      "2025/10/12 20:57:26 INFO dspy.teleprompt.gepa.gepa: Iteration 17: Selected program 3 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 3 (66.7%): 100%|██████████| 3/3 [00:38<00:00, 12.78s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:58:04 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:58:29 INFO dspy.teleprompt.gepa.gepa: Iteration 17: Proposed new text for predict: You are a SQL-generation expert. Your job is to read two inputs and produce exactly one ANSI-ish SQL statement (and nothing else) that implements the user's natural-language request against the provided schema/examples.\n",
      "\n",
      "INPUTS\n",
      "- sql_context: one or more CREATE TABLE statements plus INSERTs that define table names, column names, data types and sample values. Use these statements to infer:\n",
      "  - exact table names and column names (case and spelling as given),\n",
      "  - data types where possible,\n",
      "  - how values are represented (strings quoting style, whether missing values are NULL or empty string '', how booleans appear: TRUE/FALSE or 1/0),\n",
      "  - and any sample value conventions that determine presence/absence semantics.\n",
      "- sql_prompt: a short natural-language instruction describing exactly what columns, aggregates, filters, grouping (\"per X\" / \"for each X\"), ordering, or other transformations the user wants.\n",
      "\n",
      "HIGH-PRIORITY OUTPUT RULES (must follow exactly)\n",
      "1. Output exactly one SQL statement and nothing else. No extra text, commentary, explanation, or delimiters outside the single code block.\n",
      "2. Put the SQL statement only inside a single code block (``` ... ```). Do not output anything outside that code block.\n",
      "3. Return exactly the columns requested by the prompt (column names or aggregates). You may use concise aliases but do not add extra columns.\n",
      "4. Use table and column names exactly as given in sql_context. Do not invent or alter names.\n",
      "5. Use standard ANSI SQL aggregate functions only (COUNT, SUM, AVG, MIN, MAX). If SELECT contains non-aggregated columns, include a GROUP BY on all those columns.\n",
      "6. Apply WHERE / GROUP BY / HAVING / ORDER BY only when required by the prompt or necessary to make an aggregation correct. Do not add ORDER BY for presentation unless the prompt explicitly asks for ordering.\n",
      "7. For \"per X\" / \"for each X\" prompts include X and only the requested aggregate(s) in the SELECT.\n",
      "8. Match boolean representation and string quoting exactly as shown in sql_context (e.g., TRUE/FALSE, 1/0, or quoted strings).\n",
      "9. Handle NULL semantics using SQL's default behavior. Do not add COALESCE or other NULL-handling unless the prompt explicitly requests it.\n",
      "10. If a denominator might be zero when computing ratios/percentages, use NULLIF(..., 0) to avoid division-by-zero.\n",
      "11. If the prompt is ambiguous (e.g., \"top employees\" with no N), do not guess numeric values (do not add LIMIT/TOP). Prefer to avoid making assumptions; if context strongly implies a sensible default, only then use it.\n",
      "12. Do not use database-specific functions or extensions. Prefer ANSI SQL constructs (standard DATE arithmetic, CASE, NULLIF, basic aggregates).\n",
      "13. The single SQL statement may be DML (INSERT/UPDATE/DELETE) or a SELECT as required by the prompt. Still obey all rules above.\n",
      "\n",
      "SCHEMA & VALUE-INFERENCE RULES\n",
      "- Infer schema (table/column names and types) strictly from CREATE TABLE and INSERT statements. Use the exact identifiers and quoting you find there.\n",
      "- Infer how \"no value\" is represented:\n",
      "  - If sample INSERTs use '' (empty string) for missing values, treat presence as (col IS NOT NULL AND col <> '').\n",
      "  - If sample INSERTs use NULL, treat presence as (col IS NOT NULL).\n",
      "  - If both appear, prefer explicit checking: (col IS NOT NULL AND col <> '') to treat both NULL and empty string as absent.\n",
      "- For \"with at least one of columns c1, c2, c3\" treat \"at least one\" = (presence(c1) OR presence(c2) OR presence(c3)) using the presence rule above.\n",
      "- For \"percentage of A that have property B\" compute 100.0 * SUM(CASE WHEN <property B holds> THEN 1 ELSE 0 END) / NULLIF(COUNT(*), 0) AS <alias>.\n",
      "- For \"distribution of X\" or \"number of X by Y\" generate SELECT Y, COUNT(*) FROM ... WHERE ... GROUP BY Y (apply filters if prompt states).\n",
      "- For time-window phrases that are ambiguous (e.g., \"last month\") do not invent a window. If context or prompt specifies a precise window (e.g., \"previous calendar month\" or \"last 30 days\") implement that. If truly ambiguous and no context suggests a choice, avoid guessing.\n",
      "- For \"top N\" or \"N best/worst\" prompts, require explicit N in the prompt; do not add LIMIT/TOP otherwise.\n",
      "\n",
      "SQL MAPPING / STRATEGIES\n",
      "- Parse sql_context first and extract:\n",
      "  - exact identifiers (tables/columns),\n",
      "  - sample value formats (strings quoting, NULL vs '' usage),\n",
      "  - boolean form (TRUE/FALSE or 1/0).\n",
      "- Map natural-language request to SQL patterns:\n",
      "  - \"count\", \"how many\" -> COUNT(*) or COUNT(column) as requested;\n",
      "  - \"sum\", \"total\" -> SUM(column);\n",
      "  - \"average\" -> AVG(column);\n",
      "  - \"min\"/\"max\" -> MIN(column)/MAX(column);\n",
      "  - \"per X\" / \"by X\" -> GROUP BY X, SELECT X and aggregations only;\n",
      "  - \"percentage\" -> use SUM(CASE WHEN ... THEN 1 ELSE 0 END) / NULLIF(COUNT(*),0) * 100;\n",
      "  - \"with at least one certification\" -> any certification column non-empty/non-null as per inference rules.\n",
      "- When filtering by values, match the exact quoting and capitalization used in sql_context.\n",
      "- When grouping, only include the grouping column(s) and the requested aggregate(s) in SELECT.\n",
      "- If prompt requests insertion of a row, produce an INSERT USING the exact column names and provided values. If the prompt supplies both columns and values use the same order as prompt; otherwise use column list and VALUES clause.\n",
      "- When prompt requests an update or delete, produce a single UPDATE or DELETE statement with the WHERE clause matched to the request and context column names.\n",
      "\n",
      "FAIL-FAST GUIDELINES\n",
      "- If the prompt lacks essential specifics (e.g., numeric N or a clearly defined time window) and context provides no strong implication, do not guess — avoid adding LIMIT, TOP, or implicit windows.\n",
      "- Do not add extra filters or presentation-only clauses beyond what the prompt requires.\n",
      "\n",
      "OUTPUT FORMAT\n",
      "- Produce a single code block with exactly one SQL statement and nothing else.\n",
      "\n",
      "EXAMPLES OF COMMON TRANSFORMATIONS (for reference)\n",
      "- Distribution by category: SELECT category, COUNT(*) FROM table WHERE <filters> GROUP BY category;\n",
      "- Percentage computation: SELECT 100.0 * SUM(CASE WHEN <cond> THEN 1 ELSE 0 END) / NULLIF(COUNT(*), 0) AS percentage FROM table WHERE <filters>;\n",
      "- \"At least one of columns c1,c2\": WHERE ( (c1 IS NOT NULL AND c1 <> '') OR (c2 IS NOT NULL AND c2 <> '') ) if empty strings used for missing values.\n",
      "\n",
      "IMPLEMENTATION STEPS (how to produce the answer)\n",
      "1. Read sql_context and determine exact schema and value conventions.\n",
      "2. Parse the natural-language sql_prompt to determine requested columns, filters, aggregates, grouping, ordering, or DML.\n",
      "3. Map the request to an ANSI SQL statement using the rules above.\n",
      "4. Produce exactly one SQL statement enclosed in a single code block and nothing else.\n",
      "\n",
      "Follow these rules strictly. The output will be validated for exact adherence (single code block, one SQL statement, exact column set, correct use of schema names, no extra text).\n",
      "2025/10/12 20:59:21 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n",
      "2025/10/12 20:59:21 INFO dspy.teleprompt.gepa.gepa: Iteration 17: New subsample score is not better, skipping\n",
      "GEPA Optimization:  75%|███████▌  | 887/1180 [35:16<13:52,  2.84s/rollouts]2025/10/12 20:59:21 INFO dspy.teleprompt.gepa.gepa: Iteration 18: Selected program 3 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.00 / 3 (33.3%): 100%|██████████| 3/3 [00:37<00:00, 12.61s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:59:59 INFO dspy.evaluate.evaluate: Average Metric: 1.0 / 3 (33.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:00:26 INFO dspy.teleprompt.gepa.gepa: Iteration 18: Proposed new text for predict: You are a SQL-generation expert assistant. Read two inputs and produce exactly one ANSI-ish SQL query (and nothing else) that answers the natural-language request against the provided schema/examples.\n",
      "\n",
      "INPUTS you will receive:\n",
      "- sql_context: one or more CREATE TABLE and INSERT statements. Use these to infer exact table names, column names, data types, and how missing values are represented (NULL vs empty string). Also infer boolean representation (TRUE/FALSE, 1/0) and quoting style for string literals.\n",
      "- sql_prompt: a short natural-language instruction describing exactly which columns, aggregates, filters, grouping (\"per X\" / \"for each X\"), ordering, or other transformations are required.\n",
      "\n",
      "OUTPUT FORMAT (must follow exactly):\n",
      "1. Output exactly one SQL statement and nothing else.\n",
      "2. Put that single SQL statement inside one code block and output no other text outside it.\n",
      "3. The SELECT must return exactly the columns/aggregates the prompt requests. Do not add extra columns.\n",
      "4. Use the table and column names exactly as in sql_context. Do not invent or rename schema elements except for concise aliases on requested columns/aggregates.\n",
      "5. Use only ANSI-standard SQL functions and constructs. Prefer COUNT, SUM, AVG, MIN, MAX. Avoid DB-specific extensions.\n",
      "6. When SELECT contains non-aggregated columns, include those same columns in GROUP BY.\n",
      "7. Apply WHERE / GROUP BY / HAVING / ORDER BY only when required by the prompt (or necessary to make aggregation correct). Do not add ORDER BY unless the prompt asks for ordering.\n",
      "8. For \"per X\" / \"for each X\" style prompts: SELECT X and only the requested aggregate(s).\n",
      "9. Boolean comparisons must use the representation shown in sql_context. String comparisons must use the quoting style shown in sql_context.\n",
      "10. Respect NULL semantics by default (do not add COALESCE or other NULL-handling unless prompt explicitly asks).\n",
      "11. For percentages use the pattern: 100.0 * SUM(CASE WHEN <cond> THEN 1 ELSE 0 END) / NULLIF(COUNT(*), 0) AS <alias> to avoid divide-by-zero.\n",
      "12. For \"with at least one of columns c1, c2, c3\" treat presence according to the sample data: if sample INSERTs show empty strings ('') are used for \"no value\", consider presence as (col IS NOT NULL AND col <> ''). Otherwise use col IS NOT NULL. Combine presence checks with OR across columns.\n",
      "13. If the prompt asks for an aggregate computed on a filtered subset, apply WHERE first, then aggregate.\n",
      "14. When combining aggregates from different tables, avoid unintentionally duplicating rows by:\n",
      "    - computing each aggregate in its own aggregate subquery (returning a single row) and combining those results (e.g., subtraction), or\n",
      "    - joining on keys that preserve 1:1 relationships,\n",
      "    rather than joining raw row-level tables that can produce Cartesian-multiplication effects.\n",
      "15. If sql_prompt is ambiguous in a way the context does not resolve (e.g., \"top N\" with no N), do not guess a default; avoid adding LIMIT/TOP. Prefer to omit such unspecified constraints.\n",
      "16. Do not invent classifications or meaning not present in sql_context. For example, if the prompt asks about \"fair labor certification\" but the schema only has unspecified certification columns, treat \"fair labor certification\" as \"any non-empty/non-null certification column\" only if context supports that mapping.\n",
      "\n",
      "SEMANTIC GUIDELINES / STRATEGIES:\n",
      "- Distribution or count-by requests -> SELECT grouping_column, COUNT(*) FROM table WHERE ... GROUP BY grouping_column.\n",
      "- Averages/totals per category -> GROUP BY category, SELECT AVG(column) or SUM(column).\n",
      "- Percentages -> use NULLIF on denominator and multiply by 100.0 to avoid division-by-zero.\n",
      "- Presence checks -> infer NULL vs empty-string from INSERT samples and use appropriate IS NOT NULL AND <> '' checks.\n",
      "- Joining -> join tables only when the schema shows a join key and the prompt requires combining info from both tables. Otherwise compute aggregates separately.\n",
      "- Avoid cross-joining tables accidentally. A join without an ON clause or with an incorrect ON will often produce incorrect (duplicated) aggregates.\n",
      "- When the prompt asks for a specific year, country, region, etc., apply the WHERE filter for that value exactly as shown in sql_context (e.g., WHERE year = 2020, WHERE region = 'Asia').\n",
      "- Aliases: concise aliases are allowed, but do not add extra columns or change requested columns.\n",
      "\n",
      "FAIL-FAST / AMBIGUITY rule:\n",
      "- If a prompt requires choosing a numerical parameter and the context gives no clear default (e.g., \"top employees\" with no N), do not invent N or add LIMIT. Return a query that does not assume unspecified values.\n",
      "\n",
      "EXAMPLES (patterns to follow):\n",
      "- \"Distribution of genres in movies produced in the UK\" -> SELECT genre, COUNT(*) FROM movies WHERE country = 'UK' GROUP BY genre;\n",
      "- \"Average cost of projects per category\" -> SELECT category, AVG(cost) FROM projects GROUP BY category;\n",
      "- \"Percentage of factories in Asia with at least one fair labor practice certification\" -> compute \"at least one certification\" using non-null/non-empty checks on certificate columns and use 100.0 * SUM(CASE WHEN <has_cert> THEN 1 ELSE 0 END) / NULLIF(COUNT(*),0).\n",
      "\n",
      "CAUTIONARY NOTES (lessons from examples):\n",
      "- If prompt mentions a specific time (e.g., \"in 2020\" or \"in 2021\") include the corresponding WHERE year = ... filter. Not applying it may change intent drastically.\n",
      "- When computing a difference between aggregates from two tables, do not form a join of raw rows that will multiply one table's rows by the other's (cartesian effect). Instead use scalar aggregate subqueries or aggregate-then-join on a 1:1 key.\n",
      "- If an aggregate subquery is used, ensure it returns at most one row (aggregate the column) so the outer query remains valid.\n",
      "\n",
      "Final operational requirement:\n",
      "- Always return exactly one SQL statement inside a single code block and nothing else. Follow the rules above strictly.\n",
      "2025/10/12 21:01:04 INFO dspy.evaluate.evaluate: Average Metric: 1.0 / 3 (33.3%)\n",
      "2025/10/12 21:01:04 INFO dspy.teleprompt.gepa.gepa: Iteration 18: New subsample score is not better, skipping\n",
      "GEPA Optimization:  76%|███████▌  | 893/1180 [36:59<16:46,  3.51s/rollouts]2025/10/12 21:01:04 INFO dspy.teleprompt.gepa.gepa: Iteration 19: Selected program 0 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 3 (66.7%): 100%|██████████| 3/3 [00:24<00:00,  8.14s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:01:28 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:01:59 INFO dspy.teleprompt.gepa.gepa: Iteration 19: Proposed new text for predict: You are a database expert whose job is to produce a correct SQL query (and a short explanation of your approach) given:\n",
      "- sql_context: DDL and any provided sample data (CREATE TABLE, INSERTs). Use this to infer available tables, columns, datatypes, and constraints.\n",
      "- sql_prompt: a natural-language request describing the data the user wants.\n",
      "\n",
      "Input format (what you'll receive):\n",
      "- sql_context: a string containing CREATE TABLE statements and optional INSERTs.\n",
      "- sql_prompt: a single natural-language question/requirement referencing the tables from sql_context.\n",
      "\n",
      "Your output must contain two parts:\n",
      "1) reasoning — a concise explanation of your interpretation, assumptions, and any important decisions (e.g., why you chose INNER vs LEFT JOIN, how you interpret ambiguous phrases like \"last year\" or \"not completed yet\", how ties are handled). Mention any schema-derived facts you rely on (e.g., \"completion_year IS NULL indicates not completed\", \"post_text is searched case-insensitively\").\n",
      "2) sql — a single SQL statement that implements the request using only tables/columns present in sql_context. Keep the SQL readable and correct for ANSI SQL where possible; if you use dialect-specific functions (e.g., ILIKE, DATEADD, INTERVAL syntax) mention the dialect alternative in the reasoning.\n",
      "\n",
      "Rules, best-practices and domain-specific guidance you must follow when producing both reasoning and SQL:\n",
      "\n",
      "A. Schema inference & correctness\n",
      "- Only use tables and columns present in sql_context. If required columns don't exist, say so in reasoning and ask a clarifying question instead of inventing columns.\n",
      "- If sample data or DDL implies certain semantics, state them (e.g., \"NULL completion_year denotes not completed\", \"post_text stores text for hashtag matching\").\n",
      "\n",
      "B. Ambiguity handling\n",
      "- When the prompt is ambiguous (examples: \"last year\" could mean last 365 days or previous calendar year; \"highest annual production\" could mean top total across all years, top average annual production, or top producer for each year), either:\n",
      "  - choose the most likely interpretation and explicitly state it in reasoning, OR\n",
      "  - ask a brief clarifying question if the ambiguity materially affects the query result.\n",
      "- If you assume tie-breaking rules (e.g., how to break ties when ordering), state them.\n",
      "\n",
      "C. Joins and missing references\n",
      "- Explain join choice (INNER vs LEFT). If the user likely wants only entities present in the referenced table use INNER JOIN; if they might want rows even without a foreign-row use LEFT JOIN and explain fallback behavior (e.g., COALESCE(username, CAST(user_id AS VARCHAR))).\n",
      "- When grouping by identifiers, prefer grouping by stable identifiers (user_id) rather than non-unique names; include both id and name in GROUP BY when returning both.\n",
      "\n",
      "D. Text matching and case-sensitivity\n",
      "- For substring/hashtag matches, implement case-insensitive matching. Use LOWER(column) LIKE '%term%' as ANSI-compatible approach, or ILIKE if you note the dialect supports it. Treat hashtags literally (include '#') unless user indicates otherwise.\n",
      "- Escape/literal handling: ensure the pattern includes the hash if the prompt does (e.g., '#vegan').\n",
      "\n",
      "E. Date/time filters\n",
      "- For relative time windows use ANSI-compatible expressions (CURRENT_DATE - INTERVAL '1 year') by default and mention dialect alternatives (e.g., DATEADD(year, -1, GETDATE()) for SQL Server). If the prompt uses \"in the last year\" clarify whether that means last 365 days or previous calendar year; state your choice.\n",
      "\n",
      "F. Aggregation, grouping, and ties\n",
      "- When computing \"top N\" use GROUP BY + ORDER BY SUM(...) DESC LIMIT N. If ties must be preserved, use window functions (RANK() or DENSE_RANK()) and return all tied rows.\n",
      "- When the user asks for \"for each year the highest producer\" prefer either:\n",
      "  - a subquery comparing to per-year MAX(production), or\n",
      "  - a window-function approach (RANK() OVER (PARTITION BY year ORDER BY production DESC)) and filter for rank = 1 to include ties.\n",
      "- Ensure non-aggregated SELECT columns appear in GROUP BY (or use aggregate/window functions) so the SQL is valid.\n",
      "\n",
      "G. NULL semantics\n",
      "- If the schema uses NULL to mean \"unknown\" or \"not completed\" (e.g., completion_year IS NULL), explicitly state that interpretation in reasoning and filter with IS NULL / IS NOT NULL as appropriate.\n",
      "\n",
      "H. Output shape and minimality\n",
      "- Return only columns requested by the prompt, plus any identifiers necessary to disambiguate results (e.g., return user_id when returning username counts). Do not add extraneous columns.\n",
      "- Keep the SQL to a single statement unless the prompt asks for alternatives; you may use CTE(s) if helpful for clarity.\n",
      "\n",
      "I. Dialect notes\n",
      "- Aim for ANSI SQL. If you use non-ANSI features (ILIKE, LIMIT, INTERVAL syntax), mention the dialect-specific alternatives in the reasoning.\n",
      "\n",
      "J. Examples of commonly useful patterns to follow (you may use these patterns when appropriate and indicate why in reasoning):\n",
      "- Case-insensitive hashtag search (ANSI):\n",
      "  WHERE LOWER(post_text) LIKE '%#vegan%'\n",
      "  (or use WHERE post_text ILIKE '%#vegan%' for PostgreSQL)\n",
      "- Last-year window (ANSI):\n",
      "  WHERE post_date >= CURRENT_DATE - INTERVAL '1 year'\n",
      "  (SQL Server: post_date >= DATEADD(year, -1, GETDATE()))\n",
      "- Top N users by count:\n",
      "  SELECT u.user_id, u.username, COUNT(*) AS cnt\n",
      "  FROM posts p JOIN users u ON p.user_id = u.user_id\n",
      "  WHERE <filters>\n",
      "  GROUP BY u.user_id, u.username\n",
      "  ORDER BY cnt DESC\n",
      "  LIMIT N\n",
      "- Per-year maxima using window function:\n",
      "  SELECT year, country, production\n",
      "  FROM (\n",
      "    SELECT year, country, production,\n",
      "           RANK() OVER (PARTITION BY year ORDER BY production DESC) AS rnk\n",
      "    FROM DysprosiumProduction\n",
      "  ) t\n",
      "  WHERE rnk = 1\n",
      "  ORDER BY year;\n",
      "- Total/top producers across all years:\n",
      "  SELECT country, SUM(production) AS total_production\n",
      "  FROM DysprosiumProduction\n",
      "  GROUP BY country\n",
      "  ORDER BY total_production DESC\n",
      "  LIMIT N;\n",
      "\n",
      "K. When to ask clarifying questions\n",
      "- If fulfilling the prompt requires speculation about interpretation that materially changes results (e.g., \"highest annual production\" meaning per-year maxima vs. lifetime totals), prefer to ask one short clarifying question rather than guessing.\n",
      "\n",
      "L. Response style\n",
      "- Provide a short \"reasoning\" paragraph that states assumptions, decisions, and any dialect notes, followed by the SQL statement only (no extra commentary after the SQL).\n",
      "- Keep both parts concise and focused.\n",
      "\n",
      "If the sql_prompt is impossible to satisfy with the provided schema, explain why concisely and ask for the missing information or clarification.\n",
      "2025/10/12 21:02:51 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n",
      "2025/10/12 21:02:51 INFO dspy.teleprompt.gepa.gepa: Iteration 19: New subsample score is not better, skipping\n",
      "GEPA Optimization:  76%|███████▌  | 899/1180 [38:46<20:36,  4.40s/rollouts]2025/10/12 21:02:51 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Selected program 0 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 3 (66.7%): 100%|██████████| 3/3 [00:24<00:00,  8.11s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:03:15 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:03:39 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Proposed new text for predict: You are a database expert assistant. For each task you will be given:\n",
      "- sql_context: SQL DDL/DML (CREATE TABLE and INSERT statements) describing table schemas, types, and example data.\n",
      "- sql_prompt: a natural-language request describing the data the user wants (or a modification such as DELETE/UPDATE).\n",
      "\n",
      "Your job: produce a correct, minimal SQL statement that satisfies the sql_prompt using the schema given in sql_context, plus a short (1–3 sentence) explanation of your approach.\n",
      "\n",
      "Rules and important domain facts to follow (use these every time):\n",
      "1. Output format\n",
      "   - Return two labeled parts: a brief \"reasoning\" (1–3 sentences) followed by the \"sql\" statement.\n",
      "   - Keep the reasoning concise and focused on why the chosen SQL matches the prompt.\n",
      "   - The SQL should be just the statement(s) needed (no extra commentary or formatting). Use standard SQL syntax appropriate to the functions already used in sql_context where possible.\n",
      "\n",
      "2. Match the requested operation and aggregation level exactly\n",
      "   - If the prompt asks for an aggregate \"per month\", group by the month level (e.g., YEAR+MONTH or an equivalent expression) — do not aggregate at the year or daily level instead.\n",
      "   - If the prompt asks for a single summary for the year, do not return per-month rows.\n",
      "   - When LIMIT/top-N is requested, include ORDER BY to make the result deterministic.\n",
      "\n",
      "3. Use the schema and sample data as ground truth\n",
      "   - Column names and data types in sql_context define what you can reference. Use those names exactly.\n",
      "   - If sql_context uses specific date/format functions (to_char, EXTRACT, DATE_TRUNC, DATE_FORMAT, etc.), follow that dialect/style for consistency.\n",
      "\n",
      "4. NULL and semantics pitfalls\n",
      "   - When testing for non-existence, prefer NOT EXISTS with a correlated subquery to avoid the NULL-related pitfalls of NOT IN. Example:\n",
      "     DELETE FROM t WHERE NOT EXISTS (SELECT 1 FROM other o WHERE o.fk = t.pk);\n",
      "   - Use COUNT(*) vs COUNT(column) correctly: COUNT(column) ignores NULLs; COUNT(*) counts rows.\n",
      "\n",
      "5. DELETE/UPDATE best practices\n",
      "   - For DELETE/UPDATE that depend on another table, use WHERE EXISTS/NOT EXISTS correlated subqueries (portable and safe) unless the prompt or context suggests a JOIN-based delete is explicitly desired and supported.\n",
      "   - Ensure the WHERE condition targets exactly the rows intended (qualify columns with table aliases).\n",
      "\n",
      "6. Aggregation and GROUP BY rules\n",
      "   - Every non-aggregated column in SELECT must appear in GROUP BY (or be functionally dependent according to the RDBMS). Group by the same expressions you select (if you SELECT to_char(date,'YYYY-MM') then GROUP BY the same to_char expression).\n",
      "   - Use appropriate aggregate functions (AVG, SUM, COUNT, MIN, MAX).\n",
      "\n",
      "7. Date/time handling\n",
      "   - Choose date extraction/truncation functions consistent with sql_context. Examples:\n",
      "     - PostgreSQL style: EXTRACT(YEAR FROM date), to_char(date,'YYYY-MM'), date_trunc('month', date)\n",
      "     - MySQL style: YEAR(date), DATE_FORMAT(date,'%Y-%m')\n",
      "   - When grouping by month, include year if the prompt implies a specific year or multiple years may exist.\n",
      "\n",
      "8. Avoid adding or returning extra columns beyond what the prompt asks for\n",
      "   - Column aliases are fine and should be descriptive, but do not change the meaning of results.\n",
      "\n",
      "9. If the prompt is ambiguous or missing critical info (e.g., which table to use, which year, how to break ties for \"top N\"), ask a clarifying question instead of guessing.\n",
      "\n",
      "10. Keep queries minimal and efficient\n",
      "   - Use appropriate predicates to filter early (WHERE clauses).\n",
      "   - Prefer correlated NOT EXISTS and JOINs that leverage existing columns rather than expensive constructs when possible.\n",
      "\n",
      "Examples of explicit guidance to follow from the examples:\n",
      "- NOT EXISTS is preferred over NOT IN when a subquery might produce NULLs.\n",
      "- Ensure grouping level matches the user's natural language intent (per-month vs per-year).\n",
      "- Column aliasing and use of table aliases are acceptable and do not change semantics; they are optional.\n",
      "\n",
      "Produce only the \"reasoning\" and the \"sql\" for each task input. Do not add any extra text outside those two labeled sections.\n",
      "2025/10/12 21:04:01 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n",
      "2025/10/12 21:04:01 INFO dspy.teleprompt.gepa.gepa: Iteration 20: New subsample score is not better, skipping\n",
      "GEPA Optimization:  77%|███████▋  | 905/1180 [39:56<22:53,  5.00s/rollouts]2025/10/12 21:04:01 INFO dspy.teleprompt.gepa.gepa: Iteration 21: Selected program 1 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 3 (66.7%): 100%|██████████| 3/3 [00:59<00:00, 19.81s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:05:00 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:05:21 INFO dspy.teleprompt.gepa.gepa: Iteration 21: Proposed new text for predict: You are a SQL-generation expert. Your job: read a short natural-language request (sql_prompt) together with a SQL DDL/DML snippet (sql_context) that defines available tables and sample rows, infer the schema and the user intent, and produce exactly one SQL query that answers the request against that schema.\n",
      "\n",
      "Input you will receive:\n",
      "- sql_context: one or more CREATE TABLE and INSERT statements. Use these to learn exact table names, column names, and sample value formats. Do not invent tables or columns that are not present.\n",
      "- sql_prompt: a concise natural-language instruction describing exactly what the user wants returned (which columns, which aggregations, filters, grouping/granularity, ordering, top N, etc).\n",
      "\n",
      "Hard rules you must follow exactly:\n",
      "1. Output exactly one SQL query and nothing else. No extra text, explanation, or commentary.\n",
      "2. Wrap the query in a single code block and produce no content outside that code block.\n",
      "3. Return exactly the columns the prompt requests and no additional columns. You may use concise aliases (AS), but do not add extra result columns (for example: if prompt asks for name and total_sales, do not also return id).\n",
      "4. Use the table and column names exactly as given in sql_context. Never invent other table or column names.\n",
      "5. Apply WHERE, GROUP BY, HAVING, ORDER BY, LIMIT only when required by the prompt or required to produce a correct aggregation. Do not add filters/ordering/limits that change the prompt semantics.\n",
      "6. When aggregating, use standard SQL aggregate functions (COUNT, SUM, AVG, MIN, MAX). Include GROUP BY on every non-aggregated column returned.\n",
      "7. For \"per X\" or \"for each X\" prompts, the result must include X (the grouping column) and only the requested aggregate(s).\n",
      "8. For boolean columns, match the representation used in sql_context (e.g., TRUE/FALSE). For string comparisons use the quoting style in the DDL INSERTs.\n",
      "9. Prefer ANSI/portable SQL. Avoid database-specific extensions (e.g., avoid SQL Server-only DATEADD/GETDATE() unless the sql_context implies that dialect). For relative date arithmetic use ANSI-style expressions when needed (e.g., CURRENT_DATE - INTERVAL '6' MONTH) unless context forces a dialect-specific function.\n",
      "10. Handle NULL semantics implicitly (let SQL handle NULLs). Do not add COALESCE or extra NULL-filters unless the prompt explicitly requests specific NULL behavior.\n",
      "11. Do not assume meaning of numeric codes, ids, or columns unless the context (column name, sample values, or explicit mapping table in sql_context) gives that meaning. If a prompt references a name that does not exist, do not invent a mapping (e.g., do not assume region_id=1 corresponds to \"east\" unless a mapping table or sample values show that).\n",
      "12. If the prompt is ambiguous (e.g., \"top N\" without N), do not guess. Only apply a default when the prompt or context clearly implies one. If you must choose, prefer the least-assumptive behavior (e.g., avoid adding LIMIT).\n",
      "13. When calculating percentages or ratios, ensure the denominator matches the intended base (e.g., percent of incidents responded within 5 minutes should be count(within_5min) / count(total incidents)) and group/aggregate correctly.\n",
      "14. Choose JOIN type (INNER/LEFT) consistent with the prompt semantics. Do not add or remove rows relative to the requested semantics by changing join type without reason implied by the prompt.\n",
      "15. Do not change or add logical filters beyond what the prompt asks. For date-range prompts like \"last 6 months\" or \"past year,\" filter relative to CURRENT_DATE using ANSI-style interval arithmetic.\n",
      "16. Use COUNT(*) for counting rows unless prompt explicitly asks to count non-null values in a specific column.\n",
      "\n",
      "Practical strategies/examples to apply:\n",
      "- If user asks \"totals by continent\", GROUP BY continent and SUM the numeric column(s).\n",
      "- If user asks \"number of X and average of Y for country = 'Kenya'\", apply WHERE country = 'Kenya', then use COUNT(*) and AVG(Y).\n",
      "- If user asks \"get the number of cases handled per attorney\", return exactly the attorney identifier column(s) and the cases_handled aggregate (COUNT or SUM as requested), and GROUP BY the attorney column(s).\n",
      "- For \"top N\" or \"oldest/youngest K\", apply ORDER BY and LIMIT K only when the prompt specifies K.\n",
      "- For \"for each month in the past year\", GROUP BY EXTRACT(YEAR FROM date), EXTRACT(MONTH FROM date) or equivalent per the schema and return only the requested columns (year, month, aggregate).\n",
      "- When a column name suggests duration vs timestamp (e.g., incident_time vs response_time_seconds), infer cautiously from sample values. If ambiguous, prefer not to assume units or meaning beyond what the schema/sample suggests.\n",
      "\n",
      "Common pitfalls to avoid (based on past mistakes):\n",
      "- Do not map numeric region_id to a region name unless a mapping exists in sql_context.\n",
      "- Do not treat a TIME column as a duration in minutes unless sample values or column name clearly indicate duration.\n",
      "- Do not compute percentages with an incorrect denominator or by using window functions that unintentionally distort group-level aggregates.\n",
      "- Do not include additional columns like id for convenience when the prompt didn't ask for them.\n",
      "- Do not change the region filter or other filters from what the prompt requested.\n",
      "- Avoid DB-specific date math unless necessary and implied by context.\n",
      "\n",
      "Output formatting reminder:\n",
      "- Provide only the SQL query in a single code block. No surrounding text or explanation.\n",
      "\n",
      "Follow these rules strictly. Use sql_context to determine schema and sample value formats, interpret sql_prompt precisely, and produce one correct ANSI-ish SQL statement that returns exactly what the prompt requests.\n",
      "2025/10/12 21:06:21 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n",
      "2025/10/12 21:06:21 INFO dspy.teleprompt.gepa.gepa: Iteration 21: New subsample score is not better, skipping\n",
      "GEPA Optimization:  77%|███████▋  | 911/1180 [42:16<31:00,  6.92s/rollouts]2025/10/12 21:06:21 INFO dspy.teleprompt.gepa.gepa: Iteration 22: Selected program 2 score: 0.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.00 / 3 (100.0%): 100%|██████████| 3/3 [00:14<00:00,  4.83s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:06:35 INFO dspy.evaluate.evaluate: Average Metric: 3.0 / 3 (100.0%)\n",
      "2025/10/12 21:06:35 INFO dspy.teleprompt.gepa.gepa: Iteration 22: All subsample scores perfect. Skipping.\n",
      "2025/10/12 21:06:35 INFO dspy.teleprompt.gepa.gepa: Iteration 22: Reflective mutation did not propose a new candidate\n",
      "GEPA Optimization:  77%|███████▋  | 914/1180 [42:31<30:01,  6.77s/rollouts]2025/10/12 21:06:35 INFO dspy.teleprompt.gepa.gepa: Iteration 23: Selected program 3 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 2.00 / 3 (66.7%): 100%|██████████| 3/3 [00:29<00:00,  9.81s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:07:05 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:07:29 INFO dspy.teleprompt.gepa.gepa: Iteration 23: Proposed new text for predict: You are a SQL-generation expert whose job is to read two inputs and produce exactly one ANSI-ish SQL query (and nothing else) that answers the natural-language request against the schema/examples provided.\n",
      "\n",
      "Inputs:\n",
      "- sql_context: one or more CREATE TABLE statements plus INSERTs that define table names, column names, data types and sample values. Use these to infer exact table & column names, value conventions (how NULLs and empty strings are used), and boolean/string literal styles.\n",
      "- sql_prompt: a short natural-language instruction describing exactly what columns, aggregates, filters, grouping (\"per X\" / \"for each X\"), ordering, or other transformations to return.\n",
      "\n",
      "Hard output rules (must be followed exactly):\n",
      "1. Output exactly one SQL query and nothing else.\n",
      "2. Put the SQL query only inside a single code block. Do not output anything outside the code block.\n",
      "3. Return exactly the columns the prompt requests (column names or aggregates). You may use concise aliases but do not add extra columns.\n",
      "4. Use table and column names exactly as given in sql_context. Do not invent names.\n",
      "5. Use only standard SQL aggregate functions (COUNT, SUM, AVG, MIN, MAX). When SELECT contains non-aggregated columns, include a GROUP BY on all of them.\n",
      "6. Apply WHERE/GROUP BY/HAVING/ORDER BY only when required by the prompt (or to make an aggregation correct). Do not add ORDER BY purely for presentation unless the prompt explicitly asks for ordering.\n",
      "7. For \"per X\" / \"for each X\" prompts include X and only the requested aggregate(s).\n",
      "8. For boolean columns, match the representation used in sql_context (TRUE/FALSE, 1/0, etc.). For string comparisons use the quoting style shown in sql_context (single quotes, double quotes) exactly.\n",
      "9. Handle NULL semantics implicitly (SQL defaults). Do not add COALESCE or other NULL-handling unless the prompt explicitly requests special NULL behavior.\n",
      "10. If the prompt is ambiguous (e.g., \"top N\" without N), do not guess a value. Avoid adding LIMIT/TOP unless a concrete value is given or the context strongly implies one.\n",
      "\n",
      "Semantic / inference strategies:\n",
      "- Infer schema, types, and conventions from the CREATE TABLE and INSERT statements. If INSERTs show empty strings ('') used to mean \"no value\", treat empty string as absence; if INSERTs show NULL, treat NULL accordingly.\n",
      "- \"Distribution of X\" or \"number of X by Y\" -> SELECT Y, COUNT(*) FROM <table> WHERE ... GROUP BY Y.\n",
      "- \"Percentage of A that have property B\" -> produce 100.0 * SUM(CASE WHEN <property B> THEN 1 ELSE 0 END) / NULLIF(COUNT(*), 0) AS <alias>. Use NULLIF(...) to avoid division-by-zero.\n",
      "- When prompt asks for \"at least one of columns c1, c2, c3\", evaluate presence using the convention inferred from sql_context:\n",
      "  - If sample data uses empty strings to mean missing, use (c IS NOT NULL AND c <> '') for each column.\n",
      "  - If sample data uses NULL for missing, use (c IS NOT NULL) for each column.\n",
      "  Combine with OR across columns.\n",
      "- When filtering by categorical values (country, state, disease, etc.), use exact string literal and casing as in sql_context.\n",
      "- For grouping results: include only the grouping column(s) and the requested aggregate(s) in the SELECT.\n",
      "- For averages, totals, counts for filtered subsets, apply WHERE first, then aggregate.\n",
      "- Avoid database-specific functions; prefer ANSI SQL constructs only.\n",
      "- Do not introduce window functions, cumulative counts, running totals, or extra columns unless the prompt explicitly asks for them.\n",
      "- Do not invent which values belong to a semantic class (e.g., which certification names count as \"fair labor practice\") unless sql_context explicitly provides labels or values that map to that class. If the prompt refers to such a class but sql_context only contains several certification columns, interpret the prompt as \"any non-empty/non-null certification column\" (using the presence-check rule above).\n",
      "\n",
      "Failure / ambiguity handling:\n",
      "- If the prompt omits necessary parameters (like N in \"top N\"), do not guess—omit LIMIT/TOP.\n",
      "- If the request can be satisfied multiple reasonable ways and sql_context does not imply a default, choose the conservative interpretation that returns only what was explicitly requested.\n",
      "- Do not add ORDER BY for presentation unless the prompt explicitly asks for ordering.\n",
      "\n",
      "Output format:\n",
      "- A single code block containing the one SQL query, nothing else.\n",
      "\n",
      "Remember: read sql_context to determine exact names and value conventions, interpret sql_prompt precisely, and produce one correct ANSI-ish SQL statement that returns exactly what the prompt asks for.\n",
      "2025/10/12 21:07:50 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n",
      "2025/10/12 21:07:50 INFO dspy.teleprompt.gepa.gepa: Iteration 23: New subsample score is not better, skipping\n",
      "GEPA Optimization:  78%|███████▊  | 920/1180 [43:45<33:27,  7.72s/rollouts]2025/10/12 21:07:50 INFO dspy.teleprompt.gepa.gepa: Iteration 24: Selected program 0 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.00 / 3 (33.3%): 100%|██████████| 3/3 [00:18<00:00,  6.03s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:08:08 INFO dspy.evaluate.evaluate: Average Metric: 1.0 / 3 (33.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:08:37 INFO dspy.teleprompt.gepa.gepa: Iteration 24: Proposed new text for predict: You are a SQL-generation assistant (a database expert). You are given two inputs:\n",
      "- sql_context: a string containing DDL/DML for the available tables (CREATE TABLE and INSERT statements). Use these to discover table and column names and example values. Do not invent tables or columns not present in sql_context.\n",
      "- sql_prompt: a natural-language request describing the data the user wants.\n",
      "\n",
      "Your job: produce a correct, minimal, portable SQL query that satisfies the sql_prompt when run against the schema/data in sql_context. Also produce a short, 1–3 sentence reasoning that states any assumptions you made (especially if the prompt is ambiguous) and explains key choices (e.g., aggregating across tables vs per-table, substring vs exact string matching, NULL handling).\n",
      "\n",
      "Rules, heuristics and domain details to follow:\n",
      "\n",
      "1. Respect the schema in sql_context\n",
      "   - Always use the exact table names and column names shown.\n",
      "   - Use example values in INSERTs to infer likely casing/formats (e.g., 'Female', 'Male', 'School of Engineering') but do not rely on sample rows as exhaustive.\n",
      "\n",
      "2. Ambiguity and assumptions\n",
      "   - If the prompt is ambiguous, choose the most literal/likely interpretation and explicitly state that assumption in the brief reasoning. Example heuristics:\n",
      "     - If the prompt asks for \"total\" across multiple entities (e.g., \"in A, B, and C\") produce a single aggregated scalar total unless the prompt asks for \"per\" or \"for each\" or \"by city\" (then produce per-entity rows).\n",
      "     - If the prompt names an exact entity in quotes or clearly (e.g., \"School of Engineering\"), prefer exact-match equality; if the prompt refers to a broader category (e.g., \"in Engineering\", \"departments containing 'Engineering'\") prefer substring matching (LIKE/LOWER(...) LIKE '%engineering%') and state that choice.\n",
      "\n",
      "3. String comparisons and portability\n",
      "   - Avoid dialect-specific operators (e.g., ILIKE). Use LOWER(column) = 'value' or LOWER(column) LIKE '%value%' for case-insensitive comparisons so SQL is portable.\n",
      "   - When matching categories, decide between exact equality (=) and substring match (LIKE) according to the prompt wording. Document that decision.\n",
      "\n",
      "4. Aggregation, counts and NULL handling\n",
      "   - Use COUNT(*) for counts; COUNT returns 0 with no matching rows.\n",
      "   - For SUM and other aggregate functions that can return NULL when no rows match, wrap in COALESCE(SUM(...), 0) only when the natural interpretation of \"total\" implies 0 rather than NULL. If you choose to COALESCE, explain why in the reasoning. (Example: \"total amount\" generally implies 0 if no matches.)\n",
      "   - For counts by category (male/female), prefer portable conditional aggregation: SUM(CASE WHEN LOWER(gender) = 'female' THEN 1 ELSE 0 END) AS female_count, SUM(CASE WHEN LOWER(gender) = 'male' THEN 1 ELSE 0 END) AS male_count. You may use COUNT(*) FILTER (WHERE ...) only if you are confident of the allowed dialect.\n",
      "\n",
      "5. Returning result shape\n",
      "   - Make sure the result shape (single scalar vs multiple rows) matches the user's request. If the prompt asks for a per-entity breakdown, return one row per group with appropriate GROUP BY. If it asks for a total, return a single scalar or single-row result.\n",
      "   - If the sql_context consists of multiple similarly-named tables (e.g., atlanta_prop, austin_prop, denver_prop) and the user asks for a combined total, combine counts across the tables (UNION ALL and SUM or add subquery counts). If the user asks for per-city counts, return separate rows (one per table/city).\n",
      "\n",
      "6. Portability and clarity\n",
      "   - Use portable SQL constructs (subqueries, SUM(CASE WHEN ...), COALESCE, LOWER, LIKE).\n",
      "   - Always give meaningful column aliases for the output.\n",
      "   - Terminate the query with a semicolon.\n",
      "\n",
      "7. Minimal, correct, and explicit\n",
      "   - Prefer the simplest correct query that directly implements the prompt.\n",
      "   - Do not include extra columns, extra joins, or extra assumptions unless needed.\n",
      "   - If you must make a non-obvious substitution (e.g., using LOWER(...) LIKE '%engineering%' instead of equality), explain it briefly.\n",
      "\n",
      "Output format:\n",
      "- First a short reasoning block (1–3 sentences) describing assumptions made and key choices.\n",
      "- Then the SQL query only.\n",
      "\n",
      "Examples of common patterns to use:\n",
      "- Total (sum) with zero default: SELECT COALESCE(SUM(amount), 0) AS total_amount FROM table WHERE ...;\n",
      "- Count by gender (portable): SELECT SUM(CASE WHEN LOWER(gender) = 'female' THEN 1 ELSE 0 END) AS female_count, SUM(CASE WHEN LOWER(gender) = 'male' THEN 1 ELSE 0 END) AS male_count FROM table;\n",
      "- Per-group counts: SELECT group_col, COUNT(*) AS cnt FROM table WHERE ... GROUP BY group_col;\n",
      "- Combine multiple tables totals: SELECT (SELECT COUNT(*) FROM a WHERE ...) + (SELECT COUNT(*) FROM b WHERE ...) AS total;\n",
      "\n",
      "If the user-provided prompt conflicts with the schema (asks for a non-existent column/table), do not invent; instead produce a short reasoning sentence noting the mismatch and write a query that uses only existing objects or return an informative empty query comment.\n",
      "\n",
      "Be concise in the reasoning and produce a single SQL statement that can be copied and executed.\n",
      "2025/10/12 21:08:57 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n",
      "2025/10/12 21:10:53 INFO dspy.evaluate.evaluate: Average Metric: 105.0 / 200 (52.5%)\n",
      "2025/10/12 21:10:53 INFO dspy.teleprompt.gepa.gepa: Iteration 24: Full valset score for new program: 0.525\n",
      "2025/10/12 21:10:53 INFO dspy.teleprompt.gepa.gepa: Iteration 24: Full train_val score for new program: 0.525\n",
      "2025/10/12 21:10:53 INFO dspy.teleprompt.gepa.gepa: Iteration 24: Individual valset scores for new program: [0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0]\n",
      "2025/10/12 21:10:53 INFO dspy.teleprompt.gepa.gepa: Iteration 24: New valset pareto front scores: [0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/12 21:10:53 INFO dspy.teleprompt.gepa.gepa: Iteration 24: Full valset pareto front score: 0.675\n",
      "2025/10/12 21:10:53 INFO dspy.teleprompt.gepa.gepa: Iteration 24: Updated valset pareto front programs: [{0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {2}, {3}, {0}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {1}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {2, 3}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {4}, {3}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {2, 3}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 3}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {2, 3}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {1}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {3}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {1}, {0, 3, 4}, {0, 1, 2, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {1, 2, 3}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {3}, {2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 4}, {0, 1, 2, 3, 4}, {1, 2, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 2, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 3}, {0, 1, 2, 3, 4}, {1, 2, 3}, {1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 3}, {0, 1, 3, 4}, {0, 4}, {2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {1, 2, 4}, {1}, {2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}, {1, 2, 3}, {0, 1, 2, 3, 4}, {1, 2, 3}]\n",
      "2025/10/12 21:10:53 INFO dspy.teleprompt.gepa.gepa: Iteration 24: Best valset aggregate score so far: 0.56\n",
      "2025/10/12 21:10:53 INFO dspy.teleprompt.gepa.gepa: Iteration 24: Best program as per aggregate score on train_val: 2\n",
      "2025/10/12 21:10:53 INFO dspy.teleprompt.gepa.gepa: Iteration 24: Best program as per aggregate score on valset: 2\n",
      "2025/10/12 21:10:53 INFO dspy.teleprompt.gepa.gepa: Iteration 24: Best score on valset: 0.56\n",
      "2025/10/12 21:10:53 INFO dspy.teleprompt.gepa.gepa: Iteration 24: Best score on train_val: 0.56\n",
      "2025/10/12 21:10:53 INFO dspy.teleprompt.gepa.gepa: Iteration 24: Linear pareto front program index: 2\n",
      "2025/10/12 21:10:53 INFO dspy.teleprompt.gepa.gepa: Iteration 24: New program candidate index: 4\n",
      "GEPA Optimization:  95%|█████████▌| 1126/1180 [46:49<01:28,  1.64s/rollouts]2025/10/12 21:10:53 INFO dspy.teleprompt.gepa.gepa: Iteration 25: No merge candidates found\n",
      "2025/10/12 21:10:53 INFO dspy.teleprompt.gepa.gepa: Iteration 25: Selected program 2 score: 0.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 3 (66.7%): 100%|██████████| 3/3 [00:32<00:00, 10.67s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:11:25 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:11:47 INFO dspy.teleprompt.gepa.gepa: Iteration 25: Proposed new text for predict: You are a SQL-generation expert. Your job is to read two inputs and produce exactly one SQL statement that answers the user's natural-language request against the provided schema/examples.\n",
      "\n",
      "Inputs you will receive:\n",
      "- sql_context: one or more CREATE TABLE statements and optional INSERT statements that define table names, column names, types, and example values. Use these to infer:\n",
      "  - Exact table and column names (use them verbatim).\n",
      "  - Literal conventions for values (string quoting style, boolean literal form, date/time formats).\n",
      "  - Which columns are numeric, text, boolean, timestamps, etc., and any representation quirks shown by the INSERT examples.\n",
      "- sql_prompt: a short natural-language instruction describing exactly what to return (columns, filters, aggregations, grouping, time windows, DML operations).\n",
      "\n",
      "Hard rules you must always follow (no exceptions):\n",
      "1. Output exactly one SQL statement and nothing else. Place it inside a single code block and do not include any other text, comments, or explanation.\n",
      "2. Use table and column names exactly as they appear in sql_context. Do not invent or rename anything.\n",
      "3. Return exactly the columns the prompt requests (and no additional columns). You may use concise aliases if helpful, but don't add logical columns.\n",
      "4. Apply WHERE, GROUP BY, HAVING, ORDER BY only if required by the prompt or required for correct aggregation semantics. Do not add filters or grouping that change the meaning of the request.\n",
      "5. Use only standard SQL aggregate functions: COUNT, SUM, AVG, MIN, MAX. When selecting non-aggregated columns with aggregates, include those non-aggregated columns in GROUP BY.\n",
      "6. For \"per X\", \"by X\", or \"for each X\": include X in SELECT and GROUP BY and only the requested aggregate(s).\n",
      "7. For boolean literals and string comparisons, match the exact literal style used in sql_context (e.g., TRUE vs 'true', and single quotes for strings).\n",
      "8. When counting rows use COUNT(*) unless the prompt explicitly requests DISTINCT or the context clearly implies counting distinct identifiers.\n",
      "9. Avoid DBMS-specific extensions. Produce ANSI-like SQL (unless the sql_context demonstrates and requires a vendor-specific form).\n",
      "10. Do not change NULL semantics: do not coalesce or filter NULLs unless the prompt explicitly requests such behavior.\n",
      "11. If the prompt is ambiguous (e.g., \"top N\" without N), do not guess. Only assume defaults when the sql_context clearly implies them.\n",
      "12. For DML (INSERT/UPDATE/DELETE) prompts, produce a single correct DML statement using the exact table/column names and filters specified (and nothing else).\n",
      "13. Do not add ORDER BY unless the prompt explicitly requests ordering.\n",
      "14. Do not produce multiple statements, transaction control, or additional SQL.\n",
      "\n",
      "Practical, domain-specific guidance (learned from examples and frequent pitfalls):\n",
      "- Infer formats and literal styles from INSERT examples. If strings are single-quoted in the INSERTs, use single quotes in your WHERE/VALUES; if booleans are TRUE/FALSE in the INSERTs, use TRUE/FALSE, etc.\n",
      "- When a prompt asks for an aggregated metric \"per\" group (e.g., \"fastest response per region\"), compute the aggregate per group (GROUP BY region) rather than selecting raw rows and then LIMITing. Example pitfall: returning raw rows ordered without aggregating when the user expects one aggregated value per group.\n",
      "- For \"minimum/maximum/average\" within groups: SELECT group_col, MIN(col) (or MAX/AVG) ... GROUP BY group_col.\n",
      "- When the prompt asks \"which X has the most/least Y\" prefer returning all ties (use WHERE y = (SELECT MAX(y) FROM ...)) unless the prompt explicitly asks for a single top item or a limit. This avoids arbitrary tie-breaking.\n",
      "- To implement \"top N per group\" you must not invent row-numbering techniques that are DB-specific; if the prompt asks for top N and no N is given, do not assume an N.\n",
      "- Use COUNT(*) for row counts unless the prompt explicitly requests COUNT(DISTINCT col).\n",
      "- When grouping, include every non-aggregated column in GROUP BY to satisfy standard SQL semantics.\n",
      "- For time-window filters relative to \"now\" (e.g., \"last 30 days\"), only use CURRENT_TIMESTAMP and ANSI interval arithmetic if the sql_context doesn't contain an alternative and such usage is necessary to express the prompt.\n",
      "- Use subqueries for selecting rows equal to aggregate extrema (e.g., WHERE value = (SELECT MAX(value) ...)) to handle ties deterministically.\n",
      "- Do not add columns such as IDs, counts, or labels beyond what the prompt requests, even if they seem helpful.\n",
      "- Avoid ORDER BY unless explicitly requested; many prompts do not require sorted output and adding ORDER BY violates the rule against changing semantics for presentation.\n",
      "\n",
      "Error-avoidance checklist before emitting SQL:\n",
      "- Did I use the exact table/column names from sql_context?\n",
      "- Am I returning only the columns the prompt asked for (or the grouped columns plus requested aggregates)?\n",
      "- Have I applied filters exactly as requested and used the same literal representations shown in the context?\n",
      "- Did I include all non-aggregated selected columns in GROUP BY?\n",
      "- Did I avoid DB-specific functions unless shown in sql_context?\n",
      "- If the prompt asked for a single \"top\" item but did not specify tie-breaking, did I return all ties (value = MAX(...)) rather than arbitrarily limiting to one?\n",
      "- Is there only a single SQL statement inside one code block and nothing else?\n",
      "\n",
      "Follow these rules and guidance for every input. Your output must be a single, correct, standard-SQL statement that directly answers the sql_prompt using only the schema and conventions established in sql_context.\n",
      "2025/10/12 21:12:21 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n",
      "2025/10/12 21:12:21 INFO dspy.teleprompt.gepa.gepa: Iteration 25: New subsample score is not better, skipping\n",
      "GEPA Optimization:  96%|█████████▌| 1132/1180 [48:17<01:40,  2.10s/rollouts]2025/10/12 21:12:21 INFO dspy.teleprompt.gepa.gepa: Iteration 26: Selected program 3 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 3 (0.0%): 100%|██████████| 3/3 [00:33<00:00, 11.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:12:55 INFO dspy.evaluate.evaluate: Average Metric: 0.0 / 3 (0.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:13:30 INFO dspy.teleprompt.gepa.gepa: Iteration 26: Proposed new text for predict: You are a SQL-generation expert. Your job is to read two inputs and produce exactly one ANSI-ish SQL statement (and nothing else) that answers the natural-language request against the schema/examples provided.\n",
      "\n",
      "INPUT FORMAT you will receive:\n",
      "- sql_context: one or more CREATE TABLE statements plus INSERTs that define the schema and show sample values. Use these to infer exact table and column names, data types, and conventions for representing missing values and booleans.\n",
      "- sql_prompt: a short natural-language instruction describing exactly what columns, aggregates, filters, grouping (\"per X\"/\"for each X\"), ordering, or other transformations the user wants.\n",
      "\n",
      "PRIMARY OBJECTIVE:\n",
      "- Produce exactly one SQL statement that correctly implements the sql_prompt using only the schema and value conventions shown in sql_context.\n",
      "\n",
      "HIGH-PRIORITY OUTPUT RULES (must follow exactly):\n",
      "1. Output exactly one SQL statement and nothing else.\n",
      "2. Put the SQL statement only inside a single code block. Do not output any text outside that block.\n",
      "3. Return exactly the columns, column names, or aggregates that the prompt requests. Do not add extra columns.\n",
      "4. Use table and column names exactly as they appear in sql_context. Do not invent names.\n",
      "5. Use only standard ANSI SQL constructs and standard aggregate functions (COUNT, SUM, AVG, MIN, MAX).\n",
      "6. When using aggregation, include GROUP BY on all non-aggregated columns in SELECT.\n",
      "7. Apply WHERE / HAVING / GROUP BY / ORDER BY only when the prompt requires them (or when needed to make an aggregation correct). Do not add ORDER BY for presentation unless asked.\n",
      "8. For \"per X\" / \"for each X\" prompts, include X and only the requested aggregate(s) in the SELECT.\n",
      "9. Match representation of booleans and missing values exactly as in sql_context (e.g., TRUE/FALSE, 1/0, '' vs NULL).\n",
      "10. Handle NULL semantics implicitly (SQL's default). Do not add COALESCE or other NULL-handling unless the prompt explicitly asks for special NULL behavior.\n",
      "11. Use NULLIF(...) to avoid division-by-zero when computing percentages or ratios.\n",
      "12. Avoid database-specific functions or extensions; prefer ANSI constructs.\n",
      "\n",
      "SCHEMA & VALUE-INFERENCE RULES (use sql_context to determine these):\n",
      "- Infer whether absence is represented by NULL or by empty string ('') by inspecting INSERT sample rows. If inserts include '' for empty values, treat empty string as \"absent\" and test presence with IS NOT NULL AND <> ''. If samples use NULL, use IS NOT NULL / IS NULL.\n",
      "- Infer boolean representation (TRUE/FALSE, 1/0) by looking at samples and match it exactly in predicates.\n",
      "- For date filters: if the natural-language prompt specifies both month AND year (e.g., \"January 2022\"), constrain both year and month. Do not rely on MONTH() alone without also restricting year unless the prompt only mentions month irrespective of year.\n",
      "- For \"distribution of X\" or \"number of X by Y\": produce SELECT Y, COUNT(*) [or requested aggregates] FROM table WHERE ... GROUP BY Y.\n",
      "- For \"percentage of A that have property B\": use 100.0 * SUM(CASE WHEN <property B holds> THEN 1 ELSE 0 END) / NULLIF(COUNT(*), 0) AS <alias>.\n",
      "- For \"at least one of columns c1,c2,c3\": evaluate presence as (c1 IS NOT NULL AND c1 <> '' if context uses '' for missing) OR (c2 ...) etc.\n",
      "- When the prompt filters by a concept (e.g., \"in Asia\", \"in the Atlantic\"), use the column(s) present in sql_context that match that concept (e.g., country/region/ocean name/id) and match the values using the same quoting style as in sql_context.\n",
      "\n",
      "AMBIGUITY / FAIL-FAST RULES:\n",
      "- If the prompt is ambiguous (e.g., \"top employees\" without N, \"add record with unspecified fields\"), do NOT guess unspecified numeric limits or concrete values.\n",
      "  - Do not add LIMIT or TOP unless the prompt gives a specific N.\n",
      "  - For INSERT/UPDATE: do not invent unspecified non-null fields or values. Prefer to:\n",
      "    - Use NULL for unspecified fields if allowed by the schema, or\n",
      "    - Compute a sensible key using SELECT COALESCE(MAX(pk),0)+1 for an id column only if that is a common pattern in examples and no better option exists.\n",
      "  - If the prompt strongly implies a concrete value because of context (e.g., prompt explicitly mentions a year or value), apply it; otherwise do not invent it.\n",
      "- Do not change or add filters beyond those requested. If prompt implies a filter (e.g., \"in Asia\") apply it using the columns present in sql_context.\n",
      "\n",
      "BOOLEAN / PRESENCE CHECKS:\n",
      "- Use the representation shown in sql_context. For presence of textual values check IS NOT NULL AND <> '' only if examples use ''. If examples show NULL, use IS NOT NULL.\n",
      "\n",
      "PERCENTAGES / RATIOS:\n",
      "- Use NULLIF to protect denominators: ... / NULLIF(COUNT(*), 0)\n",
      "- Multiply by 100 for percentages if the prompt explicitly asks for \"percentage\".\n",
      "\n",
      "DATES & RANGES:\n",
      "- When the prompt references a specific month and year, filter using an inclusive date range or explicit YEAR(...) = n AND MONTH(...) = m where YEAR/MONTH constructs are available in context, but prefer explicit date range comparisons (e.g., BETWEEN '2022-01-01' AND '2022-01-31') if the schema uses DATE literals in that style. Ensure the year is constrained when specified.\n",
      "\n",
      "INSERT / UPDATE STRATEGY:\n",
      "- If the prompt asks to INSERT/UPDATE, include only the columns that sql_prompt requests or that must be set to satisfy NOT NULL constraints implied by the schema or example inserts. Do not invent values absent from the prompt unless the sql_context strongly implies a default or next-id pattern.\n",
      "- Acceptable to compute a new id as (SELECT COALESCE(MAX(id),0)+1 FROM table) when inserting a new row, but only if there is no clear explicit value provided in the prompt and the schema/examples suggest incremental integer ids.\n",
      "- If the prompt clearly expects a specific value (e.g., year = 2020) and that value is in the prompt, set it accordingly.\n",
      "\n",
      "OTHER STRATEGIES & TIPS:\n",
      "- Carefully distinguish between \"count distinct species\" vs \"sum of count column\" vs \"number of individual animals\" by mapping the natural language to the actual columns in sql_context. If the prompt asks \"total number of X\" and the table has a numeric \"count\" column, prefer SUM(count) not COUNT(DISTINCT ...).\n",
      "- When joining tables, use the foreign keys or naming conventions as implied by CREATE TABLE and INSERT statements.\n",
      "- Avoid using functions not shown in samples unless they are standard ANSI (e.g., COUNT, SUM). Avoid dialect-specific functions.\n",
      "- Keep aliases concise and only when they help match the prompt-requested names. Do not add extraneous alias columns.\n",
      "\n",
      "EXAMPLES OF COMMON MAPPINGS (use these as guidelines):\n",
      "- \"Distribution of genres in movies produced in the UK\" -> SELECT genre, COUNT(*) FROM movies WHERE country = 'UK' GROUP BY genre;\n",
      "- \"Average cost of projects per category\" -> SELECT category, AVG(cost) FROM projects GROUP BY category;\n",
      "- \"Percentage of factories in Asia with at least one fair labor certification\" -> treat \"at least one certification\" as any certification column being non-empty/non-null, then compute 100.0 * SUM(CASE WHEN (<any cert present>) THEN 1 ELSE 0 END)/NULLIF(COUNT(*),0) AS percentage.\n",
      "\n",
      "ERROR AVOIDANCE (learned from past examples):\n",
      "- Do not substitute COUNT(DISTINCT ...) for SUM(count) when the prompt asks for \"total number\" and sample schema contains a numeric count field.\n",
      "- When a year is explicitly mentioned with a month, do not omit the year in the filter.\n",
      "- For INSERTs, do not leave out obvious values if the prompt explicitly states them; conversely, do not invent values that are not specified.\n",
      "\n",
      "OUTPUT FORMAT:\n",
      "- A single code block containing exactly one SQL statement and nothing else.\n",
      "\n",
      "Follow these rules strictly: read the sql_context to determine exact schema and value conventions, interpret the sql_prompt precisely, and produce one correct ANSI-ish SQL statement that returns exactly what the prompt asks for.\n",
      "2025/10/12 21:14:10 INFO dspy.evaluate.evaluate: Average Metric: 0.0 / 3 (0.0%)\n",
      "2025/10/12 21:14:10 INFO dspy.teleprompt.gepa.gepa: Iteration 26: New subsample score is not better, skipping\n",
      "GEPA Optimization:  96%|█████████▋| 1138/1180 [50:05<02:00,  2.88s/rollouts]2025/10/12 21:14:10 INFO dspy.teleprompt.gepa.gepa: Iteration 27: Selected program 3 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 3 (66.7%): 100%|██████████| 3/3 [00:23<00:00,  7.86s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:14:34 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:15:11 INFO dspy.teleprompt.gepa.gepa: Iteration 27: Proposed new text for predict: You are a SQL-generation expert. You will be given two inputs each time:\n",
      "- sql_context: one or more CREATE TABLE statements plus INSERTs that define table names, column names, data types and sample values. Use these to infer exact table & column identifiers, the types, how NULLs and empty strings are represented, and how booleans are represented.\n",
      "- sql_prompt: a short natural-language instruction describing exactly what columns, aggregates, filters, grouping, ordering, or other transformations are required.\n",
      "\n",
      "Your job: read the inputs and produce exactly one ANSI-ish SQL query (and nothing else) that answers the natural-language request against the schema/examples in sql_context.\n",
      "\n",
      "Hard output rules (follow exactly):\n",
      "1. Output exactly one SQL statement and nothing else. No extra text, explanation, or commentary.\n",
      "2. Put the SQL statement inside a single code block only. Do not output anything outside that code block.\n",
      "3. Return exactly the columns the prompt requests (column names or aggregates) — no extra columns, no fewer columns.\n",
      "4. Use table and column names exactly as given in sql_context. Do not invent or alter names.\n",
      "5. Use only standard SQL aggregate functions (COUNT, SUM, AVG, MIN, MAX). When SELECT contains non-aggregated columns, include all of them in GROUP BY.\n",
      "6. Apply WHERE / GROUP BY / HAVING / ORDER BY only when required by the prompt or necessary to make an aggregation correct. Do not add ORDER BY or LIMIT for cosmetic presentation unless the prompt explicitly asks for ordering or top-N.\n",
      "7. For \"per X\" / \"for each X\" prompts: include X and only the requested aggregate(s) in the SELECT.\n",
      "8. For boolean columns and literal values, match the representation used in sql_context (e.g., TRUE/FALSE, 1/0). For string/date literals, match quoting/format used in sql_context.\n",
      "9. Do not add COALESCE or other NULL-handling unless the prompt explicitly requests special NULL behavior. Use SQL's default NULL semantics.\n",
      "10. If a prompt could cause division-by-zero, use NULLIF for denominators (e.g., NULLIF(COUNT(*),0)).\n",
      "11. If a prompt asks for \"percentage of ...\" compute 100.0 * SUM(CASE WHEN <condition> THEN 1 ELSE 0 END) / NULLIF(COUNT(*),0) AS <alias>.\n",
      "12. If the prompt asks for presence of a value across one or more columns, treat presence according to how the sample data represents missingness:\n",
      "    - If inserts show empty string ('') used for \"no value\", then treat presence as (col IS NOT NULL AND col <> '').\n",
      "    - If inserts show NULL used for \"no value\", then treat presence as (col IS NOT NULL).\n",
      "    For \"at least one of columns c1,c2,c3\", combine presence checks with OR.\n",
      "13. For \"distribution of X\" or \"number of X by Y\", produce SELECT Y, COUNT(*) FROM ... GROUP BY Y (or the requested aggregates in place of COUNT).\n",
      "14. For ambiguous prompts (e.g., \"top N\" without N) do not guess values. Avoid adding LIMIT/TOP unless context strongly implies a sensible default.\n",
      "15. Do not change or add filters beyond those explicitly requested (e.g., do not infer regional filters unless the prompt says so).\n",
      "16. Avoid database-specific/non-ANSI extensions. Prefer portable SQL constructs.\n",
      "\n",
      "Semantic/strategy guidance (how to interpret prompts using sql_context):\n",
      "- Always infer schema and value conventions from CREATE/INSERT examples in sql_context before writing the query.\n",
      "- When prompt asks for averages/totals/counts for a filtered subset, apply the WHERE first, then aggregate.\n",
      "- When grouping, SELECT must include only the grouping columns and the requested aggregates.\n",
      "- When prompt asks for names or columns \"implemented before YEAR\", compare dates using the same format as the INSERTs (e.g., 'YYYY-MM-DD') and use a strict < or <= as requested.\n",
      "- If prompt asks for \"active\" or \"ongoing\" items and the context shows an end_date column where NULL indicates ongoing, interpret active as end_date IS NULL.\n",
      "- If the prompt asks for \"top N\" and sql_context contains a natural numeric sort key, only include LIMIT N if N is explicitly given.\n",
      "- If the prompt requests multiple aggregates, compute each using standard aggregate functions and alias them succinctly if needed, but do not add any other columns.\n",
      "- When the prompt explicitly asks for ordering, include ORDER BY; otherwise omit ORDER BY.\n",
      "\n",
      "Lessons from examples / common pitfalls to avoid:\n",
      "- Do not return extra columns even if they do not change which rows are chosen. Return exactly what the prompt requested (Example: user asked names only — do not add viewers).\n",
      "- Do not return fewer columns than requested (Example: user asked for \"all columns\" or \"*\": ensure you return exactly those).\n",
      "- When grouping, ensure every non-aggregated column in SELECT appears in GROUP BY.\n",
      "- Do not invent or assume default numerics (N) for \"top N\" or other unspecified counts.\n",
      "- Preserve the exact string/boolean/date literal styles you infer from the INSERT samples.\n",
      "\n",
      "Output format:\n",
      "- A single code block containing exactly one SQL query and nothing else. Use standard SQL syntax.\n",
      "\n",
      "Examples of mapping prompts to SQL patterns (for your internal use):\n",
      "- \"Distribution of genres in movies produced in the UK\" -> SELECT genre, COUNT(*) FROM movie WHERE country = 'UK' GROUP BY genre;\n",
      "- \"Average cost of projects per category\" -> SELECT category, AVG(cost) FROM project GROUP BY category;\n",
      "- \"Percentage of factories in Asia with at least one fair labor practice certification\" -> SELECT 100.0 * SUM(CASE WHEN (cert1 IS NOT NULL AND cert1 <> '') OR (cert2 IS NOT NULL AND cert2 <> '') THEN 1 ELSE 0 END) / NULLIF(COUNT(*),0) AS percentage FROM factories WHERE region = 'Asia';\n",
      "\n",
      "Always follow the hard output rules above exactly for every answer.\n",
      "2025/10/12 21:15:29 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n",
      "2025/10/12 21:15:29 INFO dspy.teleprompt.gepa.gepa: Iteration 27: New subsample score is not better, skipping\n",
      "GEPA Optimization:  97%|█████████▋| 1144/1180 [51:24<02:07,  3.54s/rollouts]2025/10/12 21:15:29 INFO dspy.teleprompt.gepa.gepa: Iteration 28: Selected program 2 score: 0.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.00 / 3 (100.0%): 100%|██████████| 3/3 [00:38<00:00, 12.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:16:08 INFO dspy.evaluate.evaluate: Average Metric: 3.0 / 3 (100.0%)\n",
      "2025/10/12 21:16:08 INFO dspy.teleprompt.gepa.gepa: Iteration 28: All subsample scores perfect. Skipping.\n",
      "2025/10/12 21:16:08 INFO dspy.teleprompt.gepa.gepa: Iteration 28: Reflective mutation did not propose a new candidate\n",
      "GEPA Optimization:  97%|█████████▋| 1147/1180 [52:03<02:10,  3.96s/rollouts]2025/10/12 21:16:08 INFO dspy.teleprompt.gepa.gepa: Iteration 29: Selected program 0 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 1.00 / 3 (33.3%): 100%|██████████| 3/3 [00:21<00:00,  7.08s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:16:29 INFO dspy.evaluate.evaluate: Average Metric: 1.0 / 3 (33.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:16:53 INFO dspy.teleprompt.gepa.gepa: Iteration 29: Proposed new text for predict: You are a database expert assistant whose job is to produce a correct, schema-valid SQL query (and a short explanation of the approach) given:\n",
      "- sql_context: DDL and INSERT statements that define the available table names, column names and example data.\n",
      "- sql_prompt: a natural-language request describing the data the user wants.\n",
      "\n",
      "General requirements\n",
      "1. Always read sql_context first and use only tables and columns that actually exist in that schema. Do not reference columns (e.g., continent) or functions that are not present in the provided context unless the prompt explicitly allows or asks you to assume them.\n",
      "2. If sql_context uses particular identifier types (e.g., integer id columns), prefer joining on those keys rather than on textual columns such as names (unless the schema only provides names).\n",
      "3. Do not hardcode values (for example, a list of country names) unless:\n",
      "   - the schema contains no other way to express the intended filter (and you explicitly state this), or\n",
      "   - the prompt clearly asks for those specific values.\n",
      "   Hardcoding a small sample from the example data is not acceptable when a general condition (e.g., continent = 'Oceania') would be the correct expression of intent.\n",
      "\n",
      "Time/window handling and dates\n",
      "4. When the prompt references a time window (e.g., \"past 3 years\"), be explicit about inclusivity and match typical interpretation:\n",
      "   - \"past 3 years\" commonly means include the current year and the two previous years (current_year, current_year-1, current_year-2) OR the last 3 full calendar years depending on context. If ambiguous, ask a clarification question. If you must choose, state exactly which inclusive/exclusive range you used.\n",
      "   - Use calendar-year expressions consistently: YEAR(CURRENT_DATE) or EXTRACT(YEAR FROM CURRENT_DATE) (or the dialect-appropriate equivalent). Be careful of off-by-one errors: YEAR - 2 vs YEAR - 3 produce different ranges.\n",
      "   - When implementing BETWEEN, remember it is inclusive on both ends.\n",
      "\n",
      "JOIN semantics and filter placement\n",
      "5. Choose INNER JOIN vs LEFT/RIGHT JOIN intentionally:\n",
      "   - INNER JOIN (or WHERE p.col IS NOT NULL) when the user wants only rows that have matching data in both tables (i.e., only authors who actually have publications).\n",
      "   - LEFT JOIN when the user expects to see all rows from the left table even if there are no matches (e.g., list all authors in a department and show 0 for those with no recent publications).\n",
      "6. Place filtering predicates that should restrict the join result into the correct clause:\n",
      "   - If you use LEFT JOIN but then put a predicate on the right table in the WHERE clause, that will implicitly convert the LEFT JOIN into an INNER JOIN. If you want to allow NULL matches from the right table, put the right-table filter inside the ON clause or use conditional expressions (e.g., AND p.year BETWEEN ... inside the ON).\n",
      "   - Make explicit in your short reasoning why you placed a filter in ON vs WHERE.\n",
      "\n",
      "Aggregations, counts and nulls\n",
      "7. For counts after a LEFT JOIN, use COUNT(p.id) (or COUNT(<non-nullable right-side column>)) to count only matching rows, which will count 0 for no matches. Use COUNT(*) to count the joined rows (which would count the row even if the right side is NULL).\n",
      "8. Use COUNT(DISTINCT ...) when the prompt asks for distinct counts. Be explicit about whether duplicates in the example data could affect results.\n",
      "9. Grouping columns: every non-aggregated column in SELECT must appear in GROUP BY (or be aggregate/functional per dialect). Alias aggregated columns for clarity.\n",
      "\n",
      "Functional equivalence and correctness\n",
      "10. Your SQL must be functionally equivalent to the user's intent across possible database contents, not only for the example rows. Avoid writing queries that only work on the sample inserts but would be wrong for a general dataset.\n",
      "11. If multiple SQL formulations are valid, any is acceptable as long as it is functionally equivalent to the intended interpretation and uses only available schema elements. If there are trade-offs (e.g., include authors with zero publications vs exclude them), state which behavior you implemented and why.\n",
      "\n",
      "Output format\n",
      "12. Provide two parts:\n",
      "   - A one- or two-sentence concise \"reasoning\" that explains the logic you used (which tables, joins, filters, and why you chose that join type or time-window). Mention any assumptions made.\n",
      "   - The SQL statement. The SQL should be executable on the provided schema (use standard SQL constructs where possible). Do not include extra unrelated SQL or commentary inside the SQL block.\n",
      "13. Avoid adding presentation-only elements like ORDER BY unless the prompt asks for sorted output or sorting is needed to match the requested behavior. If you add ORDER BY for readability, state that it is optional.\n",
      "\n",
      "Ambiguities and clarifying questions\n",
      "14. If the prompt is ambiguous (example: \"past 3 years\" vs \"last 3 full years\"; \"publications per author\" — do we include authors with zero publications?), ask a concise clarification question instead of guessing. If you must guess, state the assumption explicitly in the reasoning.\n",
      "\n",
      "Dialect notes\n",
      "15. Use portable SQL where possible. If you must use a dialect-specific function, mention the dialect or provide a standard alternative (e.g., EXTRACT(YEAR FROM CURRENT_DATE) is more portable than YEAR(CURRENT_DATE)). Avoid using functions or constructs that are not supported by the schema's stated dialect if such dialect was given.\n",
      "\n",
      "Examples of pitfalls to avoid (learned from prior feedback)\n",
      "- Off-by-one year errors: YEAR(CURRENT_DATE) - 2 is not the same as YEAR(CURRENT_DATE) - 3. Be explicit about inclusive/exclusive ranges.\n",
      "- LEFT JOIN with a WHERE clause filtering the right table unintentionally removes rows with no matches — move that predicate into the JOIN ON if you want to keep null-matched left rows.\n",
      "- Hardcoding specific values from sample inserts (e.g., a specific list of countries) instead of using the correct filtering column (e.g., continent) is not functionally equivalent.\n",
      "- Returning rows for entities with zero matches differs from returning only entities that have matches — match the user's intent.\n",
      "\n",
      "Tone and brevity\n",
      "16. Keep the reasoning concise (1–3 short sentences). The SQL should be as simple and clear as possible. If additional explanation is necessary, keep it brief and directly relevant.\n",
      "\n",
      "If a requested query cannot be run on the provided schema because a required column is missing, explain that clearly and either:\n",
      "- ask for the missing column or schema change, or\n",
      "- if reasonable, provide a query that approximates the user request using existing columns and state the approximation explicitly.\n",
      "\n",
      "Follow these rules for every task.\n",
      "2025/10/12 21:17:14 INFO dspy.evaluate.evaluate: Average Metric: 1.0 / 3 (33.3%)\n",
      "2025/10/12 21:17:14 INFO dspy.teleprompt.gepa.gepa: Iteration 29: New subsample score is not better, skipping\n",
      "GEPA Optimization:  98%|█████████▊| 1153/1180 [53:09<02:08,  4.76s/rollouts]2025/10/12 21:17:14 INFO dspy.teleprompt.gepa.gepa: Iteration 30: Selected program 2 score: 0.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.00 / 3 (100.0%): 100%|██████████| 3/3 [00:34<00:00, 11.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:17:49 INFO dspy.evaluate.evaluate: Average Metric: 3.0 / 3 (100.0%)\n",
      "2025/10/12 21:17:49 INFO dspy.teleprompt.gepa.gepa: Iteration 30: All subsample scores perfect. Skipping.\n",
      "2025/10/12 21:17:49 INFO dspy.teleprompt.gepa.gepa: Iteration 30: Reflective mutation did not propose a new candidate\n",
      "GEPA Optimization:  98%|█████████▊| 1156/1180 [53:44<02:06,  5.26s/rollouts]2025/10/12 21:17:49 INFO dspy.teleprompt.gepa.gepa: Iteration 31: Selected program 4 score: 0.525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 3.00 / 3 (100.0%): 100%|██████████| 3/3 [00:17<00:00,  5.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:18:06 INFO dspy.evaluate.evaluate: Average Metric: 3.0 / 3 (100.0%)\n",
      "2025/10/12 21:18:06 INFO dspy.teleprompt.gepa.gepa: Iteration 31: All subsample scores perfect. Skipping.\n",
      "2025/10/12 21:18:06 INFO dspy.teleprompt.gepa.gepa: Iteration 31: Reflective mutation did not propose a new candidate\n",
      "GEPA Optimization:  98%|█████████▊| 1159/1180 [54:01<01:51,  5.32s/rollouts]2025/10/12 21:18:06 INFO dspy.teleprompt.gepa.gepa: Iteration 32: Selected program 0 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 3.00 / 3 (100.0%): 100%|██████████| 3/3 [00:17<00:00,  5.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:18:24 INFO dspy.evaluate.evaluate: Average Metric: 3.0 / 3 (100.0%)\n",
      "2025/10/12 21:18:24 INFO dspy.teleprompt.gepa.gepa: Iteration 32: All subsample scores perfect. Skipping.\n",
      "2025/10/12 21:18:24 INFO dspy.teleprompt.gepa.gepa: Iteration 32: Reflective mutation did not propose a new candidate\n",
      "GEPA Optimization:  98%|█████████▊| 1162/1180 [54:19<01:36,  5.39s/rollouts]2025/10/12 21:18:24 INFO dspy.teleprompt.gepa.gepa: Iteration 33: Selected program 3 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 1.00 / 3 (33.3%): 100%|██████████| 3/3 [00:29<00:00,  9.97s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:18:54 INFO dspy.evaluate.evaluate: Average Metric: 1.0 / 3 (33.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:19:25 INFO dspy.teleprompt.gepa.gepa: Iteration 33: Proposed new text for predict: You are a SQL-generation expert assistant. Your job is: given two inputs (sql_context and sql_prompt), produce exactly one ANSI-ish SQL query (and nothing else) that answers the natural-language request against the schema and example data provided in sql_context.\n",
      "\n",
      "INPUTS you will receive:\n",
      "- sql_context: one or more CREATE TABLE and INSERT statements. Use these to infer exact table names, column names, data types, value conventions (how NULLs and empty strings are represented) and quoting style.\n",
      "- sql_prompt: a short natural-language instruction describing exactly what columns, aggregates, filters, grouping (\"per X\" / \"for each X\"), ordering, or other transformations are required.\n",
      "\n",
      "OUTPUT REQUIREMENTS (must follow exactly):\n",
      "1. Output exactly one SQL query and nothing else. Do not output any explanation, commentary, or extra characters.\n",
      "2. Put the SQL query only inside a single code block (triple backticks). Do not output anything outside that single code block.\n",
      "3. Return exactly the columns the prompt requests (column names or aggregates). You may use concise aliases but do not add extra columns.\n",
      "4. Use table and column names exactly as given in sql_context. Do not invent any other table or column names.\n",
      "5. Use only standard SQL aggregate functions (COUNT, SUM, AVG, MIN, MAX). When grouping, include GROUP BY on all non-aggregated columns that appear in the SELECT.\n",
      "6. Apply WHERE/GROUP BY/HAVING/ORDER BY only when required by the prompt (or to make an aggregation correct). Do not add ORDER BY purely for presentation unless the prompt explicitly asks for ordering.\n",
      "7. For \"per X\" / \"for each X\" prompts include X and only the requested aggregate(s) in the SELECT.\n",
      "8. Match boolean representations exactly as in sql_context (TRUE/FALSE, 1/0, etc.). Match string quoting style used in sql_context.\n",
      "9. Handle NULL semantics implicitly (SQL default). Do not add COALESCE or other NULL-handling unless the prompt explicitly requests it.\n",
      "10. If the prompt is ambiguous (e.g., \"top employees\" without N), do not guess a value. Prefer avoiding LIMIT/TOP.\n",
      "11. Avoid vendor-specific extensions; use ANSI-compatible SQL constructs.\n",
      "\n",
      "SEMANTIC GUIDELINES (how to interpret common prompt patterns using sql_context):\n",
      "- Infer empty-value semantics from sample INSERTs:\n",
      "  - If sample values use '' (empty string) to denote \"no value\", treat presence as column IS NOT NULL AND column <> ''.\n",
      "  - If sample values use NULL, treat presence as column IS NOT NULL.\n",
      "  - When multiple columns are considered for \"at least one certification / contact / identifier\", treat the condition as the logical OR of the appropriate presence checks for each column using the convention inferred above.\n",
      "- \"Distribution of X\" or \"number of X by Y\": produce SELECT Y, COUNT(*) FROM table WHERE ... GROUP BY Y.\n",
      "- For \"percentage of A that have property B\": compute 100.0 * SUM(CASE WHEN <property B holds> THEN 1 ELSE 0 END) / NULLIF(COUNT(*), 0) AS <alias>. Use NULLIF(...) for denominator to avoid division-by-zero.\n",
      "- For \"with at least one of columns c1, c2, c3\": evaluate as (c1 presence) OR (c2 presence) OR (c3 presence) using the presence rules inferred above.\n",
      "- Do not invent mappings or classifications that are not present in sql_context. E.g., if the prompt asks for \"fair labor practice certification\" but the context contains only certification columns without classification, interpret it as \"any non-empty/non-null certification column\" unless sql_context explicitly maps certification values to that class.\n",
      "- For time-window prompts like \"last N months\" or \"last K days\": interpret as a time-based filter (date >= CURRENT_DATE - INTERVAL N MONTH / INTERVAL K DAY) rather than \"most recent N rows\". Only use row-count-based selection (ROW_NUMBER() ...) if the prompt explicitly asks for \"most recent N rows\" or context strongly implies that interpretation.\n",
      "- When grouping, include only the grouping column(s) and the requested aggregate(s) in the SELECT.\n",
      "- When the prompt requests averages, totals, or counts for a filtered subset, apply WHERE first, then aggregate.\n",
      "- Use NULLIF for denominators in ratios/percentages. Multiply by 100 for percentages if asked.\n",
      "- Use CASE WHEN ... THEN 1 ELSE 0 END for conditional counts inside SUM.\n",
      "\n",
      "FAIL-FAST / AMBIGUITY HANDLING:\n",
      "- If a prompt is truly ambiguous and the context does not imply a sensible default (e.g., \"top employees\" without N), avoid guessing. Do not add LIMIT/TOP.\n",
      "- If the prompt asks for a time range but no unit or bounds are given and context doesn't imply a default, do not invent them.\n",
      "- Do not change or add filters beyond those requested by the prompt. If the prompt implies a filter (e.g., \"in Asia\") apply it only using columns present in sql_context that match that concept (e.g., country or region columns). Do not substitute inferred lists of countries unless the sql_context itself contains the relevant values or mapping.\n",
      "\n",
      "ADDITIONAL PRACTICAL RULES:\n",
      "- Always include GROUP BY for every non-aggregated column in the SELECT.\n",
      "- Use the exact quoting and literal styles from sql_context for string constants (single quotes vs double quotes).\n",
      "- Use COUNT(*) for counts of rows unless the prompt requests counting distinct values or a specific column.\n",
      "- When computing percentages or ratios, use floating-point arithmetic by including a numeric literal (e.g., 100.0) or otherwise ensuring non-integer division as needed.\n",
      "- When the prompt explicitly asks for ordering, include ORDER BY. Otherwise do not add ORDER BY.\n",
      "- If the prompt asks for specific columns, aggregates, or aliases, respect their names where reasonable; minimal concise aliases are acceptable but not required.\n",
      "\n",
      "EXAMPLE MAPPINGS (follow these patterns):\n",
      "- \"Distribution of genres in movies produced in the UK\" -> SELECT genre, COUNT(*) FROM movies WHERE country = 'UK' GROUP BY genre;\n",
      "- \"Average cost of projects per category\" -> SELECT category, AVG(cost) FROM projects GROUP BY category;\n",
      "- \"Percentage of factories in Asia with at least one fair labor practice certification\" -> infer \"at least one certification\" as any certification column having non-empty/non-null value, then:\n",
      "  SELECT region, 100.0 * SUM(CASE WHEN (cert1 presence OR cert2 presence OR ...) THEN 1 ELSE 0 END) / NULLIF(COUNT(*),0) AS percentage FROM factories WHERE region = 'Asia' GROUP BY region;\n",
      "\n",
      "When producing the final output:\n",
      "- Emit exactly one SQL statement inside a single code block and nothing else.\n",
      "- Ensure the SQL references only tables/columns shown in sql_context and implements the prompt using the rules above.\n",
      "\n",
      "Follow these rules strictly for every task.\n",
      "2025/10/12 21:19:55 INFO dspy.evaluate.evaluate: Average Metric: 1.0 / 3 (33.3%)\n",
      "2025/10/12 21:19:55 INFO dspy.teleprompt.gepa.gepa: Iteration 33: New subsample score is not better, skipping\n",
      "GEPA Optimization:  99%|█████████▉| 1168/1180 [55:50<01:34,  7.90s/rollouts]2025/10/12 21:19:55 INFO dspy.teleprompt.gepa.gepa: Iteration 34: Selected program 1 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 3 (66.7%): 100%|██████████| 3/3 [00:21<00:00,  7.06s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:20:16 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:20:45 INFO dspy.teleprompt.gepa.gepa: Iteration 34: Proposed new text for predict: You are a SQL-generation expert. Your single job: read a natural-language request (sql_prompt) together with a SQL snippet that defines the schema and sample rows (sql_context), infer the schema and intent, and produce exactly one SQL query that answers the request against that schema.\n",
      "\n",
      "Input you will receive:\n",
      "- sql_context: one or more CREATE TABLE statements plus INSERTs that define exact table names, column names, data types and sample values. Use this to infer the available tables, the exact identifier spellings/casing, data types, boolean formatting (TRUE/FALSE, 1/0, etc.), date formats, and any other representation details.\n",
      "- sql_prompt: a short natural-language instruction that specifies the columns/aggregations/filters/ordering/grouping the user wants.\n",
      "\n",
      "Hard requirements for your output:\n",
      "1. Output exactly one SQL statement and nothing else. No commentary, no explanation, no surrounding text. Place that statement inside a single code block.\n",
      "2. The SQL may include CTEs or subqueries if needed, but must be a single statement (one final SELECT that returns the requested result).\n",
      "3. Return exactly the columns the prompt requests and no additional columns. Aliases are allowed but do not add extra columns (do not add id or any hidden fields).\n",
      "4. Use the table and column names exactly as they appear in sql_context. Do not invent or rename tables/columns.\n",
      "5. Use standard ANSI-ish SQL only. Avoid database-specific extensions where possible. Use standard aggregates: COUNT, SUM, AVG, MIN, MAX. If a prompt asks for a row count, prefer COUNT(*) (it counts rows including NULLs in other columns); use COUNT(col) only when the prompt intentionally requests counting non-null values of col.\n",
      "6. When aggregating, include GROUP BY on every non-aggregated column that appears in the SELECT (i.e., GROUP BY all non-aggregated output columns). If the prompt asks for \"per X\" or \"for each X\", return X and only the requested aggregate(s).\n",
      "7. Apply WHERE / HAVING / GROUP BY / ORDER BY only when required by the prompt or necessary to produce a correct aggregation. Do not add extra filters, GROUP BYs, HAVINGs, or ORDER BYs that change the semantics.\n",
      "8. If the prompt asks for \"ranked by\" or an explicit ordering, include ORDER BY. Do not add ORDER BY for presentation unless requested. If the prompt requests \"top N\" and N is specified, use an appropriate standard SQL limiting construct; if N is not specified, do not guess a default N.\n",
      "9. For queries that compare to the maximum/minimum within a subset, ensure the same filters from the prompt apply to the aggregate subquery. (E.g., when returning rows whose value equals the MAX over a filtered subset, compute MAX(...) using the same WHERE conditions.)\n",
      "10. Preserve NULL semantics implicitly. Do not add COALESCE or extra NULL-filtering unless the prompt explicitly asks for a specific treatment of NULLs. Remember AVG in SQL ignores NULLs by default.\n",
      "11. Follow the boolean and string representations used in sql_context when forming predicates (e.g., TRUE/FALSE, quoted strings exactly as seen).\n",
      "12. When the prompt is ambiguous (for example \"top N\" without N), do not invent specifics. If the prompt's context implies a reasonable default unambiguously, you may apply that; otherwise avoid adding assumptions.\n",
      "13. Use concise, meaningful aliases when helpful (e.g., AS total_quantity), but aliases must not change the semantics or add extra columns.\n",
      "14. Do not use comments in the SQL or extra statements (no multiple statements separated by semicolons beyond a terminating semicolon if you include one).\n",
      "\n",
      "Practical strategies and clarifications (apply these generally):\n",
      "- \"How many employees ...?\" → SELECT COUNT(*) FROM Employees;\n",
      "- \"Totals by X\" or \"per X\" → GROUP BY X and SUM(...) (or other requested aggregate) and return X plus the aggregates only.\n",
      "- \"Number of X and average of Y for subset Z\" → apply WHERE Z, then SELECT COUNT(*) and AVG(Y) (grouping only if prompt requests per-group).\n",
      "- \"Countries with the highest num_orgs\" → compute the maximum using a subquery or window function, but ensure any filters present in the prompt are applied inside that subquery (e.g., WHERE name LIKE '%ethical AI%') so the MAX is computed over the intended subset.\n",
      "- Use sample INSERT values to detect column types and representation conventions (e.g., if INSERTs use TRUE/FALSE then use TRUE/FALSE in predicates).\n",
      "- If the prompt requests ties to be included (e.g., \"countries with the highest ...\"), return all rows that equal the MAX (do not arbitrarily limit to one row).\n",
      "\n",
      "Output formatting rule (mandatory):\n",
      "- Place the SQL statement alone in a single code block. No other text before or after the block.\n",
      "\n",
      "Remember: your goal is a single correct, minimal SQL query that exactly matches the prompt using the schema provided in sql_context. Follow the rules strictly.\n",
      "2025/10/12 21:21:09 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n",
      "2025/10/12 21:21:09 INFO dspy.teleprompt.gepa.gepa: Iteration 34: New subsample score is not better, skipping\n",
      "GEPA Optimization:  99%|█████████▉| 1174/1180 [57:04<00:54,  9.08s/rollouts]2025/10/12 21:21:09 INFO dspy.teleprompt.gepa.gepa: Iteration 35: Selected program 0 score: 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 3 (66.7%): 100%|██████████| 3/3 [00:14<00:00,  4.88s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:21:24 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 21:21:53 INFO dspy.teleprompt.gepa.gepa: Iteration 35: Proposed new text for predict: You are a SQL-writing database expert. For each task you will be given:\n",
      "- sql_context: SQL CREATE TABLE and INSERT statements that define the available tables, columns and sample data.\n",
      "- sql_prompt: a natural-language request for the data the user wants.\n",
      "\n",
      "Your job: produce a correct, minimal, and robust SQL query that answers sql_prompt given the schema and sample data, plus a 1–3 sentence concise reasoning that explains your approach and any assumptions. Always base your answer on the exact schema and sample INSERTs in sql_context.\n",
      "\n",
      "Detailed rules and best practices you must follow:\n",
      "\n",
      "1. Examine schema and sample data first\n",
      "   - Use the exact table and column names from the CREATE TABLE statements.\n",
      "   - Use sample INSERTs to sanity-check that the query will return the expected rows/values.\n",
      "\n",
      "2. Match the prompt semantics exactly\n",
      "   - If the prompt implies a filter (e.g., \"union members\" in context where member_status exists and the intended meaning is active members), prefer to apply the explicit condition. If the prompt is ambiguous, state your assumption in the brief reasoning and then implement it.\n",
      "   - For boolean fields use explicit boolean checks (e.g., WHERE successful_cb = true) or count true values with SUM(CASE WHEN successful_cb THEN 1 ELSE 0 END).\n",
      "\n",
      "3. Aggregation and conditional counting\n",
      "   - Use GROUP BY when the prompt requests grouped aggregates.\n",
      "   - For conditional counts, prefer SUM(CASE WHEN condition THEN 1 ELSE 0 END) (portable across SQL dialects) rather than relying on dialect-specific boolean-to-integer casts.\n",
      "   - Be careful with COUNT(*) vs COUNT(column): COUNT(*) counts rows; COUNT(column) ignores NULLs. Choose the one appropriate to the prompt and explain your choice when necessary.\n",
      "\n",
      "4. Joins and filters\n",
      "   - When data spans multiple tables, JOIN on the correct key(s) with explicit ON clauses.\n",
      "   - Apply WHERE filters to limit rows before aggregation when intended by the prompt.\n",
      "\n",
      "5. Date/time comparisons\n",
      "   - Interpret natural language precisely: \"after 2021-06-01\" typically means capture_time > '2021-06-01' (exclude that date/time); \"on or after\" means >=. If the column is TIMESTAMP and the prompt gives only a DATE, be explicit in reasoning about inclusivity/exclusivity.\n",
      "   - Use explicit string literals for dates/times in ISO format (e.g., '2021-06-01' or '2021-06-01 00:00:00').\n",
      "\n",
      "6. Column selection and aliases\n",
      "   - Return only requested columns. Do not use SELECT * unless the prompt explicitly asks for all columns.\n",
      "   - Provide clear column aliases that reflect the prompt (e.g., AS total_installed_capacity).\n",
      "\n",
      "7. NULLs, casing, and string matching\n",
      "   - Consider NULL values: if counting or aggregating, handle NULLs explicitly if they affect the result.\n",
      "   - For string comparisons that might be case-insensitive in intent, either use explicit LOWER(...) comparisons or state a case-sensitivity assumption in your reasoning.\n",
      "\n",
      "8. SQL portability and dialect caution\n",
      "   - Prefer standard SQL constructs (SUM(CASE ...), ANSI JOINs, ISO date literals) and avoid dialect-only features unless the schema or context clearly uses them.\n",
      "   - If you must use a dialect-specific feature (e.g., ::int cast in PostgreSQL), state that assumption in the reasoning.\n",
      "\n",
      "9. Output format\n",
      "   - Provide a short reasoning (1–3 sentences) that states approach and any assumptions/edge-cases handled.\n",
      "   - Then provide the SQL query only (no additional commentary).\n",
      "   - Keep the SQL concise and syntactically valid.\n",
      "\n",
      "10. Sanity-check results\n",
      "   - Mentally verify the query against the sample INSERTs in sql_context and ensure it would return the expected sample results. If the sample data suggests a special-case behavior (e.g., all statuses are 'active' in samples but schema allows others), mention that in the reasoning if it affects assumptions.\n",
      "\n",
      "Failure modes to avoid (learned from examples):\n",
      "- Don't omit important filters implied by the prompt (e.g., member_status = 'active')—COUNT(*) vs conditional count matters.\n",
      "- Don't return wrong columns or wrong aggregation (e.g., forgetting GROUP BY, or grouping by the wrong field).\n",
      "- Don't change semantics by only modifying order; ordering is fine but must not be the only difference when filter/conditions are required.\n",
      "\n",
      "Follow these rules for every task. Provide the reasoning and then the SQL query that will satisfy sql_prompt given sql_context.\n",
      "2025/10/12 21:22:09 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n",
      "2025/10/12 21:22:09 INFO dspy.teleprompt.gepa.gepa: Iteration 35: New subsample score is not better, skipping\n",
      "GEPA Optimization:  99%|█████████▉| 1174/1180 [58:04<00:17,  2.97s/rollouts]\n"
     ]
    }
   ],
   "source": [
    "max_variants_to_try = 20 # number of variants to test\n",
    "mini_batch_size = 3 # mini-batch size\n",
    "val_set_size = 200 # val-set size\n",
    "\n",
    "def budget_for_variants(N, V, k, slack=2):\n",
    "    # slack handles occasional extra probes/promotions\n",
    "    return V + N * (k + slack)\n",
    "\n",
    "def metric_with_feedback(example, pred, trace=None, pred_name=None, pred_trace=None):\n",
    "    judge_response = judge(sql_context=example.sql_context, sql_prompt=example.sql_prompt, golden_sql=example.sql, candidate_sql=pred.sql)\n",
    "    score = 0\n",
    "    if (judge_response.similar):\n",
    "        score = 1\n",
    "    return dspy.Prediction(score=score, feedback=judge_response.reasoning)\n",
    "\n",
    "val_for_tracking = val_set[:val_set_size]   # 128–512 is a good range\n",
    "train_set_for_optimization = train_set[:val_set_size]\n",
    "optimizer = GEPA(\n",
    "    metric=metric_with_feedback,\n",
    "    num_threads=32,\n",
    "    track_stats=True,\n",
    "    reflection_minibatch_size=mini_batch_size,\n",
    "    reflection_lm=lm,\n",
    "    use_wandb=True,\n",
    "    wandb_api_key=wandb_api_key,\n",
    "    log_dir=\"logs\",\n",
    "    auto=\"light\"   \n",
    ")\n",
    "\n",
    "optimized_program = optimizer.compile(\n",
    "    program,\n",
    "    trainset=train_set_for_optimization,\n",
    "    valset=val_for_tracking,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d21aeb",
   "metadata": {},
   "source": [
    "## Review original and optimized prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51f8d446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a database expert. You are provided with context for how some table(s) were constructed, and a natural language prompt for what the user wants. Your job is to write a SQL query to provide them with the required data.\n"
     ]
    }
   ],
   "source": [
    "print(program.predict.signature.instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf6cf895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a SQL-generation expert whose sole job is:\n",
      "- Read two inputs: sql_context and sql_prompt.\n",
      "  - sql_context: one or more CREATE TABLE statements plus INSERTs that define the available tables, column names, and example values (use these to infer exact table/column names and types and the representation conventions used, e.g., boolean TRUE/FALSE or 't'/'f', date/time formats, string quoting).\n",
      "  - sql_prompt: a short natural-language instruction describing exactly what columns/aggregations/filters/aggregation-granularity the user wants.\n",
      "\n",
      "Your output:\n",
      "- Produce exactly one SQL statement (query or DML) that answers the sql_prompt against the schema in sql_context.\n",
      "- Output only that SQL statement, and nothing else.\n",
      "- Place the SQL statement inside a single code block and do not include any other text, comments, or explanation outside the code block.\n",
      "\n",
      "Hard rules you must follow:\n",
      "1. Single SQL statement only. No extra text, no explanations, no alternative queries.\n",
      "2. Use table names and column names exactly as they appear in sql_context. Do not invent tables or columns.\n",
      "3. Return exactly the columns the prompt requests (and no additional columns). Aliases are allowed but must not add extra logical columns.\n",
      "4. Do not change, add, or remove filters/aggregations that change the semantics of the user's request. Apply WHERE, GROUP BY, HAVING, ORDER BY only when required by the prompt or required for correct SQL aggregation semantics.\n",
      "5. Use standard SQL aggregate functions only: COUNT, SUM, AVG, MIN, MAX. When selecting non-aggregated columns with aggregates, include those non-aggregated columns in GROUP BY.\n",
      "6. When the prompt requests \"per X\", \"by X\", or \"for each X\", the result must include X (the grouping column) and only the requested aggregate(s).\n",
      "7. For boolean columns, respect the representation shown in sql_context (e.g., TRUE/FALSE or 'true'/'false'); use the same literal form in WHERE clauses.\n",
      "8. For comparisons against strings use the same quoting style shown in sql_context (single quotes).\n",
      "9. Avoid DBMS-specific extensions. Produce ANSI-ish SQL that will run on common RDBMSs. (Do not rely on vendor-only functions or nonstandard interval/date arithmetic unless the sql_context already demonstrates that form.)\n",
      "10. Handle NULL semantics implicitly (do not coalesce or filter NULLs unless the prompt explicitly requests a specific behavior for NULLs).\n",
      "11. If the prompt is ambiguous (for example \"top N\" without an N), do not guess; only make a reasonable default if the sql_context clearly implies it. Prefer not to assume unspecified parameters.\n",
      "12. When the prompt asks for a count/number of rows, use COUNT(*) unless the prompt explicitly requests distinct counts or the context clearly implies counting distinct identifiers.\n",
      "13. For UPDATE/DELETE/INSERT prompts, produce a single correct DML statement using the exact table/column names and filters specified (and nothing else).\n",
      "14. Do not add ORDER BY unless the prompt explicitly requests a sort. Avoid ORDER BY purely for presentation.\n",
      "\n",
      "Practical strategies / interpretation guidance:\n",
      "- Use the CREATE TABLE and INSERT values to determine exact column names, types, and examples of how values are formatted (this guides string quoting, boolean literals, timestamp formats).\n",
      "- For \"average budget for each continent\" -> GROUP BY continent, SELECT continent, AVG(budget).\n",
      "- For time-window filters relative to \"last month\" or \"past N days\": prefer standard SQL constructs shown in sql_context; if context contains no time arithmetic example and the prompt is relative to now, you may use CURRENT_TIMESTAMP with standard ANSI interval syntax only if necessary and not DB-specific.\n",
      "- For \"number of X\" + \"average of Y\" + filters: apply WHERE then use COUNT(*) and AVG(Y).\n",
      "- For \"get the number of cases handled per attorney\": return exactly the attorney column requested (name or id as in schema) and the cases_handled aggregate/count — do not add id or ordering unless requested.\n",
      "- For \"distinct\" or \"unique\" explicitly requested, use COUNT(DISTINCT column) or SELECT DISTINCT column(s).\n",
      "- If prompt asks to \"include only rows where column is NULL/non-NULL\", apply WHERE column IS (NOT) NULL exactly as requested.\n",
      "- When grouping, include all non-aggregated SELECT columns in GROUP BY.\n",
      "- Column aliases are allowed and may be used to clarify output names but should be concise and reflect the requested metric.\n",
      "\n",
      "Common mistakes to avoid (do not do):\n",
      "- Do not add columns not requested, even if they seem helpful.\n",
      "- Do not change or add filters that alter the user's requested subset.\n",
      "- Do not pick COUNT(DISTINCT) when the prompt asks for a count and does not specify distinct (unless the context implies duplicates must be deduplicated).\n",
      "- Do not add ORDER BY unless explicitly asked.\n",
      "- Do not include multiple statements or transaction control statements; return one statement only.\n",
      "\n",
      "Output formatting:\n",
      "- Put the single SQL statement inside one code block and nothing else.\n",
      "\n",
      "Use these rules for every input. The sql_context is authoritative for schema, naming, and literal representations. Make no external assumptions beyond what's implied by sql_context and the natural-language sql_prompt.\n"
     ]
    }
   ],
   "source": [
    "print(optimized_program.predict.signature.instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e85afb4",
   "metadata": {},
   "source": [
    "## Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64245ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datasets import Dataset\n",
    "from time import perf_counter\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "def evaluate_program(\n",
    "    program,\n",
    "    ds_test: Dataset,\n",
    "    limit: int = 100,\n",
    "    max_workers: int = 8,\n",
    "    field_map: Optional[Dict[str, str]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate a DSPy program on the first `limit` rows of a HF Dataset split.\n",
    "\n",
    "    Args:\n",
    "        program: a DSPy Module with signature program(sql_prompt=..., sql_context=...)\n",
    "        ds_test: Hugging Face Dataset (e.g., ds[\"test\"])\n",
    "        limit: number of rows to evaluate (default 100)\n",
    "        max_workers: parallel threads for I/O-bound LM + judge\n",
    "        field_map: optional mapping if your column names differ:\n",
    "                   {\"sql_prompt\": \"...\", \"sql_context\": \"...\", \"sql\": \"...\"}\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"accuracy\": float,\n",
    "          \"correct\": int,\n",
    "          \"total\": int,\n",
    "          \"avg_latency_s\": float,\n",
    "          \"failures\": [ {idx, reason, pred_sql, feedback} ... up to 20 ],\n",
    "        }\n",
    "    \"\"\"\n",
    "    if field_map is None:\n",
    "        field_map = {\"sql_prompt\": \"sql_prompt\", \"sql_context\": \"sql_context\", \"sql\": \"sql\"}\n",
    "\n",
    "    n = min(limit, len(ds_test))\n",
    "    subset = ds_test.select(range(n))\n",
    "    start = perf_counter()\n",
    "\n",
    "    def _eval_one(i_row):\n",
    "        i, row = i_row\n",
    "        try:\n",
    "            pred = program(\n",
    "                sql_prompt=row[field_map[\"sql_prompt\"]],\n",
    "                sql_context=row[field_map[\"sql_context\"]],\n",
    "            )\n",
    "            pred_sql = getattr(pred, \"sql\", None) or (pred.get(\"sql\") if isinstance(pred, dict) else None) or \"\"\n",
    "            jr = judge(\n",
    "                sql_context=row[field_map[\"sql_context\"]],\n",
    "                sql_prompt=row[field_map[\"sql_prompt\"]],\n",
    "                golden_sql=row[field_map[\"sql\"]],\n",
    "                candidate_sql=pred_sql,\n",
    "            )\n",
    "            ok = bool(getattr(jr, \"similar\", False))\n",
    "            feedback = getattr(jr, \"reasoning\", \"\") or \"\"\n",
    "            return (i, ok, pred_sql, feedback, None)\n",
    "        except Exception as e:\n",
    "            return (i, False, \"\", \"\", f\"{type(e).__name__}: {e}\")\n",
    "\n",
    "    results = []\n",
    "    # Threaded evaluation (I/O bound: LM + judge). Tune max_workers to your provider limits.\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futures = [ex.submit(_eval_one, (i, subset[i])) for i in range(n)]\n",
    "        for f in as_completed(futures):\n",
    "            results.append(f.result())\n",
    "\n",
    "    # Sort back to input order\n",
    "    results.sort(key=lambda x: x[0])\n",
    "\n",
    "    correct = sum(1 for _, ok, *_ in results if ok)\n",
    "    total = n\n",
    "    acc = correct / total if total else 0.0\n",
    "    elapsed = perf_counter() - start\n",
    "    avg_lat = elapsed / total if total else 0.0\n",
    "\n",
    "    failures = []\n",
    "    for i, ok, pred_sql, feedback, err in results:\n",
    "        if not ok and len(failures) < 20:\n",
    "            failures.append({\n",
    "                \"idx\": i,\n",
    "                \"reason\": (\"error: \" + err) if err else \"mismatch\",\n",
    "                \"pred_sql\": pred_sql,\n",
    "                \"feedback\": feedback,\n",
    "            })\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"correct\": correct,\n",
    "        \"total\": total,\n",
    "        \"avg_latency_s\": avg_lat,\n",
    "        \"failures\": failures,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "525c8c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 0.634 (317/500)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate original and optimized on ds[\"test\"][:100]\n",
    "test_split = ds[\"test\"]\n",
    "orig_metrics = evaluate_program(program, test_split, limit=500, max_workers=32)\n",
    "\n",
    "print(\"Original:\", orig_metrics[\"accuracy\"], f\"({orig_metrics['correct']}/{orig_metrics['total']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a26d5116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized: 0.646 (323/500)\n"
     ]
    }
   ],
   "source": [
    "opt_metrics  = evaluate_program(optimized_program, test_split, limit=500, max_workers=32)\n",
    "print(\"Optimized:\", opt_metrics[\"accuracy\"], f\"({opt_metrics['correct']}/{opt_metrics['total']})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
