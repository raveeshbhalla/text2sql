{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b76a8c0d",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a234a155",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df341691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U dspy datasets tabulate duckdb pandas numpy ipywidgets \"sqlglot[rs]\" wandb --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4329c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from datasets import load_dataset\n",
    "import tabulate\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "288f15f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env.local\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "if not wandb_api_key:\n",
    "    raise ValueError(\"WANDB_API_KEY not found in environment variables\")\n",
    "\n",
    "lm = dspy.LM(\"openai/gpt-4.1-mini\", api_key=openai_api_key, temperature=1, max_tokens=16000)\n",
    "reflection_lm = dspy.LM(\"openai/gpt-5-mini\", api_key=openai_api_key, temperature=1, max_tokens=16000)\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e655b0",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aef6f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"gretelai/synthetic_text_to_sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13420ef6",
   "metadata": {},
   "source": [
    "# Set up DSPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cab5f8",
   "metadata": {},
   "source": [
    "## Set up Signature and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4380853",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProblemDef(dspy.Signature):\n",
    "    \"\"\"You are a database expert. You are provided with context for how some table(s) were constructed, and a natural language prompt for what the user wants. Your job is to write a SQL query to provide them with the required data.\"\"\"\n",
    "    \n",
    "    sql_context: str = dspy.InputField(description=\"SQL queries for creating the table(s) and loading some data\")\n",
    "    sql_prompt: str = dspy.InputField(description=\"User's natural language prompt\")\n",
    "    sql: str = dspy.OutputField(description=\"SQL query that delivers on the user's request. Format as code that can be directly run without any changes – do not use new lines or anything else of that sort.\")\n",
    "\n",
    "program = dspy.ChainOfThought(ProblemDef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c92e6cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install duckdb pandas numpy sqlglot --quiet\n",
    "import duckdb, pandas as pd, numpy as np, re\n",
    "import sqlglot\n",
    "from sqlglot import parse_one\n",
    "\n",
    "_ORDER_BY = re.compile(r\"\\border\\s+by\\b\", re.IGNORECASE)\n",
    "\n",
    "def _split_sql_statements(script: str):\n",
    "    out, buf, q = [], [], None\n",
    "    i, n = 0, len(script)\n",
    "    while i < n:\n",
    "        ch = script[i]\n",
    "        if q:\n",
    "            buf.append(ch)\n",
    "            if ch == q:\n",
    "                if i + 1 < n and script[i+1] == q:\n",
    "                    buf.append(script[i+1]); i += 1\n",
    "                else:\n",
    "                    q = None\n",
    "        else:\n",
    "            if ch in (\"'\", '\"', \"`\"):\n",
    "                q = ch; buf.append(ch)\n",
    "            elif ch == ';':\n",
    "                s = \"\".join(buf).strip()\n",
    "                if s: out.append(s)\n",
    "                buf = []\n",
    "            else:\n",
    "                buf.append(ch)\n",
    "        i += 1\n",
    "    tail = \"\".join(buf).strip()\n",
    "    if tail: out.append(tail)\n",
    "    return out\n",
    "\n",
    "import re\n",
    "from sqlglot import parse_one\n",
    "\n",
    "_SQLITE_DATE_RE = re.compile(\n",
    "    r\"\"\"\\bdate\\s*\\(\\s*'now'\\s*(?:,\\s*'([+-])\\s*(\\d+)\\s*(year|month|day)s?'\\s*)?\\)\"\"\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "_SQLITE_DATETIME_RE = re.compile(\n",
    "    r\"\"\"\\bdatetime\\s*\\(\\s*'now'\\s*(?:,\\s*'([+-])\\s*(\\d+)\\s*(year|month|day|hour|minute|second)s?'\\s*)?\\)\"\"\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "def _normalize_sqlite_dates(sql: str) -> str:\n",
    "    # date('now') or date('now','-1 year') -> CURRENT_DATE +/- INTERVAL 'N unit'\n",
    "    def _date_subst(m):\n",
    "        sign, num, unit = m.group(1), m.group(2), m.group(3)\n",
    "        if not sign:  # just date('now')\n",
    "            return \"CURRENT_DATE\"\n",
    "        op = \"-\" if sign == \"-\" else \"+\"\n",
    "        return f\"CURRENT_DATE {op} INTERVAL '{num} {unit.lower()}'\"\n",
    "    sql = _SQLITE_DATE_RE.sub(_date_subst, sql)\n",
    "\n",
    "    # datetime('now') / datetime('now','+/-N unit') -> CURRENT_TIMESTAMP +/- INTERVAL 'N unit'\n",
    "    def _dt_subst(m):\n",
    "        sign, num, unit = m.group(1), m.group(2), m.group(3)\n",
    "        if not sign:\n",
    "            return \"CURRENT_TIMESTAMP\"\n",
    "        op = \"-\" if sign == \"-\" else \"+\"\n",
    "        return f\"CURRENT_TIMESTAMP {op} INTERVAL '{num} {unit.lower()}'\"\n",
    "    sql = _SQLITE_DATETIME_RE.sub(_dt_subst, sql)\n",
    "\n",
    "    return sql\n",
    "\n",
    "def _mysql_to_duckdb(stmt: str) -> str:\n",
    "    s = _normalize_sqlite_dates(stmt)  # <-- NEW: normalize SQLite first\n",
    "    try:\n",
    "        return parse_one(s, read=\"mysql\").sql(dialect=\"duckdb\")\n",
    "    except Exception:\n",
    "        # minimal fallbacks for MySQLisms if parse fails\n",
    "        s = re.sub(r\"`([^`]+)`\", r'\"\\1\"', s)\n",
    "        s = re.sub(\n",
    "            r\"DATE_SUB\\s*\\(\\s*(CURRENT_DATE|NOW\\(\\))\\s*,\\s*INTERVAL\\s+(\\d+)\\s+(YEAR|MONTH|DAY)\\s*\\)\",\n",
    "            lambda m: f\"{'CURRENT_DATE' if m.group(1).startswith('CURRENT') else 'CURRENT_DATE'} - INTERVAL '{m.group(2)} {m.group(3).lower()}'\",\n",
    "            s, flags=re.IGNORECASE,\n",
    "        )\n",
    "        s = re.sub(\n",
    "            r\"DATE_ADD\\s*\\(\\s*(CURRENT_DATE|NOW\\(\\))\\s*,\\s*INTERVAL\\s+(\\d+)\\s+(YEAR|MONTH|DAY)\\s*\\)\",\n",
    "            lambda m: f\"{'CURRENT_DATE' if m.group(1).startswith('CURRENT') else 'CURRENT_DATE'} + INTERVAL '{m.group(2)} {m.group(3).lower()}'\",\n",
    "            s, flags=re.IGNORECASE,\n",
    "        )\n",
    "        s = re.sub(r\"\\bIFNULL\\s*\\(\", \"COALESCE(\", s, flags=re.IGNORECASE)\n",
    "        s = re.sub(r\"\\bLOCATE\\s*\\(\\s*([^,]+)\\s*,\\s*([^)]+)\\)\", r\"STRPOS(\\2, \\1)\", s, flags=re.IGNORECASE)\n",
    "        return s\n",
    "\n",
    "def _normalize_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == \"O\":\n",
    "            try:\n",
    "                df[c] = pd.to_numeric(df[c])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return df.replace({np.nan: None})\n",
    "\n",
    "def _exec_script_capture_last_select(con, script: str):\n",
    "    last_df, last_sel_sql = None, None\n",
    "    for raw in _split_sql_statements(script):\n",
    "        stmt = _mysql_to_duckdb(raw)\n",
    "        # detect SELECT after minimal comment strip\n",
    "        s = re.sub(r\"^\\s*(--[^\\n]*\\n|/\\*.*?\\*/\\s*)*\", \"\", stmt, flags=re.DOTALL)\n",
    "        if re.match(r\"(?is)^\\s*(with\\b.*?select|select)\\b\", s):\n",
    "            last_df = con.execute(stmt).fetchdf()\n",
    "            last_sel_sql = stmt\n",
    "        else:\n",
    "            con.execute(stmt)\n",
    "    if last_df is not None:\n",
    "        last_df = _normalize_df(last_df)\n",
    "    return last_df, last_sel_sql\n",
    "\n",
    "def evaluate_sql(sql_context: str, golden_sql: str, predicted_sql: str):\n",
    "    con = duckdb.connect(\":memory:\")\n",
    "\n",
    "    # context\n",
    "    try:\n",
    "        for raw in _split_sql_statements(sql_context):\n",
    "            con.execute(_mysql_to_duckdb(raw))\n",
    "    except Exception as e:\n",
    "        return 0, {\"reason\": \"context_error\", \"detail\": str(e)}\n",
    "\n",
    "    # golden\n",
    "    try:\n",
    "        gold_df, gold_last_select = _exec_script_capture_last_select(con, golden_sql)\n",
    "    except Exception as e:\n",
    "        return 0, {\"reason\": \"gold_error\", \"detail\": str(e)}\n",
    "    if gold_df is None:\n",
    "        return 0, {\"reason\": \"gold_no_select\", \"detail\": \"No SELECT in golden_sql.\"}\n",
    "\n",
    "    # predicted\n",
    "    try:\n",
    "        pred_df, pred_last_select = _exec_script_capture_last_select(con, predicted_sql)\n",
    "    except Exception as e:\n",
    "        return 0, {\"reason\": \"pred_error\", \"detail\": str(e)}\n",
    "    if pred_df is None:\n",
    "        return 0, {\"reason\": \"pred_no_select\", \"detail\": \"No SELECT in predicted_sql.\"}\n",
    "\n",
    "    # column alignment (allow pred supersets; else try set/positional)\n",
    "    gold_cols, pred_cols = list(gold_df.columns), list(pred_df.columns)\n",
    "    if gold_cols == pred_cols:\n",
    "        pass\n",
    "    elif set(gold_cols).issubset(pred_cols):\n",
    "        pred_df = pred_df[gold_cols]\n",
    "    elif set(gold_cols) == set(pred_cols):\n",
    "        pred_df = pred_df[gold_cols]\n",
    "    elif gold_df.shape[1] == pred_df.shape[1]:\n",
    "        new_names = [f\"c{i}\" for i in range(gold_df.shape[1])]\n",
    "        gold_df = gold_df.copy(); pred_df = pred_df.copy()\n",
    "        gold_df.columns = new_names; pred_df.columns = new_names\n",
    "    else:\n",
    "        return 0, {\"reason\": \"column_mismatch\",\n",
    "                   \"detail\": f\"Different number of columns: expected {gold_df.shape[1]}, got {pred_df.shape[1]}\"}\n",
    "\n",
    "    # ordering rule from gold's last SELECT\n",
    "    gold_has_order = bool(_ORDER_BY.search(gold_last_select or \"\"))\n",
    "    if not gold_has_order:\n",
    "        try:\n",
    "            g = gold_df.sort_values(by=list(gold_df.columns), kind=\"mergesort\").reset_index(drop=True)\n",
    "            p = pred_df.sort_values(by=list(gold_df.columns), kind=\"mergesort\").reset_index(drop=True)\n",
    "        except Exception:\n",
    "            g = gold_df.reset_index(drop=True); p = pred_df.reset_index(drop=True)\n",
    "    else:\n",
    "        g = gold_df.reset_index(drop=True); p = pred_df.reset_index(drop=True)\n",
    "\n",
    "    # value compare\n",
    "    if g.shape != p.shape:\n",
    "        return 0, {\"reason\": \"shape_mismatch\", \"detail\": f\"gold {g.shape} vs pred {p.shape}\"}\n",
    "\n",
    "    for c in g.columns:\n",
    "        if pd.api.types.is_numeric_dtype(g[c]) and pd.api.types.is_numeric_dtype(p[c]):\n",
    "            if not np.allclose(g[c].values, p[c].values, rtol=1e-6, atol=1e-8, equal_nan=True):\n",
    "                return 0, {\"reason\": \"value_mismatch\", \"detail\": f\"Numeric mismatch in '{c}'\",\n",
    "                           \"gold_head\": g.head(10).to_dict(\"records\"),\n",
    "                           \"pred_head\": p.head(10).to_dict(\"records\")}\n",
    "        else:\n",
    "            eq = [(x == y) or (x is None and y is None) for x, y in zip(g[c].values, p[c].values)]\n",
    "            if not all(eq):\n",
    "                return 0, {\"reason\": \"value_mismatch\", \"detail\": f\"Mismatch in '{c}'\",\n",
    "                           \"gold_head\": g.head(10).to_dict(\"records\"),\n",
    "                           \"pred_head\": p.head(10).to_dict(\"records\")}\n",
    "    return 1, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb24b156",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "936c6363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: CREATE TABLE upgrades (id INT, cost FLOAT, type TEXT); INSERT INTO upgrades (id, cost, type) VALUES (1, 500, 'Insulation'), (2, 1000, 'HVAC'), (3, 1500, 'Lighting');\n",
      "Prompt: Find the energy efficiency upgrades with the highest cost and their types.\n",
      "Golden sql: SELECT type, cost FROM (SELECT type, cost, ROW_NUMBER() OVER (ORDER BY cost DESC) as rn FROM upgrades) sub WHERE rn = 1;\n",
      "Prediction(\n",
      "    reasoning=\"To find the energy efficiency upgrades with the highest cost, we need to identify the maximum value in the 'cost' column and then select all rows that have that cost, including their types.\",\n",
      "    sql='SELECT type, cost FROM upgrades WHERE cost = (SELECT MAX(cost) FROM upgrades);'\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "demo_index = 4\n",
    "context = ds['train'][demo_index]['sql_context']\n",
    "prompt = ds['train'][demo_index]['sql_prompt']\n",
    "golden_sql = ds['train'][demo_index]['sql']\n",
    "\n",
    "print(f\"Context: {context}\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Golden sql: {golden_sql}\")\n",
    "result = program(sql_context=context, sql_prompt=prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42b7fd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 None\n"
     ]
    }
   ],
   "source": [
    "score, info = evaluate_sql(context, golden_sql, result.sql)\n",
    "print(score, info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58142861",
   "metadata": {},
   "source": [
    "## Environment didn't work, let's use LLM as Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c72a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Judge(dspy.Signature):\n",
    "    \"\"\"You are required to judge two SQL queries for functional similarity. You will be given a context of how the table(s) and data were created, and the natural language prompt from the user\"\"\"\n",
    "\n",
    "    sql_context: str = dspy.InputField(description=\"SQL statement(s) creating the table(s) and the input data\")\n",
    "    sql_prompt: str = dspy.InputField(description=\"Natural language prompt from the user\")\n",
    "    golden_sql: str = dspy.InputField(description=\"The golden SQL query from our dataset\")\n",
    "    candidate_sql: str = dspy.InputField(description=\"A SQL query generated by a model for the same prompt\")\n",
    "    similar: bool = dspy.OutputField(description=\"True if the candidate SQL query is functionally similar to the golden SQL query\")\n",
    "\n",
    "judge = dspy.ChainOfThought(Judge)\n",
    "judge.lm = reflection_lm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1677bf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: CREATE TABLE upgrades (id INT, cost FLOAT, type TEXT); INSERT INTO upgrades (id, cost, type) VALUES (1, 500, 'Insulation'), (2, 1000, 'HVAC'), (3, 1500, 'Lighting');\n",
      "Prompt: Find the energy efficiency upgrades with the highest cost and their types.\n",
      "Golden SQL: SELECT type, cost FROM (SELECT type, cost, ROW_NUMBER() OVER (ORDER BY cost DESC) as rn FROM upgrades) sub WHERE rn = 1;\n",
      "Candidate SQL: SELECT type, cost FROM upgrades WHERE cost = (SELECT MAX(cost) FROM upgrades);\n",
      "Judge Response: Prediction(\n",
      "    reasoning='Both the golden SQL and the candidate SQL retrieve the type and cost of the upgrade(s) with the highest cost from the upgrades table. The golden SQL uses the ROW_NUMBER() window function ordered by cost descending and filters for the first row, effectively picking the single highest cost upgrade. The candidate SQL uses a subquery to find the maximum cost, then selects all upgrades with that max cost. The key behavioral difference is that the golden SQL returns exactly one row even if there are ties, while the candidate SQL returns all rows with the highest cost, thus handling ties by including multiple rows if present. However, for typical intent (\"highest cost upgrade(s)\"), returning all top-cost items is often acceptable and functionally similar for many use cases. Since the prompt asks for \"the upgrades with the highest cost,\" the candidate query arguably better matches the plural form and the prompt\\'s requirement. Hence, functionally, the candidate SQL is similar to the golden SQL despite a slight difference in handling ties.',\n",
      "    similar=True\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "judge_response = judge(sql_context=context, sql_prompt=prompt, golden_sql=golden_sql, candidate_sql=result.sql)\n",
    "print(f\"Context: {context}\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Golden SQL: {golden_sql}\")\n",
    "print(f\"Candidate SQL: {result.sql}\")\n",
    "print(f\"Judge Response: {judge_response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7746087",
   "metadata": {},
   "source": [
    "# Get ready to GEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c35b7379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets dspy-ai\n",
    "import math, random\n",
    "from typing import Callable, List, Tuple, Optional\n",
    "from datasets import Dataset, DatasetDict\n",
    "from dspy import GEPA\n",
    "\n",
    "def split_for_gepa(\n",
    "    ds: Dataset,\n",
    "    to_example: Callable[[dict], \"dspy.Example\"],\n",
    "    val_size: float = 0.15,\n",
    "    seed: int = 42,\n",
    "    group_col: Optional[str] = None,\n",
    "    stratify_col: Optional[str] = None,\n",
    ") -> Tuple[List[\"dspy.Example\"], List[\"dspy.Example\"]]:\n",
    "    \"\"\"\n",
    "    Return (train_set, val_set) as lists of dspy.Example.\n",
    "    - If group_col is set: group-aware split (no group leakage).\n",
    "    - Else if stratify_col is set: use HF stratified split.\n",
    "    - Else: random split.\n",
    "    \"\"\"\n",
    "    assert 0.0 < val_size < 1.0, \"val_size must be in (0,1)\"\n",
    "    rng = random.Random(seed)\n",
    "\n",
    "    # --- Group-aware split (preferred for text2sql) ---\n",
    "    if group_col:\n",
    "        groups = ds[group_col]\n",
    "        # Build group -> indices\n",
    "        g2idx = {}\n",
    "        for i, g in enumerate(groups):\n",
    "            g2idx.setdefault(g, []).append(i)\n",
    "        uniq_groups = list(g2idx.keys())\n",
    "        rng.shuffle(uniq_groups)\n",
    "        n_val_groups = max(1, math.floor(val_size * len(uniq_groups)))\n",
    "        val_groups = set(uniq_groups[:n_val_groups])\n",
    "\n",
    "        val_idx = [i for g in val_groups for i in g2idx[g]]\n",
    "        train_idx = [i for g in uniq_groups[n_val_groups:] for i in g2idx[g]]\n",
    "\n",
    "        # Edge case: if a group is gigantic, ensure both splits non-empty\n",
    "        if not train_idx or not val_idx:\n",
    "            # fallback: plain random split\n",
    "            perm = list(range(len(ds)))\n",
    "            rng.shuffle(perm)\n",
    "            cut = max(1, math.floor(val_size * len(ds)))\n",
    "            val_idx, train_idx = perm[:cut], perm[cut:]\n",
    "\n",
    "        ds_train = ds.select(train_idx)\n",
    "        ds_val = ds.select(val_idx)\n",
    "\n",
    "    # --- Stratified split (when you have a label/cluster column) ---\n",
    "    elif stratify_col:\n",
    "        # HF does stratify on categorical-like columns\n",
    "        parts: DatasetDict = ds.train_test_split(\n",
    "            test_size=val_size,\n",
    "            seed=seed,\n",
    "            stratify_by_column=stratify_col,\n",
    "        )\n",
    "        ds_train, ds_val = parts[\"train\"], parts[\"test\"]\n",
    "\n",
    "    # --- Simple random split ---\n",
    "    else:\n",
    "        parts: DatasetDict = ds.train_test_split(test_size=val_size, seed=seed)\n",
    "        ds_train, ds_val = parts[\"train\"], parts[\"test\"]\n",
    "\n",
    "    # Map to dspy.Example lists\n",
    "    train_set = [to_example(r) for r in ds_train]\n",
    "    val_set = [to_example(r) for r in ds_val]\n",
    "    return train_set, val_set\n",
    "\n",
    "def to_dspy_example(row):\n",
    "    # mark inputs; leave gold 'sql' as label\n",
    "    return dspy.Example(\n",
    "        sql_prompt=row[\"sql_prompt\"],\n",
    "        sql_context=row[\"sql_context\"],\n",
    "        sql=row[\"sql\"],          # gold label\n",
    "    ).with_inputs(\"sql_prompt\", \"sql_context\")\n",
    "\n",
    "\n",
    "# call function that splits ds['train'] into train_set and val_set as needed\n",
    "# ds is your loaded HF dataset dict; we split ds[\"train\"]\n",
    "train_set, val_set = split_for_gepa(\n",
    "    ds[\"train\"],\n",
    "    to_dspy_example,          # your to_dspy_example(row)\n",
    "    val_size=0.5,\n",
    "    seed=42,\n",
    "    group_col=None,      # e.g., \"db_id\" if available\n",
    "    stratify_col=None,   # or a column like \"op_class\" if you want stratification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88d14596",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_variants_to_try = 20 # number of variants to test\n",
    "mini_batch_size = 20 # mini-batch size\n",
    "val_set_size = 200 # val-set size\n",
    "train_set_size = 200 # train-set size\n",
    "\n",
    "def budget_for_variants(N, V, k, slack=2):\n",
    "    # slack handles occasional extra probes/promotions\n",
    "    return V + N * (k + slack)\n",
    "\n",
    "def metric_with_feedback(example, pred, trace=None, pred_name=None, pred_trace=None):\n",
    "    judge_response = judge(sql_context=example.sql_context, sql_prompt=example.sql_prompt, golden_sql=example.sql, candidate_sql=pred.sql)\n",
    "    score = 0\n",
    "    if (judge_response.similar):\n",
    "        score = 1\n",
    "    return dspy.Prediction(score=score, feedback=judge_response.reasoning)\n",
    "\n",
    "val_for_tracking = val_set[:val_set_size]   # 128–512 is a good range\n",
    "train_set_for_optimization = train_set[:train_set_size]\n",
    "optimizer = GEPA(\n",
    "    metric=metric_with_feedback,\n",
    "    num_threads=32,\n",
    "    track_stats=True,\n",
    "    reflection_minibatch_size=mini_batch_size,\n",
    "    reflection_lm=reflection_lm,\n",
    "    use_wandb=True,\n",
    "    wandb_api_key=wandb_api_key,\n",
    "    log_dir=\"logs\",\n",
    "    auto=\"light\"   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77daa2b",
   "metadata": {},
   "source": [
    "# Run GEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "940c1f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 16:00:34 INFO dspy.teleprompt.gepa.gepa: Running GEPA for approx 1180 metric calls of the program. This amounts to 2.95 full evals on the train+val set.\n",
      "2025/10/14 16:00:34 INFO dspy.teleprompt.gepa.gepa: Using 200 examples for tracking Pareto scores. You can consider using a smaller sample of the valset to allow GEPA to explore more diverse solutions within the same budget.\n",
      "/Users/raveesh/dev/text2sql/.venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/Users/raveesh/dev/text2sql/.venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/raveesh/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mraveeshbhalla90\u001b[0m (\u001b[33mraveeshbhalla90-personal\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/raveesh/dev/text2sql/wandb/run-20251014_160035-uwb5plhj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/raveeshbhalla90-personal/text2sql/runs/uwb5plhj' target=\"_blank\">lunar-firefly-14</a></strong> to <a href='https://wandb.ai/raveeshbhalla90-personal/text2sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/raveeshbhalla90-personal/text2sql' target=\"_blank\">https://wandb.ai/raveeshbhalla90-personal/text2sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/raveeshbhalla90-personal/text2sql/runs/uwb5plhj' target=\"_blank\">https://wandb.ai/raveeshbhalla90-personal/text2sql/runs/uwb5plhj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [dspy, litellm, openai] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "GEPA Optimization:   0%|          | 0/1180 [00:00<?, ?rollouts/s]2025/10/14 16:00:37 INFO dspy.evaluate.evaluate: Average Metric: 136.0 / 200 (68.0%)\n",
      "2025/10/14 16:00:37 INFO dspy.teleprompt.gepa.gepa: Iteration 0: Base program full valset score: 0.68\n",
      "GEPA Optimization:  17%|█▋        | 200/1180 [00:01<00:07, 136.71rollouts/s]2025/10/14 16:00:37 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Selected program 0 score: 0.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.00 / 20 (70.0%): 100%|██████████| 20/20 [00:00<00:00, 157.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 16:00:37 INFO dspy.evaluate.evaluate: Average Metric: 14.0 / 20 (70.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 16:01:19 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Proposed new text for predict: You are a SQL-writing database expert. You will be given two inputs:\n",
      "- sql_context: one or more CREATE TABLE and INSERT statements describing the schema and sample data.\n",
      "- sql_prompt: a natural-language request describing the data or change the user wants.\n",
      "\n",
      "Your job: produce a single correct SQL statement that answers the sql_prompt using the tables and columns exactly as defined in sql_context. Also include a very short (1–3 sentence) plain-language reasoning comment explaining your interpretation/assumptions. If the prompt is an action (INSERT/UPDATE/DELETE) produce that DML statement; if it's a question, produce a SELECT that returns the requested result.\n",
      "\n",
      "Output rules and best practices (apply these on every task):\n",
      "\n",
      "1. Output format\n",
      "   - Begin with a one- or two-sentence reasoning comment describing how you interpreted the prompt and any important assumptions.\n",
      "   - Then output the SQL query only (no extra text). The reasoning may be formatted either as a prefixed SQL comment (e.g., -- reasoning ...) or as a very short plaintext line immediately before the SQL. Keep everything concise.\n",
      "\n",
      "2. Use the provided schema\n",
      "   - Use only table and column names present in sql_context. Do not invent table/column names unless you explicitly state them as an assumption in the short reasoning.\n",
      "   - Prefer authoritative sources in the context: if an attribute logically belongs in another table (e.g., company sector in a companies table), join to that table rather than trusting a duplicate field elsewhere, unless context shows the other column is the authoritative one.\n",
      "\n",
      "3. Aggregation, grouping, and result shape\n",
      "   - If the question asks for totals \"for each X\" or \"by X\", use GROUP BY X and return one row per X.\n",
      "   - If the question asks for totals of multiple categories in a single summary (e.g., \"total number of male and female members\"), return a single row with separate columns using conditional aggregation:\n",
      "     SUM(CASE WHEN gender = 'Male' THEN 1 ELSE 0 END) AS total_male, SUM(CASE WHEN gender = 'Female' THEN 1 ELSE 0 END) AS total_female\n",
      "   - If the prompt implies a single scalar (e.g., maximum, total, average), use the appropriate aggregate (MAX, SUM, AVG) and return one column. Use AS alias to make the output clear.\n",
      "\n",
      "4. Filtering and predicates\n",
      "   - Use WHERE to apply filters exactly as asked (e.g., region = 'Mediterranean Sea'), and include all logical conditions implied by the prompt (e.g., both country and year if asked).\n",
      "   - For text matching when the prompt implies a substring (e.g., \"dress\", \"AI ethics course\"), use LIKE '%term%' and make the assumption explicit in the short reasoning if case-sensitivity or alternative naming might matter.\n",
      "   - For date phrasing convert natural language to explicit date literals using the same format as in sql_context (e.g., \"third day of 2022\" -> '2022-01-03').\n",
      "\n",
      "5. Joins and subqueries\n",
      "   - Use JOINs to combine tables when attributes needed are in different tables.\n",
      "   - Prefer NOT EXISTS over NOT IN when excluding rows based on subqueries to avoid NULL pitfalls, unless the dataset clearly lacks NULLs.\n",
      "   - Use HAVING to filter grouped results (e.g., routes with COUNT(*) > 5).\n",
      "\n",
      "6. Missing or insufficient data\n",
      "   - If required tables or columns for the requested calculation are missing from sql_context, do not invent data silently. Instead either:\n",
      "     a) Ask for clarification (very short), or\n",
      "     b) Provide a commented hypothetical query that shows the assumed table/column names and states assumptions in the brief reasoning.\n",
      "   - If the context clearly permits a best-effort answer, produce the query and explicitly state assumptions.\n",
      "\n",
      "7. Mutating statements (INSERT/UPDATE/DELETE)\n",
      "   - Always include a WHERE clause for DELETE/UPDATE unless the prompt explicitly requests deleting/updating all rows.\n",
      "   - For INSERTs, explicitly list target columns in the INSERT INTO (...) VALUES (...) to avoid relying on column order.\n",
      "\n",
      "8. Dialect and limits\n",
      "   - Use commonly accepted SQL constructs. For limiting rows, you may use LIMIT N (acceptable) or FETCH FIRST N ROWS ONLY; either is fine but be consistent.\n",
      "   - Use standard SQL aggregation and conditional expressions (SUM(CASE WHEN ... THEN 1 ELSE 0 END)) rather than vendor-specific shortcuts.\n",
      "\n",
      "9. Result formatting choices\n",
      "   - If the prompt's natural language admits multiple reasonable output shapes (e.g., grouped rows vs one-row pivot), choose the shape that most directly matches the wording. If ambiguous, state the chosen shape and why in the short reasoning.\n",
      "\n",
      "10. Safety and correctness\n",
      "   - Do not run or simulate the SQL — just produce a syntactically correct statement that would run given the schema.\n",
      "   - Avoid extraneous columns or joins that are not needed to answer the prompt.\n",
      "   - Make your assumptions explicit and minimal.\n",
      "\n",
      "Examples of patterns to use (not exhaustive)\n",
      "   - Count per group: SELECT group_col, COUNT(*) AS cnt FROM table GROUP BY group_col;\n",
      "   - Conditional counts as columns: SELECT SUM(CASE WHEN gender='Male' THEN 1 ELSE 0 END) AS total_male, SUM(CASE WHEN gender='Female' THEN 1 ELSE 0 END) AS total_female FROM table WHERE ...;\n",
      "   - Join to authoritative table: SELECT COUNT(*) FROM investments i JOIN companies c ON i.company_id = c.id WHERE c.sector = 'renewable energy';\n",
      "   - Handle \"no data\" situation: -- Insufficient data: Donations table missing. (or provide a hypothetical query and state assumptions)\n",
      "\n",
      "Follow these rules strictly. Be brief in reasoning and provide a single, correct SQL statement that addresses the user's prompt given the provided sql_context.\n",
      "2025/10/14 16:01:26 INFO dspy.evaluate.evaluate: Average Metric: 16.0 / 20 (80.0%)\n",
      "2025/10/14 16:01:59 INFO dspy.evaluate.evaluate: Average Metric: 133.0 / 200 (66.5%)\n",
      "2025/10/14 16:01:59 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Full valset score for new program: 0.665\n",
      "2025/10/14 16:01:59 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Full train_val score for new program: 0.665\n",
      "2025/10/14 16:01:59 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Individual valset scores for new program: [0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/14 16:01:59 INFO dspy.teleprompt.gepa.gepa: Iteration 1: New valset pareto front scores: [0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/14 16:01:59 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Full valset pareto front score: 0.71\n",
      "2025/10/14 16:01:59 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Updated valset pareto front programs: [{0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}]\n",
      "2025/10/14 16:01:59 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best valset aggregate score so far: 0.68\n",
      "2025/10/14 16:01:59 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best program as per aggregate score on train_val: 0\n",
      "2025/10/14 16:01:59 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best program as per aggregate score on valset: 0\n",
      "2025/10/14 16:01:59 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best score on valset: 0.68\n",
      "2025/10/14 16:01:59 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best score on train_val: 0.68\n",
      "2025/10/14 16:01:59 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Linear pareto front program index: 0\n",
      "2025/10/14 16:01:59 INFO dspy.teleprompt.gepa.gepa: Iteration 1: New program candidate index: 1\n",
      "GEPA Optimization:  37%|███▋      | 440/1180 [01:23<02:41,  4.58rollouts/s] 2025/10/14 16:01:59 INFO dspy.teleprompt.gepa.gepa: Iteration 2: No merge candidates found\n",
      "2025/10/14 16:01:59 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Selected program 1 score: 0.665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.00 / 20 (75.0%): 100%|██████████| 20/20 [00:10<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 16:02:10 INFO dspy.evaluate.evaluate: Average Metric: 15.0 / 20 (75.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 16:02:39 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Proposed new text for predict: You are a SQL-writing database expert. You will be given two inputs:\n",
      "- sql_context: one or more CREATE TABLE and INSERT statements describing the schema and sample data.\n",
      "- sql_prompt: a natural-language request describing the data or change the user wants.\n",
      "\n",
      "Your job: produce exactly one syntactically correct SQL statement that answers the sql_prompt using the tables and columns exactly as defined in sql_context, plus a very short plain-language reasoning comment (1–3 sentences) immediately before the SQL. Follow these rules strictly.\n",
      "\n",
      "1) Output shape and format\n",
      "   - Always begin with a 1–3 sentence reasoning comment explaining how you interpreted the prompt and any assumptions. The comment may be formatted as an SQL comment line (e.g., -- reasoning ...) or as a single plain-text line immediately before the SQL.\n",
      "   - After that comment, output only one SQL statement and nothing else. Do not include extra explanation, examples, or multiple SQL statements.\n",
      "   - Keep the reasoning concise and explicit about any assumptions (date interpretation, substring matching, missing columns, default values, etc.).\n",
      "\n",
      "2) Use only the provided schema and names\n",
      "   - Use only table and column names present in sql_context. Do not invent names unless you explicitly state the assumption in the brief reasoning comment and the SQL is a hypothetical commented example (see rule 6).\n",
      "   - If an attribute logically belongs in another table and that authoritative table exists in context, join to it rather than trusting duplicated columns elsewhere.\n",
      "\n",
      "3) Aggregation and grouping\n",
      "   - If the prompt asks for totals \"for each X\" or \"by X\", return one row per X: GROUP BY X.\n",
      "   - If the prompt asks for totals of multiple categories in a single summary, use conditional aggregation (SUM(CASE WHEN ... THEN 1 ELSE 0 END) AS ...).\n",
      "   - If the prompt implies a single scalar (total, max, avg), use an aggregate (SUM, MAX, AVG) and return one column with an AS alias.\n",
      "   - Use HAVING to filter grouped results (e.g., HAVING COUNT(*) > 1).\n",
      "\n",
      "4) Filtering, text matching, and dates\n",
      "   - Apply filters exactly as asked (e.g., WHERE region = 'Mediterranean Sea' AND year = 2020).\n",
      "   - If the prompt implies substring/text matching, use LIKE '%term%' and state any case-sensitivity assumptions in the reasoning.\n",
      "   - For relative date language (e.g., \"last week\", \"Q2 2019\"), prefer converting to explicit date literals matching the date format present in sql_context. If you must use CURRENT_DATE or a dynamic interval, state that assumption clearly and use standard SQL (e.g., CURRENT_DATE - INTERVAL '7 DAY' or DATE_SUB(CURRENT_DATE, INTERVAL 1 WEEK) depending on context).\n",
      "   - When the prompt uses ambiguous date ranges (e.g., \"last week\"), explicitly say what you interpret it to mean in the reasoning.\n",
      "\n",
      "5) Joins, duplicates, and counts\n",
      "   - JOIN tables when needed to get authoritative attributes. Use LEFT JOIN when optional child rows may be missing and you still want the parent rows.\n",
      "   - When joining 1-to-many tables but the desired result is one row per parent (e.g., likes/shares per post), aggregate the child table(s) (SUM/COUNT) and GROUP BY the parent (post_id) to avoid duplicate rows.\n",
      "   - Use COALESCE(...) when returning counts or sums from LEFT JOINed tables to replace NULLs with 0 if appropriate.\n",
      "\n",
      "6) Excluding rows and NULL-safe patterns\n",
      "   - Prefer NOT EXISTS over NOT IN to exclude rows unless context guarantees no NULLs.\n",
      "   - For DELETE/UPDATE, always include a WHERE clause unless the prompt explicitly asks to affect all rows.\n",
      "\n",
      "7) Mutating statements (INSERT/UPDATE/DELETE)\n",
      "   - For INSERTs always list target columns (INSERT INTO table (col1, col2, ...) VALUES (...)).\n",
      "   - For INSERTs: if required columns are unspecified by the prompt, state assumptions in the reasoning and set remaining columns to NULL or sensible defaults, or ask for clarification (very short).\n",
      "   - For UPDATE/DELETE: always include a WHERE clause unless prompt explicitly requests modifying all rows.\n",
      "\n",
      "8) Missing or insufficient data\n",
      "   - If required tables or columns are missing, do not invent them silently. Either:\n",
      "     a) Ask for clarification in one short sentence (e.g., \"Missing table X — please provide it.\"), or\n",
      "     b) Provide a commented hypothetical query that shows assumed table/column names and explicitly state the assumptions in the reasoning comment.\n",
      "   - If an attribute is ambiguous (e.g., \"sorted by genre\" but genre column absent), explicitly say how you resolved the ambiguity and why, then produce the SQL accordingly.\n",
      "\n",
      "9) Result-shape choices and ambiguity\n",
      "   - If the prompt admits multiple reasonable output shapes (e.g., grouped rows vs a pivot), choose the shape that most directly matches the natural language. If ambiguous, pick a reasonable default and state that choice in the reasoning (one sentence).\n",
      "   - If the prompt asks for both an identifier and an aggregate (e.g., \"location with highest avg age\"), you may return only the identifier if that's what the prompt asked; if you include the aggregate, that's also acceptable but state this choice.\n",
      "\n",
      "10) SQL dialect and style\n",
      "   - Use broadly compatible SQL. For limiting rows, use LIMIT N or FETCH FIRST N ROWS ONLY; be consistent.\n",
      "   - Use standard SQL aggregation and conditional expressions (SUM(CASE WHEN ... THEN ... ELSE ... END)).\n",
      "   - Do not include vendor-specific functions unless clearly necessary and consistent with sql_context.\n",
      "\n",
      "11) Safety, correctness, and minimalism\n",
      "   - Do not run or simulate the SQL — produce a syntactically correct statement only.\n",
      "   - Avoid extraneous columns or unnecessary joins.\n",
      "   - Make any assumptions explicit in the brief reasoning; keep assumptions minimal.\n",
      "   - If the sql_context already satisfies an action asked (e.g., CREATE TABLE X already present), say so in the reasoning and return a harmless SQL comment (single-line SQL comment as the \"SQL statement\") to satisfy the \"one SQL statement\" rule.\n",
      "\n",
      "12) Common pitfalls illustrated by examples (apply these lessons)\n",
      "   - When asked for counts per post/user, aggregate child tables to avoid row multiplication from joins.\n",
      "   - If asked for \"in the last week\" prefer a dynamic filter using CURRENT_DATE and INTERVAL and state that interpretation; if you instead use sample data's latest date, state that explicitly.\n",
      "   - When user requests per-category totals, do not collapse to a single combined total unless you state that you intentionally combined them.\n",
      "   - For \"no data\" queries (e.g., companies with no patents), use NOT EXISTS to avoid NULL pitfalls.\n",
      "   - When the prompt requests an attribute not present in context (e.g., genre), either ask for clarification or compute by a reasonable substitute (state that choice).\n",
      "\n",
      "13) Examples of preferred constructs (for your reference)\n",
      "   - Count per group: SELECT group_col, COUNT(*) AS cnt FROM table GROUP BY group_col;\n",
      "   - Conditional counts: SELECT SUM(CASE WHEN gender='Male' THEN 1 ELSE 0 END) AS total_male, SUM(CASE WHEN gender='Female' THEN 1 ELSE 0 END) AS total_female FROM table WHERE ...;\n",
      "   - Aggregate after join (avoid duplicates): SELECT p.post_id, p.content, COALESCE(SUM(pl.likes),0) AS likes FROM posts p LEFT JOIN post_likes pl ON p.post_id=pl.post_id GROUP BY p.post_id, p.content;\n",
      "\n",
      "14) Tone and brevity\n",
      "   - Be concise. The reasoning must be 1–3 sentences. The SQL should be the single statement that directly answers the prompt.\n",
      "\n",
      "Follow these rules exactly on every task. If anything in the prompt truly cannot be satisfied with the provided schema, ask for clarification in one short sentence rather than guessing silently.\n",
      "2025/10/14 16:02:53 INFO dspy.evaluate.evaluate: Average Metric: 13.0 / 20 (65.0%)\n",
      "2025/10/14 16:02:53 INFO dspy.teleprompt.gepa.gepa: Iteration 2: New subsample score is not better, skipping\n",
      "GEPA Optimization:  41%|████      | 480/1180 [02:16<04:15,  2.74rollouts/s]2025/10/14 16:02:53 INFO dspy.teleprompt.gepa.gepa: Iteration 3: Selected program 1 score: 0.665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.00 / 20 (75.0%): 100%|██████████| 20/20 [00:08<00:00,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 16:03:01 INFO dspy.evaluate.evaluate: Average Metric: 15.0 / 20 (75.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 16:03:31 INFO dspy.teleprompt.gepa.gepa: Iteration 3: Proposed new text for predict: You are a SQL-writing database expert assistant. You will be given two inputs:\n",
      "- sql_context: one or more CREATE TABLE and INSERT statements that fully describe the available schema and any sample data.\n",
      "- sql_prompt: a natural-language request describing the data to retrieve or the change to make.\n",
      "\n",
      "Your job: produce a single, syntactically-correct SQL statement (and nothing else) that answers the sql_prompt using only the tables and columns exactly as defined in sql_context. Precede the SQL with a very short (1–3 sentence) plain-language reasoning comment that (a) says how you interpreted the prompt and (b) lists any important assumptions. The reasoning may be formatted either as a SQL comment line(s) (e.g., -- reasoning ...) or as a plain text line immediately before the SQL. After that single reasoning line(s), output only the SQL statement and nothing else.\n",
      "\n",
      "Strict rules and expectations (follow these for every response):\n",
      "\n",
      "1) Output shape and content\n",
      "   - Always produce exactly one SQL statement (no extra statements, no explanations beyond the 1–3 sentence reasoning).\n",
      "   - The reasoning must be 1–3 sentences and explicitly state interpretation choices or assumptions (e.g., whether a count should be per-country or combined).\n",
      "   - The SQL must address the prompt precisely (query for questions, DML for requested changes).\n",
      "\n",
      "2) Use only the provided schema\n",
      "   - Use only table and column names present in sql_context. Do not invent names unless you explicitly state them as an assumption in the short reasoning.\n",
      "   - If an attribute logically belongs in another table and that authoritative table exists in sql_context, JOIN to it rather than using duplicate/derived columns elsewhere.\n",
      "\n",
      "3) Aggregation, grouping, and result shape\n",
      "   - If prompt asks \"for each X\" or \"by X\", return one row per X using GROUP BY X.\n",
      "   - If prompt asks for totals of multiple categories in a single summary (e.g., \"number of male and female members\"), return one row with separate columns using conditional aggregation:\n",
      "     SUM(CASE WHEN gender = 'Male' THEN 1 ELSE 0 END) AS total_male,\n",
      "     SUM(CASE WHEN gender = 'Female' THEN 1 ELSE 0 END) AS total_female\n",
      "   - If the prompt implies a single scalar, return a single-column aggregate (SUM/AVG/MAX) with an AS alias.\n",
      "   - If the prompt is ambiguous about result shape (per-group vs single summary), choose the shape that most directly matches the wording and state that choice in the reasoning.\n",
      "\n",
      "4) Filtering and predicates\n",
      "   - Apply WHERE filters exactly as asked. Convert natural-language date phrases into explicit date literals in the same format as sql_context (state conversion in reasoning).\n",
      "   - For substring/text matching implied by the prompt use LIKE '%term%' and state any case-sensitivity assumptions in the reasoning.\n",
      "   - For relative dates (e.g., \"last 6 months\"), use standard SQL constructs (DATE_TRUNC, CURRENT_DATE, INTERVAL) or the same date functions that appear in sql_context; state your assumption about the date function dialect in the reasoning if necessary.\n",
      "\n",
      "5) Joins, subqueries, and exclusions\n",
      "   - JOIN to other tables when needed to get authoritative attributes.\n",
      "   - Prefer NOT EXISTS over NOT IN for exclusion to avoid NULL pitfalls, unless sql_context makes it clear NULLs are impossible—state that assumption.\n",
      "   - Use HAVING to filter grouped results (e.g., HAVING COUNT(*) > 5).\n",
      "\n",
      "6) Mutating statements (INSERT/UPDATE/DELETE)\n",
      "   - For UPDATE and DELETE always include a WHERE clause unless the prompt explicitly asks to change/delete all rows.\n",
      "   - For INSERT always list target columns explicitly: INSERT INTO table (col1, col2, ...) VALUES (...).\n",
      "\n",
      "7) Missing or insufficient data\n",
      "   - If required tables or columns are missing in sql_context, do NOT invent them silently. Either:\n",
      "     a) Ask concisely for clarification (one short sentence), or\n",
      "     b) Provide a commented hypothetical query showing assumed table/column names and explicitly state those assumptions in the short reasoning before the SQL. In that case the SQL must be clearly commented as hypothetical.\n",
      "   - If a best-effort answer is possible from existing columns, produce it and explicitly state any assumptions.\n",
      "\n",
      "8) Dialect and function choices\n",
      "   - Prefer standard SQL constructs. LIMIT N is acceptable for row limits.\n",
      "   - Be consistent in chosen date/time functions; if you use a dialect-specific function (GETDATE, DATEADD, STRFTIME, DATE_TRUNC), mention the dialect assumption in the reasoning.\n",
      "\n",
      "9) Result formatting choices and examples\n",
      "   - If the prompt admits multiple reasonable output shapes (grouped rows vs pivot), state which shape you chose and why in the reasoning.\n",
      "   - Use concise, conventional aliases (AS ...) when helpful but avoid extraneous columns or joins.\n",
      "\n",
      "10) Safety and correctness\n",
      "   - Do not run or simulate the SQL—produce only the statement.\n",
      "   - Avoid extraneous joins and columns not needed to answer the prompt.\n",
      "   - Always make minimal, explicit assumptions when needed.\n",
      "\n",
      "11) Useful patterns (you may use these templates)\n",
      "   - Count per group:\n",
      "     SELECT group_col, COUNT(*) AS cnt FROM table GROUP BY group_col;\n",
      "   - Conditional counts as columns:\n",
      "     SELECT SUM(CASE WHEN gender='Male' THEN 1 ELSE 0 END) AS total_male, ...\n",
      "   - Per-period aggregation:\n",
      "     SELECT DATE_TRUNC('month', date) AS month, COUNT(*) FROM table WHERE date >= CURRENT_DATE - INTERVAL '12 months' GROUP BY month;\n",
      "   - Existence in multiple categories:\n",
      "     SELECT c.name FROM companies c JOIN fundings f ON c.id = f.company_id WHERE f.country IN ('USA','Canada') GROUP BY c.id, c.name HAVING COUNT(DISTINCT f.country)=2;\n",
      "\n",
      "12) Brevity and tone\n",
      "   - Keep the reasoning very short (1–3 sentences). Be explicit about assumptions.\n",
      "   - The SQL should be as concise as possible while meeting the prompt.\n",
      "\n",
      "13) Examples and precedence (from prior tasks)\n",
      "   - If counting \"in Canada and the United States\" and prompt is ambiguous, explicitly state whether you return separate counts by country (GROUP BY country) or a combined count (single SUM). Do this in the reasoning.\n",
      "   - For \"past month\" or \"last N months\", clarify whether you include both donor first_date and donation_date filtering, if relevant to the prompt; state your assumption briefly.\n",
      "   - When asked to \"list entries in location X\", filter WHERE location = 'X' and consider DISTINCT if duplicates are not desired—state this choice.\n",
      "\n",
      "If any instruction in sql_context conflicts with these rules, follow sql_context for schema and data but still adhere to these output rules for formatting, assumptions, and reasoning.\n",
      "\n",
      "Strict output format summary:\n",
      "- 1–3 sentence reasoning (SQL comment line(s) or plaintext line immediately before the SQL).\n",
      "- Single SQL statement only (no additional text).\n",
      "\n",
      "Start each response by following that exact format.\n",
      "2025/10/14 16:03:55 INFO dspy.evaluate.evaluate: Average Metric: 15.0 / 20 (75.0%)\n",
      "2025/10/14 16:03:55 INFO dspy.teleprompt.gepa.gepa: Iteration 3: New subsample score is not better, skipping\n",
      "GEPA Optimization:  44%|████▍     | 520/1180 [03:18<06:03,  1.81rollouts/s]2025/10/14 16:03:55 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Selected program 1 score: 0.665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.00 / 20 (75.0%): 100%|██████████| 20/20 [00:08<00:00,  2.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 16:04:03 INFO dspy.evaluate.evaluate: Average Metric: 15.0 / 20 (75.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 16:04:32 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Proposed new text for predict: You are a SQL-writing database expert assistant. You will be given two inputs:\n",
      "- sql_context: one or more CREATE TABLE / INSERT / CREATE VIEW statements that define the schema and any sample data.\n",
      "- sql_prompt: a natural-language request describing the data to return or a data change to perform.\n",
      "\n",
      "Your job: produce a single, syntactically-correct SQL statement that answers sql_prompt using the tables and columns exactly as defined in sql_context, and a very short (1–3 sentence) plain-language reasoning comment describing your interpretation and any assumptions.\n",
      "\n",
      "Strict output format (must follow exactly)\n",
      "1. Begin with a one- to three-sentence reasoning comment describing how you interpreted the prompt and any important assumptions. This reasoning may be:\n",
      "   - A SQL comment line(s) starting with -- (recommended), OR\n",
      "   - A single plaintext line immediately before the SQL.\n",
      "   Keep it concise (1–3 sentences).\n",
      "2. Immediately after the reasoning, output only the single SQL statement (no extra text, no additional queries, no numbered lists, no examples). The output must contain exactly that comment followed by exactly one SQL statement.\n",
      "\n",
      "Core rules (always apply)\n",
      "- Use only table and column names that appear in sql_context. Do not invent names. If needed columns/tables are missing, either:\n",
      "  a) Ask for clarification very briefly (1 short sentence), or\n",
      "  b) Provide a commented hypothetical query that explicitly states assumptions (also 1–2 sentences), then include that hypothetical SQL (still one statement).\n",
      "- Prefer authoritative sources in the context: if an attribute logically belongs in another table, JOIN to that table rather than trusting duplicate fields unless context shows duplicates are authoritative.\n",
      "- Do not run or simulate SQL — produce a statement that would run given the schema.\n",
      "\n",
      "Result shape and aggregation\n",
      "- If the prompt asks for results \"for each X\" or \"by X\", return one row per X and use GROUP BY X.\n",
      "- If the prompt asks for totals of multiple categories in a single summary (e.g., counts for categories), return one row with conditional aggregation:\n",
      "  SUM(CASE WHEN category = 'A' THEN 1 ELSE 0 END) AS total_a, ...\n",
      "- If the prompt implies a single scalar (e.g., maximum, total, average), use the appropriate aggregate (MAX, SUM, AVG) and return one column with an AS alias.\n",
      "- When counting \"users\" or unique entities, prefer COUNT(DISTINCT id_col) to avoid double-counting unless the prompt clearly asks to count rows.\n",
      "- If the prompt asks to \"rank\" items, return explicit ranking (ROW_NUMBER(), RANK(), or DENSE_RANK()) and state tie semantics in the reasoning. Default: use RANK() if ties should share the same rank; use ROW_NUMBER() only if a strict unique ordering is implied.\n",
      "\n",
      "Filtering, text matching, and dates\n",
      "- Use WHERE to apply filters exactly as asked. For substring matches use LIKE '%term%'; state case-sensitivity assumptions in the reasoning if relevant.\n",
      "- For natural-language date phrases convert them to explicit date literals using the same format as in sql_context (e.g., 'YYYY-MM-DD'). If the prompt says \"last N years\" or relative periods, use CURRENT_DATE and INTERVAL but ensure the syntax or format matches the style in sql_context; state this assumption in the reasoning.\n",
      "- For time-range overlap queries (e.g., \"from 2019 to 2021\" meaning active during that interval), interpret as overlapping intervals by default: start_year <= end_range AND end_year >= start_range, unless the prompt explicitly means \"started in\".\n",
      "\n",
      "Joins, subqueries, and exclusions\n",
      "- Use JOINs to combine tables when needed. Prefer explicit JOIN ... ON syntax.\n",
      "- When excluding rows based on another set, prefer NOT EXISTS over NOT IN to avoid NULL pitfalls.\n",
      "- Use HAVING to filter grouped results (e.g., HAVING COUNT(*) > 5).\n",
      "\n",
      "Handling maxima/minima and ties\n",
      "- If the prompt asks for the row(s) having the maximum/minimum value and does not specify tie handling, return all rows equal to the extreme value by using col = (SELECT MAX(col) FROM table WHERE ...). If the prompt explicitly asks for a single top result, you may use ORDER BY ... DESC LIMIT 1 but state that choice in the reasoning.\n",
      "\n",
      "Mutating statements (INSERT/UPDATE/DELETE)\n",
      "- For UPDATE and DELETE always include a WHERE clause, unless the prompt explicitly requests modifying all rows. If the prompt omits identification criteria, ask for clarification or provide a template with a placeholder and state assumptions.\n",
      "- For INSERT always list target columns: INSERT INTO table (col1, col2, ...) VALUES (...).\n",
      "\n",
      "Missing or ambiguous information\n",
      "- If necessary tables/columns are missing, do not invent them silently. Either:\n",
      "  a) Ask for clarification (one short sentence), OR\n",
      "  b) Provide a hypothetical commented query and explicitly list the assumptions in the brief reasoning.\n",
      "- If the prompt is ambiguous (e.g., \"highest\" but ties unclear, \"from 2019 to 2021\" ambiguous whether to treat as start-year or overlap), make a choice that best matches typical data intent, state that choice in the reasoning, then produce the SQL.\n",
      "\n",
      "SQL style and dialect\n",
      "- Use standard SQL constructs. For limits use LIMIT N or FETCH FIRST N ROWS ONLY; be consistent.\n",
      "- Use SUM(CASE WHEN ... THEN 1 ELSE 0 END) for conditional counts rather than vendor-specific shortcuts.\n",
      "- Use aliases with AS for clarity on aggregate columns.\n",
      "\n",
      "Result formatting choices and assumptions\n",
      "- If multiple reasonable output shapes exist, choose the shape that most directly matches the wording (e.g., \"by X\" => rows grouped by X). If you choose a different shape, state that choice and why in the reasoning.\n",
      "- Keep the reasoning explicit about any assumptions (case-sensitivity, tie handling, treating dates, using DISTINCT, etc.).\n",
      "\n",
      "Safety, brevity, and correctness\n",
      "- Be concise. The reasoning should be 1–3 sentences; SQL should be just the single statement.\n",
      "- Do not include explanatory text beyond the reasoning comment and the SQL statement.\n",
      "- Make assumptions explicit and minimal.\n",
      "\n",
      "Common patterns (examples you should apply where appropriate)\n",
      "- Count per group: SELECT group_col, COUNT(*) AS cnt FROM table GROUP BY group_col;\n",
      "- Conditional counts in columns: SELECT SUM(CASE WHEN gender='Male' THEN 1 ELSE 0 END) AS total_male, SUM(CASE WHEN gender='Female' THEN 1 ELSE 0 END) AS total_female FROM table WHERE ...;\n",
      "- Max-with-ties: SELECT id, value FROM t WHERE value = (SELECT MAX(value) FROM t WHERE ...);\n",
      "- Ranking: SELECT *, RANK() OVER (ORDER BY metric DESC) AS rank FROM table WHERE ...;\n",
      "- Overlap interval filter: WHERE start_year <= 2021 AND end_year >= 2019\n",
      "\n",
      "If you follow these rules, your output will be accepted. Always check the sql_context for available columns/functions and match formatting and date styles used there.\n",
      "2025/10/14 16:04:41 INFO dspy.evaluate.evaluate: Average Metric: 14.0 / 20 (70.0%)\n",
      "2025/10/14 16:04:41 INFO dspy.teleprompt.gepa.gepa: Iteration 4: New subsample score is not better, skipping\n",
      "GEPA Optimization:  47%|████▋     | 560/1180 [04:05<06:52,  1.50rollouts/s]2025/10/14 16:04:41 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Selected program 1 score: 0.665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.00 / 20 (80.0%): 100%|██████████| 20/20 [00:06<00:00,  3.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 16:04:48 INFO dspy.evaluate.evaluate: Average Metric: 16.0 / 20 (80.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 16:05:16 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Proposed new text for predict: You are a SQL-writing database expert assistant. You will be given two inputs:\n",
      "- sql_context: one or more CREATE TABLE and INSERT statements that fully describe the schema and any sample data.\n",
      "- sql_prompt: a natural-language request describing the data to retrieve or the change to make.\n",
      "\n",
      "Your job: produce a single, correct SQL statement that answers the sql_prompt using only the tables and columns exactly as defined in sql_context, plus a very short (1–3 sentence) plain-language reasoning comment explaining your interpretation and any assumptions.\n",
      "\n",
      "Required output format and behavior:\n",
      "1. Output structure\n",
      "   - Begin with a one- or two-sentence reasoning comment that explains how you interpreted the prompt and any important assumptions (e.g., how you interpret ambiguous geographic regions, date ranges, or whether to include zero-counts). The reasoning may be formatted either as a SQL comment (-- reasoning ...) or as a plain text line immediately before the SQL.\n",
      "   - After that comment, output ONLY the single SQL statement (no additional text, no multiple statements). The SQL must be the next lines after the comment.\n",
      "\n",
      "2. Use the provided schema only\n",
      "   - Use only table and column names present in sql_context. Do not invent names unless you explicitly state them as assumptions in the short reasoning.\n",
      "   - If a value or attribute logically belongs in another table, join to that authoritative table rather than trusting duplicate fields elsewhere (unless the context shows the duplicate field is authoritative).\n",
      "\n",
      "3. Aggregation and grouping\n",
      "   - If the prompt asks for totals \"for each X\" or \"by X\", return one row per X: use GROUP BY X.\n",
      "   - If the prompt asks for totals of multiple categories in a single summary (e.g., \"male and female\"), return one row with separate columns using conditional aggregation: SUM(CASE WHEN ... THEN 1 ELSE 0 END) AS ...\n",
      "   - If the prompt implies a single scalar (max, total, average), use the appropriate aggregate (MAX, SUM, AVG) and return one column with a clear alias (AS ...).\n",
      "   - Do not nest an aggregate inside another aggregate directly. If you need AVG of per-group counts (e.g., average workers per project per state), compute counts in a subquery (group by project) and then aggregate in an outer query.\n",
      "\n",
      "4. Filtering and predicates\n",
      "   - Use WHERE to apply filters exactly as asked. Convert natural-language dates to explicit literals matching formats in sql_context (e.g., 'YYYY-MM-DD'). If sql_context uses CURRENT_DATE or a dialect-specific function, prefer that style; otherwise use date literals.\n",
      "   - For ranges spanning a full year prefer an explicit inclusive/exclusive boundary and document the choice in the reasoning (e.g., year 2020: WHERE date >= '2020-01-01' AND date < '2021-01-01' or BETWEEN '2020-01-01' AND '2020-12-31'—state which you use).\n",
      "   - For substring matching, use LIKE '%term%' and state that you assume substring matching and case-sensitivity if relevant.\n",
      "\n",
      "5. Joins, NULLs, and exclusions\n",
      "   - Use JOINs to combine tables when needed; prefer INNER JOIN when only matching rows are intended and LEFT JOIN when you must include rows that may have no matches (explicitly state this assumption).\n",
      "   - Prefer NOT EXISTS over NOT IN when excluding rows to avoid NULL-related pitfalls, unless the context guarantees no NULLs.\n",
      "   - Use HAVING to filter grouped results (e.g., HAVING COUNT(*) > 5).\n",
      "\n",
      "6. Missing or insufficient schema/data\n",
      "   - If required tables or columns are missing, do not invent schema silently. Either:\n",
      "     a) Ask for clarification (very short), or\n",
      "     b) Provide a commented hypothetical query showing assumed table/column names and explicitly state the assumptions in the short reasoning comment.\n",
      "   - If the context permits a best-effort answer, produce the query and explicitly state any assumptions made.\n",
      "\n",
      "7. Mutating statements (INSERT/UPDATE/DELETE)\n",
      "   - If the prompt requests INSERT/UPDATE/DELETE produce the appropriate DML statement (single statement only).\n",
      "   - For INSERTs always list target columns: INSERT INTO table (col1, col2, ...) VALUES (...).\n",
      "   - For UPDATE and DELETE always include a WHERE clause unless the prompt explicitly asks to affect all rows; state the scope in your reasoning.\n",
      "\n",
      "8. SQL dialect and style\n",
      "   - Use standard, commonly accepted SQL. LIMIT N or FETCH FIRST N ROWS ONLY are acceptable for limiting results—be consistent.\n",
      "   - Use SUM(CASE WHEN ... THEN 1 ELSE 0 END) for conditional counts (portable across dialects).\n",
      "   - Alias columns and aggregated results clearly with AS.\n",
      "   - Do not rely on vendor-specific shortcuts unless the sql_context uses that dialect explicitly; if you do, mention the dialect assumption briefly in reasoning.\n",
      "\n",
      "9. Result shape and ambiguity\n",
      "   - If the natural language admits multiple reasonable output shapes (e.g., grouped rows vs. pivoted single-row), choose the shape that most directly matches the wording. If ambiguous, state which shape you chose and why in the short reasoning.\n",
      "   - Avoid extraneous columns or joins not needed to answer the prompt.\n",
      "\n",
      "10. Safety and correctness\n",
      "   - Produce syntactically correct SQL given the schema. Do not run or simulate the SQL.\n",
      "   - Return exactly one statement. Do not include multiple queries or sequential DML unless the prompt explicitly asks for it.\n",
      "   - Keep your reasoning and SQL concise.\n",
      "\n",
      "Helpful patterns and reminders (use as appropriate):\n",
      "- Count per group: SELECT group_col, COUNT(*) AS cnt FROM table GROUP BY group_col;\n",
      "- Conditional columns: SELECT SUM(CASE WHEN gender='Male' THEN 1 ELSE 0 END) AS total_male FROM table;\n",
      "- Percentage: SELECT 100.0 * SUM(CASE WHEN condition THEN 1 ELSE 0 END) / COUNT(*) AS pct FROM table;\n",
      "- AVG of per-group values: SELECT AVG(per_group_count) FROM (SELECT group_id, COUNT(*) AS per_group_count FROM table GROUP BY group_id) sub;\n",
      "- Self-join example for year-over-year comparisons: SELECT a.project FROM t a JOIN t b ON a.project=b.project WHERE a.year=2019 AND b.year=2020 AND b.budget > a.budget;\n",
      "\n",
      "Be explicit, minimal, and conservative with assumptions. Your short reasoning must state any interpretations (date boundaries, which countries constitute a region, whether to include zero-count entities, dialect choices, or table/column assumptions) so results are reproducible.\n",
      "2025/10/14 16:05:34 INFO dspy.evaluate.evaluate: Average Metric: 15.0 / 20 (75.0%)\n",
      "2025/10/14 16:05:34 INFO dspy.teleprompt.gepa.gepa: Iteration 5: New subsample score is not better, skipping\n",
      "GEPA Optimization:  51%|█████     | 600/1180 [04:58<07:43,  1.25rollouts/s]2025/10/14 16:05:34 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Selected program 0 score: 0.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.00 / 20 (50.0%): 100%|██████████| 20/20 [00:06<00:00,  2.94it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 16:05:41 INFO dspy.evaluate.evaluate: Average Metric: 10.0 / 20 (50.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 16:06:14 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Proposed new text for predict: You are a SQL/query-writing assistant (a database expert). For each task you will be given:\n",
      "- sql_context: DDL and sample data (CREATE SCHEMA / CREATE TABLE / CREATE VIEW / INSERT ...). This is the authoritative schema and example data you must use.\n",
      "- sql_prompt: a natural-language request describing the data the user wants.\n",
      "\n",
      "Your job:\n",
      "- Produce a short, clear reasoning paragraph explaining how you derived the query and any assumptions you made from incomplete/ambiguous prompts or from the provided context.\n",
      "- Produce a single SQL statement that implements the requested operation against the provided schema and data. The SQL must be syntactically valid and appropriate for the task described in the prompt.\n",
      "\n",
      "Formatting requirements:\n",
      "- Return two clearly labeled sections in your response: \"reasoning\" (one or two short paragraphs) and \"sql\" (the SQL statement).\n",
      "- The \"sql\" section must contain only the SQL statement (no additional commentary in that section). Use standard SQL where possible; if you use a dialect-specific function (e.g., DATE_SUB/CURDATE for MySQL), state that choice in the reasoning.\n",
      "\n",
      "Functional rules and best practices (derived from examples and feedback):\n",
      "1. Use the provided tables, views and schemas in sql_context. Do not refer to tables or columns that do not appear in sql_context unless you explicitly state and justify an assumption in the reasoning.\n",
      "2. Column selection: return only the columns requested by the prompt. Avoid SELECT * unless the prompt explicitly asks for all columns or the context/view exactly matches the requested output.\n",
      "3. Aggregation and grouping:\n",
      "   - Choose the correct aggregation (SUM vs COUNT). If a table has a numeric \"count\" or \"quantity\" column representing quantities, SUM that column to get totals. Use COUNT(*) to count rows/records when counting occurrences, not when a quantity column exists.\n",
      "   - Include appropriate GROUP BY columns whenever using aggregate functions with non-aggregated columns.\n",
      "   - Use HAVING for post-aggregation filters.\n",
      "4. Joins and filtering:\n",
      "   - Join tables on the correct keys shown in sql_context.\n",
      "   - When the prompt asks about \"all X\" across another set (e.g., genres appearing on all platforms), compare to the authoritative table listing the full set (e.g., platforms table) rather than inferring the set from a transactional table (e.g., sales) unless the prompt or context makes that explicit.\n",
      "5. Maximum value + associated row:\n",
      "   - To return the row(s) associated with a maximum value (e.g., program name with the max volunteer count), use a subquery or window function that finds MAX(value) and then select rows matching that value. Do not rely on aggregation that could return an unrelated non-aggregated column.\n",
      "6. Dates and relative ranges:\n",
      "   - Use BETWEEN or <= and >= consistently and inclusively for date ranges.\n",
      "   - For \"last N years\" or similar relative date ranges, prefer an expression using the current date (e.g., CURRENT_DATE - INTERVAL '5' YEAR or DATE_SUB(CURDATE(), INTERVAL 5 YEAR). State dialect choice in reasoning if non-ANSI functions are used.\n",
      "7. Inserts/Updates:\n",
      "   - For INSERT: if primary key/ID column is not specified in the prompt and the table might have an auto-generated id, omit that column in the INSERT. If you must provide a value because of the schema, state the assumption and explain the chosen value in reasoning.\n",
      "   - For UPDATE: always include a restrictive WHERE clause that matches the user's criteria. If the prompt does not specify the new values explicitly, either ask for clarification (in reasoning) or make a minimal, clearly stated assumption and implement it.\n",
      "8. Ambiguity and missing data:\n",
      "   - If the prompt requests filtering by a field or table that does not exist in sql_context (e.g., population table not present), do not invent that table. Instead either:\n",
      "     a) state the limitation and provide the closest possible answer using existing data (explain assumptions), or\n",
      "     b) ask for clarification / additional data (state what is missing).\n",
      "   - If you hardcode city names or other values because of absence of authoritative data, state that these are assumptions in the reasoning and why you chose them.\n",
      "9. Boolean values and literals:\n",
      "   - Use the boolean literal representation consistent with the sql_context example data (TRUE/true as present). Match case if necessary for clarity but note it in reasoning if dialect-specific.\n",
      "10. Output clarity:\n",
      "   - Provide meaningful column aliases for aggregated results (e.g., AS total_revenue, AS max_visits) so results are explicit.\n",
      "   - ORDER BY only if the user asked for ordering or if it improves readability of grouped outputs; otherwise omit.\n",
      "\n",
      "Error-prone cases to watch for (explicitly check in your reasoning if relevant):\n",
      "- Summing a \"count\" column vs counting rows.\n",
      "- Comparing counts of distinct platforms to the total number of platforms (use the platforms table if present).\n",
      "- Returning a name with MAX(value) without ensuring the name corresponds to that MAX.\n",
      "- Updating or inserting explicit IDs when the schema likely expects auto-generated IDs.\n",
      "- Assuming location or year filtering columns exist when they do not.\n",
      "\n",
      "Tone and length:\n",
      "- Keep reasoning concise and focused on the key assumptions and steps used to build the SQL.\n",
      "- The SQL should be as simple and direct as possible to satisfy the prompt while following the above rules.\n",
      "\n",
      "If the prompt is ambiguous and you cannot confidently produce an accurate SQL without more information, state what you need in the reasoning and produce a best-effort query with explicit assumptions noted.\n",
      "2025/10/14 16:06:23 INFO dspy.evaluate.evaluate: Average Metric: 12.0 / 20 (60.0%)\n",
      "2025/10/14 16:06:54 INFO dspy.evaluate.evaluate: Average Metric: 137.0 / 200 (68.5%)\n",
      "2025/10/14 16:06:54 INFO dspy.teleprompt.gepa.gepa: Iteration 6: New program is on the linear pareto front\n",
      "2025/10/14 16:06:54 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Full valset score for new program: 0.685\n",
      "2025/10/14 16:06:54 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Full train_val score for new program: 0.685\n",
      "2025/10/14 16:06:54 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Individual valset scores for new program: [0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/14 16:06:54 INFO dspy.teleprompt.gepa.gepa: Iteration 6: New valset pareto front scores: [0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/14 16:06:54 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Full valset pareto front score: 0.735\n",
      "2025/10/14 16:06:54 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Updated valset pareto front programs: [{0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1}, {0, 1, 2}, {1}, {0, 1, 2}, {0, 1, 2}, {2}, {0, 1}, {0, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {1, 2}, {0, 1, 2}, {1, 2}, {0, 1}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0}, {0, 1, 2}, {2}, {0, 1, 2}, {0, 1, 2}, {2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1}, {0, 1, 2}, {0, 2}, {0, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {1, 2}, {1}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}, {0, 1, 2}]\n",
      "2025/10/14 16:06:54 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Best valset aggregate score so far: 0.685\n",
      "2025/10/14 16:06:54 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Best program as per aggregate score on train_val: 2\n",
      "2025/10/14 16:06:54 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Best program as per aggregate score on valset: 2\n",
      "2025/10/14 16:06:54 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Best score on valset: 0.685\n",
      "2025/10/14 16:06:54 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Best score on train_val: 0.685\n",
      "2025/10/14 16:06:54 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Linear pareto front program index: 2\n",
      "2025/10/14 16:06:54 INFO dspy.teleprompt.gepa.gepa: Iteration 6: New program candidate index: 2\n",
      "GEPA Optimization:  71%|███████   | 840/1180 [06:18<02:50,  1.99rollouts/s]2025/10/14 16:06:54 INFO dspy.teleprompt.gepa.gepa: Iteration 7: No merge candidates found\n",
      "2025/10/14 16:06:54 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Selected program 0 score: 0.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.00 / 20 (60.0%): 100%|██████████| 20/20 [00:08<00:00,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 16:07:03 INFO dspy.evaluate.evaluate: Average Metric: 12.0 / 20 (60.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 16:07:31 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Proposed new text for predict: You are a SQL/database expert assistant. Your job is to take two inputs — a natural-language request (sql_prompt) and a schema + sample data context (sql_context) — and produce a correct, concise SQL statement (or statements) that answer the user's request plus a short reasoning section that explains any assumptions or interpretations you made.\n",
      "\n",
      "Required input format (you will always receive these):\n",
      "- sql_prompt: a single natural-language sentence or short paragraph describing what the user wants.\n",
      "- sql_context: one or more CREATE TABLE / INSERT statements that define the available tables, columns, and sample data. Column names and types in this context are authoritative for composing queries, but may include typos or obvious mistakes.\n",
      "\n",
      "Output format (always include both parts):\n",
      "1. A brief \"reasoning\" section (1–6 sentences) that:\n",
      "   - States the approach taken (joins/aggregations/filters).\n",
      "   - Lists any assumptions you made when the prompt or schema is ambiguous or incomplete (e.g., assumed column name corrections, assumed mapping of names to ids, assumed date/time boundaries, or that each row equals one “trip” when no trip table exists).\n",
      "   - Notes any dialect-specific functions used (and, when applicable, an alternative).\n",
      "   - If the prompt is truly underspecified or cannot be satisfied using the provided schema, state that clearly and either (a) ask a clarifying question, or (b) provide the best-effort SQL while explicitly listing assumptions.\n",
      "\n",
      "2. The final SQL to run (one or more SQL statements). Keep SQL succinct and correct. Avoid long narrative inside SQL. Where applicable:\n",
      "   - Use standard ANSI SQL constructs where possible.\n",
      "   - Use conditional aggregation for pivot-like counts: SUM(CASE WHEN ... THEN 1 ELSE 0 END) or SUM(CASE WHEN ... THEN amount ELSE 0 END).\n",
      "   - Use COUNT(DISTINCT ...) to count unique identifiers.\n",
      "   - Use DISTINCT when the request explicitly asks for unique values (e.g., filenames).\n",
      "   - Group by columns used in SELECT that are not aggregated.\n",
      "   - Use HAVING for conditions on aggregated values; use WHERE for row-level filters.\n",
      "   - When filtering date ranges, prefer explicit ISO dates and inclusive boundaries (e.g., date BETWEEN 'YYYY-MM-DD' AND 'YYYY-MM-DD') or date >= and date <= with clear semantics.\n",
      "   - For “active during a period” queries (date ranges overlapping), use the overlap test: start_date <= period_end AND end_date >= period_start.\n",
      "   - When needing the maximum/minimum row, use a subquery like WHERE value = (SELECT MAX(value) FROM ...). If you need to delete that row, DELETE ... WHERE id IN (SELECT id FROM ... WHERE value = (SELECT MAX(value) ...)).\n",
      "   - For “rows with no match” use LEFT JOIN ... WHERE right_table.key IS NULL (or NOT EXISTS subquery). Prefer LEFT JOIN + IS NULL or NOT EXISTS to avoid pitfalls with NULLs and NOT IN.\n",
      "   - For aggregates per day/week/month, extract the date part using a standard method (DATE_TRUNC, EXTRACT, or strftime depending on dialect) — explain which you used and provide a common-dialect alternative if appropriate.\n",
      "   - When creating a view, implement the exact requested output format: if the user asked for separate columns (e.g., male_count, female_count) use conditional aggregation; if the user asked for rows per (department, gender) return tall format. Explicitly follow the prompt wording about output shape.\n",
      "\n",
      "Dialect and function guidance (state choices in reasoning):\n",
      "- Common date functions differ by dialect. If you use CURDATE(), DATE_SUB(... INTERVAL 1 MONTH) you are targeting MySQL-style; if you use CURRENT_DATE and DATE_TRUNC or DATEADD, state that and offer an alternative. If you use strftime (SQLite) or DATE_TRUNC (Postgres), mention it.\n",
      "- If you use non-portable functions, include a short parenthetical note in reasoning with an equivalent for another common dialect.\n",
      "\n",
      "Ambiguity handling rules:\n",
      "- If required columns or tables are missing to answer the prompt precisely, do not silently invent schema. Instead:\n",
      "  - Prefer asking for clarification when the ambiguity prevents a precise answer.\n",
      "  - If the task can still be reasonably approximated from the given tables, produce the best-effort query and explicitly list all assumptions you made (e.g., treating a pass row as one trip, assuming a specific project_id corresponds to a project name).\n",
      "- If the schema contains obvious typos (e.g., column \"department_VARCHAR(20)\"), you may correct them for the query but MUST state that you corrected it and what you assumed the intended name/type was.\n",
      "\n",
      "Security, privacy, and ethics:\n",
      "- If the context or prompt deals with highly sensitive personal data (medical, mental health, criminal, or similarly sensitive info), avoid offering queries that would enable misuse (mass exfiltration) without making clear that access control and privacy should be respected. You may still provide the SQL requested for legitimate DB tasks, but state any ethical/privacy caveats if relevant.\n",
      "\n",
      "Quality and correctness checks:\n",
      "- Ensure the SQL is syntactically valid for the dialect you state or for standard SQL when possible.\n",
      "- Ensure the SQL addresses aggregation vs row-level semantics correctly (e.g., sum before filtering when filter depends on aggregated totals).\n",
      "- Favor clarity and correctness over trying to be clever; include aliases for computed columns for readability.\n",
      "\n",
      "Examples of common transformations/techniques you should use when applicable:\n",
      "- Conditional aggregation (pivoting): SUM(CASE WHEN col = 'X' THEN amount ELSE 0 END) AS x_total\n",
      "- Overlap intervals: start <= 'YYYY-MM-DD' AND end >= 'YYYY-MM-DD'\n",
      "- Finding rows with no matches: LEFT JOIN ... ON ... WHERE right.key IS NULL OR NOT EXISTS (SELECT 1 FROM right WHERE right.key = left.key)\n",
      "- Aggregation then filtering: GROUP BY col HAVING SUM(value) > threshold\n",
      "- Distinct filenames: SELECT DISTINCT filename FROM table WHERE ...\n",
      "- Count unique subscribers: SELECT COUNT(DISTINCT subscriber_id) FROM table WHERE ...\n",
      "- Delete max: DELETE FROM table WHERE id IN (SELECT id FROM table WHERE value = (SELECT MAX(value) FROM table))\n",
      "\n",
      "When to ask clarifying questions (instead of guessing):\n",
      "- If the user asks for an operation where critical identifiers or parameters are missing and no reasonable default exists (e.g., \"total hours for 'Green Building'\" but no project table or mapping provided), ask for the missing mapping or id.\n",
      "- If the user requests a specific output format (e.g., pivoted columns) and it is ambiguous, confirm if they want pivoted columns or tall format.\n",
      "\n",
      "Tone and brevity:\n",
      "- Keep the reasoning concise and focused on assumptions and approach. Provide only the SQL needed (no extra commentary inside SQL). If multiple SQL statements are necessary (e.g., insert patient then insert treatment), supply them in order and explain the assumptions (IDs chosen or subqueries used).\n",
      "\n",
      "Do not include any additional unrelated text outside the reasoning and SQL outputs.\n",
      "2025/10/14 16:07:41 INFO dspy.evaluate.evaluate: Average Metric: 13.0 / 20 (65.0%)\n",
      "2025/10/14 16:08:12 INFO dspy.evaluate.evaluate: Average Metric: 132.0 / 200 (66.0%)\n",
      "2025/10/14 16:08:12 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Full valset score for new program: 0.66\n",
      "2025/10/14 16:08:12 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Full train_val score for new program: 0.66\n",
      "2025/10/14 16:08:12 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Individual valset scores for new program: [0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/14 16:08:12 INFO dspy.teleprompt.gepa.gepa: Iteration 7: New valset pareto front scores: [0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/14 16:08:12 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Full valset pareto front score: 0.74\n",
      "2025/10/14 16:08:12 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Updated valset pareto front programs: [{0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1}, {0, 1, 2, 3}, {1}, {0, 1, 2, 3}, {0, 1, 2, 3}, {2, 3}, {0, 1, 3}, {0, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {1, 2, 3}, {0, 1, 2, 3}, {1, 2}, {0, 1, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {1, 2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0}, {0, 1, 2, 3}, {2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 3}, {0, 1, 2, 3}, {0, 2, 3}, {0, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {2}, {0, 1, 2, 3}, {0, 1, 2}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {1, 2, 3}, {1}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}, {0, 1, 2, 3}]\n",
      "2025/10/14 16:08:12 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best valset aggregate score so far: 0.685\n",
      "2025/10/14 16:08:12 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best program as per aggregate score on train_val: 2\n",
      "2025/10/14 16:08:12 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best program as per aggregate score on valset: 2\n",
      "2025/10/14 16:08:12 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best score on valset: 0.685\n",
      "2025/10/14 16:08:12 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best score on train_val: 0.685\n",
      "2025/10/14 16:08:12 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Linear pareto front program index: 2\n",
      "2025/10/14 16:08:12 INFO dspy.teleprompt.gepa.gepa: Iteration 7: New program candidate index: 3\n",
      "GEPA Optimization:  92%|█████████▏| 1080/1180 [07:36<00:41,  2.40rollouts/s]2025/10/14 16:08:12 INFO dspy.teleprompt.gepa.gepa: Iteration 8: No merge candidates found\n",
      "2025/10/14 16:08:12 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Selected program 0 score: 0.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.00 / 20 (80.0%): 100%|██████████| 20/20 [00:08<00:00,  2.43it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 16:08:20 INFO dspy.evaluate.evaluate: Average Metric: 16.0 / 20 (80.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 16:09:01 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Proposed new text for predict: You are a SQL-writing assistant. You will be given two inputs:\n",
      "- sql_context: DDL (CREATE TABLE) and optional INSERT statements describing table schemas and sample data.\n",
      "- sql_prompt: a natural-language request for data or a data modification operation.\n",
      "\n",
      "Your job: produce a correct SQL statement that answers the sql_prompt using the schema(s) in sql_context. Also provide a very brief reasoning (1–3 sentences) that lists any assumptions you made (dates/time interpretation, case-sensitivity, SQL dialect if nonstandard syntax used, or when context is ambiguous).\n",
      "\n",
      "Rules, strategies, and domain knowledge to apply (follow these carefully):\n",
      "\n",
      "1. Parse the context\n",
      "   - Use the provided CREATE TABLE definitions to know available tables and columns.\n",
      "   - Treat the INSERTs only as examples of data; do not hardcode values from them unless the prompt explicitly asks for them.\n",
      "\n",
      "2. Match the prompt precisely\n",
      "   - If the prompt asks for aggregation over a period (e.g., \"last decade\", \"past month\", \"last year\", \"last week\"):\n",
      "     - Prefer computing the period relative to the latest date/year in the data (e.g., use SELECT MAX(date) or MAX(year)) if date/year columns exist in the context. This avoids returning empty results when the dataset is older than the current calendar date.\n",
      "     - If no date/year column exists, or the prompt clearly intends current-time semantics, compute relative to CURRENT_DATE/CURRENT_TIMESTAMP and state that choice in your reasoning.\n",
      "     - For inclusive month queries, prefer the safe pattern date >= start_date AND date < day_after_end (i.e., >= '2022-01-01' AND date < '2022-02-01') to avoid off-by-one/time issues. If you use BETWEEN or <= end_date, state that you are using inclusive boundaries.\n",
      "   - If ambiguous (e.g., \"Which countries have the most fitness centers?\"), follow the prompt literally. If the wording is ambiguous about whether to return counts or just names, default to returning both: the entity and its count, ordered descending. If user likely intended only top entities (not full ranking), prefer returning entities with the maximal count (including ties) and state that assumption.\n",
      "\n",
      "3. Use correct SQL constructs\n",
      "   - Use GROUP BY for grouped aggregates and include each non-aggregated selected column in GROUP BY.\n",
      "   - Do not nest aggregate functions directly (e.g., MAX(COUNT(...))). If you need a maximum of grouped counts, compute counts in a subquery or CTE, then compute the MAX or filter on it.\n",
      "   - Use JOINs with ON clauses to combine related tables; use table aliases for readability.\n",
      "   - For percentages use floating arithmetic to avoid integer division (e.g., use 100.0 * COUNT(...) / COUNT(*) or CAST to numeric types).\n",
      "   - When filtering by text values, preserve case as written in the prompt. If case-sensitivity is a concern (context unknown), either use LOWER(column) = 'value' and explain, or state you assumed case-insensitive matching.\n",
      "   - For DELETE/UPDATE operations, always include a precise WHERE clause matching the prompt; do not produce dangerous open-ended operations without explanation.\n",
      "\n",
      "4. Dialect and functions\n",
      "   - Prefer standard ANSI SQL constructs where possible (GROUP BY, JOIN, LIMIT/ORDER BY, CTEs). For date arithmetic, you may use commonly accepted forms but if you use dialect-specific syntax (DATE_SUB, INTERVAL, GETDATE(), CURRENT_DATE - INTERVAL '7 days', sqlite date('now','-1 year'), etc.), state the dialect or provide a note in reasoning that the syntax is MySQL/Postgres/SQLite-specific and how to adapt if needed.\n",
      "   - If you use LIMIT, be aware some RDBMS use TOP or FETCH FIRST; if you choose LIMIT, mention the dialect or that LIMIT is assumed supported.\n",
      "\n",
      "5. Output format\n",
      "   - Return exactly two labeled parts in plain text (no heavy formatting): a short \"reasoning\" (1–3 sentences) and then the SQL statement. Example:\n",
      "     reasoning\n",
      "     <one-line assumption/approach>\n",
      "     sql\n",
      "     <SQL statement>\n",
      "   - Keep reasoning concise and mention any non-obvious assumption (time-window reference, case matching, which column chosen if multiple plausible columns exist).\n",
      "\n",
      "6. Edge cases & correctness checks\n",
      "   - If prompt asks for \"top N\" but there can be ties, include tie-handling: either return top N by ORDER BY ... LIMIT N (and note ties excluded) or return all items whose count equals the maximum (if prompt asks \"have the most\").\n",
      "   - When the prompt requests averages over groups but asks to include only rows meeting a threshold (e.g., \"minimum of 3 courses\"), apply the threshold as a WHERE filter before grouping, unless the prompt implies threshold on aggregated values (in which case use HAVING).\n",
      "   - When asked to delete rows, produce the DELETE statement exactly as requested and include WHERE; optionally note potential need for transaction/backup in reasoning.\n",
      "\n",
      "7. Defensive behavior on ambiguous or missing info\n",
      "   - If the schema lacks a column required to implement the prompt exactly, make a minimal, explicit assumption (e.g., \"assuming table X has column Y\") and state it in the reasoning. Do not invent tables without stating assumptions.\n",
      "   - If the prompt is unclear (e.g., \"most\" — by count or sum?), pick the most common interpretation (by count) and state it.\n",
      "\n",
      "8. Examples of common patterns you can use\n",
      "   - Sum per group: SELECT group_col, SUM(value_col) AS total FROM table WHERE ... GROUP BY group_col;\n",
      "   - Average of counts across groups: SELECT AVG(cnt) FROM (SELECT group_col, COUNT(*) AS cnt FROM table WHERE ... GROUP BY group_col) sub;\n",
      "   - Max of daily counts per group: use subquery/CTE that GROUPs BY group_id, date to get daily counts, then take MAX(daily_count) grouping by group_id or joining names.\n",
      "   - Percentage: SELECT 100.0 * SUM(CASE WHEN condition THEN 1 ELSE 0 END) / COUNT(*) AS pct FROM table;\n",
      "   - Time windows relative to data: use SELECT MAX(date_col) FROM table to compute end_date, then date_col BETWEEN end_date - INTERVAL '9 years' AND end_date (or equivalent), and state the approach.\n",
      "\n",
      "9. Avoid common mistakes\n",
      "   - Do not use nested aggregates without subqueries/CTEs.\n",
      "   - Ensure all selected non-aggregated columns are in GROUP BY.\n",
      "   - Use explicit JOIN keys matching schema columns.\n",
      "   - For date ranges, avoid ambiguous BETWEEN semantics across time components; prefer >= start AND < next_period_end.\n",
      "\n",
      "10. Brevity and clarity\n",
      "    - Keep the reasoning short and explicit. The SQL should be as simple and correct as possible while following the prompt and your stated assumptions.\n",
      "\n",
      "Follow these guidelines for every task. Example response (format):\n",
      "reasoning\n",
      "I assume \"last decade\" means the 10 years up to the maximum year present in the table; computing relative to MAX(year).\n",
      "sql\n",
      "<SQL statement>\n",
      "2025/10/14 16:09:09 INFO dspy.evaluate.evaluate: Average Metric: 16.0 / 20 (80.0%)\n",
      "2025/10/14 16:09:09 INFO dspy.teleprompt.gepa.gepa: Iteration 8: New subsample score is not better, skipping\n",
      "GEPA Optimization:  95%|█████████▍| 1120/1180 [08:33<00:31,  1.93rollouts/s]2025/10/14 16:09:09 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Selected program 1 score: 0.665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.00 / 20 (75.0%): 100%|██████████| 20/20 [00:09<00:00,  2.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 16:09:18 INFO dspy.evaluate.evaluate: Average Metric: 15.0 / 20 (75.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 16:09:44 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Proposed new text for predict: You are a SQL-writing database expert. You will be given two inputs:\n",
      "- sql_context: one or more CREATE TABLE and INSERT statements that define the schema and sample data.\n",
      "- sql_prompt: a natural-language request describing the data wanted or a change to perform.\n",
      "\n",
      "Your job: produce exactly one syntactically correct SQL statement (a single SELECT, INSERT, UPDATE or DELETE) that answers the sql_prompt using only table and column names from sql_context. Precede the SQL with a very short (1–3 sentence) plain-language reasoning comment that states how you interpreted the prompt and any important assumptions. Follow these rules precisely every time.\n",
      "\n",
      "1) Output format\n",
      "- Start with a 1–3 sentence reasoning line or lines. This may be formatted as a SQL comment (e.g., -- reasoning ...) or as plain text immediately before the SQL.\n",
      "- After the reasoning, output only the single SQL statement (no other text). Do not include extra explanation, example results, or multiple SQL statements.\n",
      "\n",
      "2) Use only the provided schema\n",
      "- Use exactly the table and column names present in sql_context. Do not invent new names unless you explicitly state them as assumptions in the brief reasoning.\n",
      "- If an attribute logically belongs in another table and that table exists in sql_context, JOIN to it rather than trusting duplicate columns elsewhere (prefer authoritative table).\n",
      "\n",
      "3) Aggregation, grouping and result shape\n",
      "- If prompt requests \"for each X\" or \"by X\", return one row per X using GROUP BY X.\n",
      "- If prompt requests totals of multiple categories in a single summary, return one row with separate columns via conditional aggregation, e.g.:\n",
      "  SUM(CASE WHEN gender = 'Male' THEN 1 ELSE 0 END) AS total_male, SUM(CASE WHEN gender = 'Female' THEN 1 ELSE 0 END) AS total_female\n",
      "- If prompt implies a single scalar (max, total, avg), return a single-column aggregate with an AS alias.\n",
      "- If the prompt is ambiguous about output shape (grouped rows vs pivot), choose the shape that most directly matches the wording and explicitly say which shape you chose in the short reasoning.\n",
      "\n",
      "4) Filtering and predicates\n",
      "- Use WHERE to apply filters exactly as asked. Include all logically implied conditions.\n",
      "- For substring/text matching when the prompt implies a substring search, use LIKE '%term%' and state the assumption about case-sensitivity if relevant.\n",
      "- For date phrasing convert natural language to explicit literals matching the date format used in sql_context (e.g., 'YYYY-MM-DD'). For \"year\" grouping you may use EXTRACT(YEAR FROM date_column) or explicit range predicates (date >= 'YYYY-01-01' AND date < 'YYYY+1-01-01').\n",
      "- Do not rely on SELECT alias names in WHERE; filter aggregates with HAVING.\n",
      "\n",
      "5) Joins, subqueries and exclusions\n",
      "- JOIN tables when needed; prefer explicit joins (JOIN ... ON ...) and use the authoritative table for attributes.\n",
      "- Prefer NOT EXISTS for exclusion subqueries to avoid NULL pitfalls unless context shows NULLs are impossible.\n",
      "- Use HAVING to filter grouped results (e.g., HAVING COUNT(*) > 5).\n",
      "- When you need a per-(A,B) aggregate and then an aggregate of those results (e.g., max count per group), compute the inner aggregation in a subquery and aggregate the subquery.\n",
      "\n",
      "6) DISTINCT, duplicates and counting\n",
      "- If the prompt asks for \"unique\" or \"excluding duplicates\", use COUNT(DISTINCT ...). If the schema suggests possible duplicate rows for the same logical entity, prefer COUNT(DISTINCT ...) and state the assumption briefly.\n",
      "- If unclear whether duplicates can exist, explicitly state your assumption in the reasoning; choose the safer DISTINCT behavior unless the prompt implies counting rows.\n",
      "\n",
      "7) Mutating statements (INSERT/UPDATE/DELETE)\n",
      "- For UPDATE and DELETE always include a WHERE clause unless the prompt explicitly requests changing all rows.\n",
      "- For INSERT always list target columns in INSERT INTO (col1, col2, ...) VALUES (...).\n",
      "- Do not invent values for unspecified required columns without stating them in the short reasoning; when necessary, provide example values and mention them.\n",
      "\n",
      "8) Missing or insufficient schema\n",
      "- If required tables or columns are missing, do NOT invent them silently. Either:\n",
      "  a) Ask for clarification (very short), or\n",
      "  b) Provide a commented hypothetical query showing assumed table/column names and state assumptions in the brief reasoning.\n",
      "- If a best-effort is possible (use available columns), explicitly state any assumptions made.\n",
      "\n",
      "9) Case-sensitivity and text equality\n",
      "- Be mindful that string equality may be case-sensitive in some DBs. If you choose a literal that differs in case from the schema/sample data, state the assumption about case-sensitivity.\n",
      "\n",
      "10) Dialect and limits\n",
      "- Use standard SQL constructs. You may use LIMIT N or FETCH FIRST N ROWS ONLY for top-N results; be consistent.\n",
      "- Use standard aggregate functions (SUM, AVG, MAX, MIN, COUNT) and conditional expressions (CASE WHEN ... THEN ... ELSE ... END).\n",
      "\n",
      "11) Safety and correctness\n",
      "- Do not run or simulate SQL — just produce syntactically correct SQL for the provided schema.\n",
      "- Avoid extraneous columns, tables, or joins that are unnecessary to answer the prompt.\n",
      "- Keep reasoning brief and explicit about assumptions and ambiguous choices.\n",
      "- Use aliases and column lists to make intent clear, but do not expose internal DB engine specifics.\n",
      "\n",
      "12) Common patterns and examples you should follow (use these as templates)\n",
      "- Count per group:\n",
      "  SELECT group_col, COUNT(*) AS cnt FROM table GROUP BY group_col;\n",
      "- Conditional totals as columns:\n",
      "  SELECT SUM(CASE WHEN type='A' THEN amount ELSE 0 END) AS total_A, SUM(CASE WHEN type='B' THEN amount ELSE 0 END) AS total_B FROM table;\n",
      "- Top-N by aggregated value:\n",
      "  SELECT key_col, SUM(amount) AS total FROM table WHERE ... GROUP BY key_col ORDER BY total DESC LIMIT N;\n",
      "- Max of per-subject counts:\n",
      "  SELECT s.subject_name, MAX(sub.course_count) AS max_courses_per_teacher FROM (SELECT subject_id, teacher_id, COUNT(*) AS course_count FROM courses GROUP BY subject_id, teacher_id) sub JOIN subjects s ON s.subject_id = sub.subject_id GROUP BY s.subject_name;\n",
      "\n",
      "13) Style and brevity\n",
      "- Keep the reasoning to 1–3 sentences and concise.\n",
      "- The SQL should be correct and minimal — no extra selects, comments, or scaffolding beyond the reasoning comment + single SQL.\n",
      "\n",
      "If you understand, wait for the sql_context and sql_prompt and then produce the reasoning plus one SQL statement following the rules above.\n",
      "2025/10/14 16:09:52 INFO dspy.evaluate.evaluate: Average Metric: 15.0 / 20 (75.0%)\n",
      "2025/10/14 16:09:52 INFO dspy.teleprompt.gepa.gepa: Iteration 9: New subsample score is not better, skipping\n",
      "GEPA Optimization:  98%|█████████▊| 1160/1180 [09:16<00:11,  1.69rollouts/s]2025/10/14 16:09:52 INFO dspy.teleprompt.gepa.gepa: Iteration 10: Selected program 2 score: 0.685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.00 / 20 (75.0%): 100%|██████████| 20/20 [00:07<00:00,  2.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 16:09:59 INFO dspy.evaluate.evaluate: Average Metric: 15.0 / 20 (75.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 16:10:21 INFO dspy.teleprompt.gepa.gepa: Iteration 10: Proposed new text for predict: You are a SQL/query-writing assistant (a database expert). For each task you will be given two inputs:\n",
      "- sql_context: DDL and sample data (CREATE SCHEMA / CREATE TABLE / CREATE VIEW / INSERT ...) — this is the authoritative schema and sample data you must use.\n",
      "- sql_prompt: a natural-language request describing the data the user wants.\n",
      "\n",
      "Your job:\n",
      "- Produce two clearly labeled sections in your response: \"reasoning\" and \"sql\".\n",
      "  - \"reasoning\": one or two short paragraphs explaining how you derived the query and any assumptions you made (including any handling of ambiguous or missing information). If you use any dialect-specific functions (non-ANSI-standard SQL), state the dialect choice in this reasoning section.\n",
      "  - \"sql\": a single, syntactically valid SQL statement that implements the requested operation against the provided schema and data. The \"sql\" section must contain only the SQL statement and no extra commentary.\n",
      "\n",
      "Formatting and content requirements:\n",
      "1. Use only the tables, views and schemas provided in sql_context. Do not reference tables or columns that do not appear in sql_context unless you explicitly state and justify an assumption in the \"reasoning\" section.\n",
      "2. Column selection:\n",
      "   - Return only the columns requested by the prompt. Do NOT use SELECT * unless the prompt explicitly asks for all columns or a provided view exactly matches the requested output.\n",
      "   - Provide meaningful aliases for aggregated columns (e.g., AS total_revenue).\n",
      "3. Aggregation and grouping:\n",
      "   - Use SUM for numeric quantity/amount columns representing totals. Use COUNT(*) to count rows/records. If a table has a column named like \"count\" or \"quantity\" and it represents quantity, SUM that column.\n",
      "   - When mixing aggregates and non-aggregates, include an appropriate GROUP BY on the non-aggregated columns.\n",
      "   - Use HAVING for filters that apply after aggregation.\n",
      "4. Joins and filtering:\n",
      "   - Join tables on the keys shown in sql_context. If multiple reasonable join keys exist, state your chosen key and why in the reasoning.\n",
      "   - For comparisons across full sets (e.g., \"appears on all platforms\"), compare to the authoritative table listing the full set (e.g., platforms table) rather than inferring it from transactional rows, unless the prompt or context indicates otherwise.\n",
      "5. Maximum/minimum value with associated rows:\n",
      "   - To return the row(s) associated with a MAX or MIN, use a subquery or window function to compute the extremum, then select rows matching that value. Do not rely on aggregation that returns a non-aggregated column alongside MAX without ensuring it corresponds to the MAX.\n",
      "   - Return all tied rows when appropriate (e.g., multiple rows share the max value).\n",
      "6. Dates and relative ranges:\n",
      "   - Use BETWEEN or <= and >= inclusively for date ranges.\n",
      "   - For relative ranges like \"last N days/years\", prefer CURRENT_DATE - INTERVAL 'N' DAY / YEAR (ANSI SQL style). If you use dialect-specific functions (e.g., DATE_SUB/CURDATE for MySQL, GETDATE/DATEDIFF for SQL Server, AGE/DATE_PART for PostgreSQL), state that dialect in the reasoning.\n",
      "7. INSERT / UPDATE / DELETE:\n",
      "   - INSERT: If the table has an auto-generated PK not present in the prompt, omit that column. If the schema requires a value and you provide one, state the assumption and rationale in the reasoning.\n",
      "   - UPDATE: Always include a restrictive WHERE clause matching the user's criteria. If the prompt doesn't specify new values, ask for clarification in the reasoning or make a minimal, clearly stated assumption.\n",
      "   - DELETE: Include a restrictive WHERE clause matching the user's criteria.\n",
      "8. Ambiguity and missing data:\n",
      "   - If the prompt asks for information not present in sql_context (e.g., country, continent, mapping tables), do NOT invent tables. Instead either:\n",
      "     a) explain the limitation and offer the closest possible answer using existing data (clearly state assumptions), or\n",
      "     b) ask for clarification / additional data (state exactly what is missing).\n",
      "   - If you hardcode values because authoritative data is absent, state and justify those assumptions in reasoning.\n",
      "9. Boolean literals:\n",
      "   - Use the boolean literal format consistent with the sample data in sql_context (TRUE/true) and note any dialect assumptions in reasoning if relevant.\n",
      "10. Output ordering:\n",
      "   - Only include ORDER BY if the user requested ordering or if ordering improves readability of grouped outputs; otherwise omit ORDER BY. Use LIMIT only if the prompt asks for a top-N result.\n",
      "11. Dialect and functions:\n",
      "   - Prefer standard (ANSI) SQL. If you must use a dialect-specific function, name the dialect and the function in the reasoning section.\n",
      "12. Error-prone checks (explicitly verify in reasoning when relevant):\n",
      "   - If counting \"items\", confirm whether to use COUNT(*) or SUM(quantity).\n",
      "   - When comparing \"counts of distinct X\" to the total number of X, use the authoritative reference table if present (e.g., platforms).\n",
      "   - When selecting names associated with MAX(value), ensure the name row actually corresponds to that MAX (use subquery/window function).\n",
      "   - For date-based \"last N\" queries, use CURRENT_DATE +/- INTERVAL; state dialect if different.\n",
      "   - For UPDATE/INSERT, avoid inserting/updating explicit IDs that likely are auto-generated unless the schema forces you to.\n",
      "13. Ambiguity handling policy:\n",
      "   - If the prompt is ambiguous and you cannot confidently produce an accurate SQL without more information, state what you need in the reasoning and produce a best-effort query with explicitly listed assumptions.\n",
      "14. Conciseness and tone:\n",
      "   - Keep reasoning concise and focused on key assumptions and steps used to build the SQL (one or two short paragraphs). The SQL should be as simple and direct as possible while satisfying the prompt and following the rules above.\n",
      "\n",
      "Behavioral expectations:\n",
      "- Be precise and conservative: prefer to ask for clarification rather than assume when a choice would materially change results, unless a reasonable default can be justified and documented.\n",
      "- Favor correctness with respect to the provided schema and data over guessing about external context.\n",
      "- Ensure the provided SQL runs against the schema in sql_context (respect schema qualification, table/column names and data types).\n",
      "\n",
      "Examples and common patterns (do these unless prompt suggests otherwise):\n",
      "- Percentage by group: use 100.0 * SUM(CASE WHEN condition THEN 1 ELSE 0 END) / COUNT(*) AS pct or 100.0 * AVG(CASE WHEN condition THEN 1 ELSE 0 END).\n",
      "- Max with rows: WITH t AS (SELECT *, MAX(value) OVER () AS max_val FROM table) SELECT columns FROM t WHERE value = max_val.\n",
      "- Year-over-year difference: use LAG(value) OVER (ORDER BY year) and ABS() if the prompt asks for the \"biggest difference\".\n",
      "- Date range \"last N days\": WHERE date BETWEEN CURRENT_DATE - INTERVAL 'N' DAY AND CURRENT_DATE (or state dialect alternative).\n",
      "\n",
      "Remember: every response must return exactly two labeled sections: \"reasoning\" and \"sql\". The sql section must contain only the SQL statement.\n",
      "2025/10/14 16:10:29 INFO dspy.evaluate.evaluate: Average Metric: 14.0 / 20 (70.0%)\n",
      "2025/10/14 16:10:29 INFO dspy.teleprompt.gepa.gepa: Iteration 10: New subsample score is not better, skipping\n",
      "GEPA Optimization:  98%|█████████▊| 1160/1180 [09:53<00:10,  1.95rollouts/s]\n"
     ]
    }
   ],
   "source": [
    "optimized_program = optimizer.compile(\n",
    "    program,\n",
    "    trainset=train_set_for_optimization,\n",
    "    valset=val_for_tracking,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d21aeb",
   "metadata": {},
   "source": [
    "# Review original and optimized prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51f8d446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a database expert. You are provided with context for how some table(s) were constructed, and a natural language prompt for what the user wants. Your job is to write a SQL query to provide them with the required data.\n"
     ]
    }
   ],
   "source": [
    "print(program.predict.signature.instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf6cf895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a SQL/query-writing assistant (a database expert). For each task you will be given:\n",
      "- sql_context: DDL and sample data (CREATE SCHEMA / CREATE TABLE / CREATE VIEW / INSERT ...). This is the authoritative schema and example data you must use.\n",
      "- sql_prompt: a natural-language request describing the data the user wants.\n",
      "\n",
      "Your job:\n",
      "- Produce a short, clear reasoning paragraph explaining how you derived the query and any assumptions you made from incomplete/ambiguous prompts or from the provided context.\n",
      "- Produce a single SQL statement that implements the requested operation against the provided schema and data. The SQL must be syntactically valid and appropriate for the task described in the prompt.\n",
      "\n",
      "Formatting requirements:\n",
      "- Return two clearly labeled sections in your response: \"reasoning\" (one or two short paragraphs) and \"sql\" (the SQL statement).\n",
      "- The \"sql\" section must contain only the SQL statement (no additional commentary in that section). Use standard SQL where possible; if you use a dialect-specific function (e.g., DATE_SUB/CURDATE for MySQL), state that choice in the reasoning.\n",
      "\n",
      "Functional rules and best practices (derived from examples and feedback):\n",
      "1. Use the provided tables, views and schemas in sql_context. Do not refer to tables or columns that do not appear in sql_context unless you explicitly state and justify an assumption in the reasoning.\n",
      "2. Column selection: return only the columns requested by the prompt. Avoid SELECT * unless the prompt explicitly asks for all columns or the context/view exactly matches the requested output.\n",
      "3. Aggregation and grouping:\n",
      "   - Choose the correct aggregation (SUM vs COUNT). If a table has a numeric \"count\" or \"quantity\" column representing quantities, SUM that column to get totals. Use COUNT(*) to count rows/records when counting occurrences, not when a quantity column exists.\n",
      "   - Include appropriate GROUP BY columns whenever using aggregate functions with non-aggregated columns.\n",
      "   - Use HAVING for post-aggregation filters.\n",
      "4. Joins and filtering:\n",
      "   - Join tables on the correct keys shown in sql_context.\n",
      "   - When the prompt asks about \"all X\" across another set (e.g., genres appearing on all platforms), compare to the authoritative table listing the full set (e.g., platforms table) rather than inferring the set from a transactional table (e.g., sales) unless the prompt or context makes that explicit.\n",
      "5. Maximum value + associated row:\n",
      "   - To return the row(s) associated with a maximum value (e.g., program name with the max volunteer count), use a subquery or window function that finds MAX(value) and then select rows matching that value. Do not rely on aggregation that could return an unrelated non-aggregated column.\n",
      "6. Dates and relative ranges:\n",
      "   - Use BETWEEN or <= and >= consistently and inclusively for date ranges.\n",
      "   - For \"last N years\" or similar relative date ranges, prefer an expression using the current date (e.g., CURRENT_DATE - INTERVAL '5' YEAR or DATE_SUB(CURDATE(), INTERVAL 5 YEAR). State dialect choice in reasoning if non-ANSI functions are used.\n",
      "7. Inserts/Updates:\n",
      "   - For INSERT: if primary key/ID column is not specified in the prompt and the table might have an auto-generated id, omit that column in the INSERT. If you must provide a value because of the schema, state the assumption and explain the chosen value in reasoning.\n",
      "   - For UPDATE: always include a restrictive WHERE clause that matches the user's criteria. If the prompt does not specify the new values explicitly, either ask for clarification (in reasoning) or make a minimal, clearly stated assumption and implement it.\n",
      "8. Ambiguity and missing data:\n",
      "   - If the prompt requests filtering by a field or table that does not exist in sql_context (e.g., population table not present), do not invent that table. Instead either:\n",
      "     a) state the limitation and provide the closest possible answer using existing data (explain assumptions), or\n",
      "     b) ask for clarification / additional data (state what is missing).\n",
      "   - If you hardcode city names or other values because of absence of authoritative data, state that these are assumptions in the reasoning and why you chose them.\n",
      "9. Boolean values and literals:\n",
      "   - Use the boolean literal representation consistent with the sql_context example data (TRUE/true as present). Match case if necessary for clarity but note it in reasoning if dialect-specific.\n",
      "10. Output clarity:\n",
      "   - Provide meaningful column aliases for aggregated results (e.g., AS total_revenue, AS max_visits) so results are explicit.\n",
      "   - ORDER BY only if the user asked for ordering or if it improves readability of grouped outputs; otherwise omit.\n",
      "\n",
      "Error-prone cases to watch for (explicitly check in your reasoning if relevant):\n",
      "- Summing a \"count\" column vs counting rows.\n",
      "- Comparing counts of distinct platforms to the total number of platforms (use the platforms table if present).\n",
      "- Returning a name with MAX(value) without ensuring the name corresponds to that MAX.\n",
      "- Updating or inserting explicit IDs when the schema likely expects auto-generated IDs.\n",
      "- Assuming location or year filtering columns exist when they do not.\n",
      "\n",
      "Tone and length:\n",
      "- Keep reasoning concise and focused on the key assumptions and steps used to build the SQL.\n",
      "- The SQL should be as simple and direct as possible to satisfy the prompt while following the above rules.\n",
      "\n",
      "If the prompt is ambiguous and you cannot confidently produce an accurate SQL without more information, state what you need in the reasoning and produce a best-effort query with explicit assumptions noted.\n"
     ]
    }
   ],
   "source": [
    "print(optimized_program.predict.signature.instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f2a893",
   "metadata": {},
   "source": [
    "# Store Programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e93a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "program.save(\"./dspy_program/program.json\", save_program=False)\n",
    "optimized_program.save(\"./optimized_program/program.json\", save_program=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e85afb4",
   "metadata": {},
   "source": [
    "# Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64245ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datasets import Dataset\n",
    "from time import perf_counter\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "def evaluate_program(\n",
    "    program,\n",
    "    ds_test: Dataset,\n",
    "    limit: int = 100,\n",
    "    max_workers: int = 8,\n",
    "    field_map: Optional[Dict[str, str]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate a DSPy program on the first `limit` rows of a HF Dataset split.\n",
    "\n",
    "    Args:\n",
    "        program: a DSPy Module with signature program(sql_prompt=..., sql_context=...)\n",
    "        ds_test: Hugging Face Dataset (e.g., ds[\"test\"])\n",
    "        limit: number of rows to evaluate (default 100)\n",
    "        max_workers: parallel threads for I/O-bound LM + judge\n",
    "        field_map: optional mapping if your column names differ:\n",
    "                   {\"sql_prompt\": \"...\", \"sql_context\": \"...\", \"sql\": \"...\"}\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"accuracy\": float,\n",
    "          \"correct\": int,\n",
    "          \"total\": int,\n",
    "          \"avg_latency_s\": float,\n",
    "          \"failures\": [ {idx, reason, pred_sql, feedback} ... up to 20 ],\n",
    "        }\n",
    "    \"\"\"\n",
    "    if field_map is None:\n",
    "        field_map = {\"sql_prompt\": \"sql_prompt\", \"sql_context\": \"sql_context\", \"sql\": \"sql\"}\n",
    "\n",
    "    n = min(limit, len(ds_test))\n",
    "    subset = ds_test.select(range(n))\n",
    "    start = perf_counter()\n",
    "\n",
    "    def _eval_one(i_row):\n",
    "        i, row = i_row\n",
    "        try:\n",
    "            pred = program(\n",
    "                sql_prompt=row[field_map[\"sql_prompt\"]],\n",
    "                sql_context=row[field_map[\"sql_context\"]],\n",
    "            )\n",
    "            pred_sql = getattr(pred, \"sql\", None) or (pred.get(\"sql\") if isinstance(pred, dict) else None) or \"\"\n",
    "            jr = judge(\n",
    "                sql_context=row[field_map[\"sql_context\"]],\n",
    "                sql_prompt=row[field_map[\"sql_prompt\"]],\n",
    "                golden_sql=row[field_map[\"sql\"]],\n",
    "                candidate_sql=pred_sql,\n",
    "            )\n",
    "            ok = bool(getattr(jr, \"similar\", False))\n",
    "            feedback = getattr(jr, \"reasoning\", \"\") or \"\"\n",
    "            return (i, ok, pred_sql, feedback, None)\n",
    "        except Exception as e:\n",
    "            return (i, False, \"\", \"\", f\"{type(e).__name__}: {e}\")\n",
    "\n",
    "    results = []\n",
    "    # Threaded evaluation (I/O bound: LM + judge). Tune max_workers to your provider limits.\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futures = [ex.submit(_eval_one, (i, subset[i])) for i in range(n)]\n",
    "        for f in as_completed(futures):\n",
    "            results.append(f.result())\n",
    "\n",
    "    # Sort back to input order\n",
    "    results.sort(key=lambda x: x[0])\n",
    "\n",
    "    correct = sum(1 for _, ok, *_ in results if ok)\n",
    "    total = n\n",
    "    acc = correct / total if total else 0.0\n",
    "    elapsed = perf_counter() - start\n",
    "    avg_lat = elapsed / total if total else 0.0\n",
    "\n",
    "    failures = []\n",
    "    for i, ok, pred_sql, feedback, err in results:\n",
    "        if not ok and len(failures) < 20:\n",
    "            failures.append({\n",
    "                \"idx\": i,\n",
    "                \"reason\": (\"error: \" + err) if err else \"mismatch\",\n",
    "                \"pred_sql\": pred_sql,\n",
    "                \"feedback\": feedback,\n",
    "            })\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"correct\": correct,\n",
    "        \"total\": total,\n",
    "        \"avg_latency_s\": avg_lat,\n",
    "        \"failures\": failures,\n",
    "    }\n",
    "    \n",
    "test_split = ds[\"test\"]\n",
    "test_split = test_split.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ca6e0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Program: {'accuracy': 0.694, 'correct': 347, 'total': 500, 'avg_latency_s': 0.19754389458199148, 'failures': [{'idx': 9, 'reason': 'mismatch', 'pred_sql': \"SELECT COUNT(*) AS total_articles FROM ny_times WHERE article_date >= '2021-01-01' AND article_date < '2021-03-01';\", 'feedback': 'The golden SQL query returns two counts separately: one count of articles published in January 2021 and another count of articles published in February 2021. It uses a UNION ALL to list these counts as two separate rows. In contrast, the candidate SQL query provides a single count of all articles published in the combined date range from January 1, 2021, to just before March 1, 2021, which effectively covers both January and February 2021 collectively. Although the candidate query sums the two months together and returns one single total count, while the golden SQL returns two separate counts, the underlying aggregation of the number of articles for the two months combined is effectively the same. The prompt asks for \"the total number of news articles published in January 2021 and February 2021\" which is naturally interpreted as one aggregated total count for both months combined. The candidate query correctly returns this single total count. The golden SQL query provides two separate counts rather than one combined total, which is not the best fit for the prompt. Hence, functionally, the candidate query fulfills the prompt by delivering the total combined count, whereas the golden query provides two separate counts. Given this, they are not functionally equivalent because one returns two rows (counts per month), and the other returns one row (total combined count). Therefore, they are not functionally similar.'}, {'idx': 10, 'reason': 'mismatch', 'pred_sql': 'SELECT MAX(amount_invested) AS max_amount_invested FROM shariah_compliant_funds_investments;', 'feedback': 'The golden SQL query joins the clients table with the shariah_compliant_funds_investments table to retrieve the client_name along with the maximum amount invested. This means it returns both the name of the client who made the maximum investment and the value of that maximum investment. On the other hand, the candidate SQL query only returns the maximum amount_invested value from the shariah_compliant_funds_investments table without any client identification. Since the user prompt asks \"What is the maximum amount invested by a client in Shariah-compliant funds?\"—which can be interpreted as wanting the maximum amount value—both queries get the maximum amount invested, but only the golden SQL also provides the client name. The candidate omits the client name, which arguably makes it less informative. Therefore, they are not functionally equivalent because the golden query provides additional key information (client name) about the maximum amount, which the candidate query does not.'}, {'idx': 12, 'reason': 'mismatch', 'pred_sql': 'SELECT name FROM species WHERE growth_rate > (SELECT AVG(growth_rate) FROM species);', 'feedback': 'The golden SQL query selects species names and their growth rates where the growth rate is greater than the average growth rate calculated across all species joined with protected areas, ensuring only species linked to protected areas are considered. Additionally, it uses GROUP BY on species name and growth rate, which is redundant but does not affect correctness.\\n\\nThe candidate SQL query selects species names where the growth rate is greater than the average growth rate across all species without joining with protected areas. This means it includes all species regardless of their protected_area_id. Since the prompt specifically asks for species with growth rates higher than the average for protected areas, the candidate query may include species not linked to protected areas or may calculate the average over all species rather than those strictly within protected areas.\\n\\nTherefore, the candidate SQL query is not functionally equivalent to the golden SQL query because it does not consider the average growth rate for species in protected areas only, and it also omits selecting the growth rate itself as requested.'}, {'idx': 14, 'reason': 'mismatch', 'pred_sql': \"SELECT COUNT(*) AS total_immunization_records_past_month FROM immunization_records WHERE record_date >= CURRENT_DATE - INTERVAL '30 days';\", 'feedback': \"The golden SQL query counts the immunization records from the past month specifically for patients in Nigeria, filtering by the condition `country = 'Nigeria'`. However, the `immunization_records` table schema provided does not include a `country` column, so this filter might be invalid or the table context might be incomplete. The candidate SQL query counts records over the past 30 days but does not filter by country at all.\\n\\nWhile both queries aim to count records from roughly the last month, the golden query filters by country (Nigeria), but the candidate query does not include this filter. Since the prompt explicitly requests the total records in Nigeria over the past month, missing the country filter is a functional difference.\\n\\nThus, the candidate query is not functionally similar to the golden query because it does not restrict the result to Nigeria, which is the key aspect of the user's prompt.\"}, {'idx': 15, 'reason': 'mismatch', 'pred_sql': 'WITH pickup_counts AS (SELECT pickup_time, COUNT(*) AS cnt FROM taxi_trip GROUP BY pickup_time ORDER BY cnt DESC, pickup_time LIMIT 1), dropoff_counts AS (SELECT dropoff_time, COUNT(*) AS cnt FROM taxi_trip GROUP BY dropoff_time ORDER BY cnt DESC, dropoff_time LIMIT 1) SELECT pickup_time AS most_common_pickup_time FROM pickup_counts UNION ALL SELECT dropoff_time AS most_common_dropoff_time FROM dropoff_counts;', 'feedback': 'The golden SQL query attempts to find the most common combination of pickup_time and dropoff_time together, by grouping on both columns and returning the one with the highest trip count. It returns a single row with the most common pickup and dropoff times as a pair.\\n\\nThe candidate SQL query instead finds the most common pickup_time and the most common dropoff_time independently in two separate subqueries, each selecting the single most frequent time for pickup and dropoff separately. It then returns two rows, one for the most common pickup_time and one for the most common dropoff_time, rather than a single pair.\\n\\nBecause the candidate SQL does not find the most common pick-up and drop-off time pair, but rather the most common pick-up and drop-off times separately, it produces different results. The prompt specifically asks for the most common pick-up and drop-off times for taxi trips, which is ambiguous but the golden query captures the intuitively most frequent combination of both, while the candidate treats them independently.\\n\\nTherefore, the two queries are not functionally similar.'}, {'idx': 19, 'reason': 'mismatch', 'pred_sql': \"SELECT crop_name, SUM(production_tons) AS total_production_tons FROM organic_crops WHERE country IN ('Australia', 'New Zealand', 'Papua New Guinea') GROUP BY crop_name;\", 'feedback': \"The golden SQL query filters the data to include only rows where the country is exactly 'Oceania', which does not match any entries in the dataset since the countries listed are 'Australia', 'New Zealand', and 'Papua New Guinea'. The candidate SQL query, however, correctly interprets 'Oceania' as a region comprising Australia's, New Zealand's, and Papua New Guinea's data by including all three countries with a WHERE clause using IN. This approach aligns better with the prompt's intention of summing production in the Oceania region. Despite differing WHERE clauses, the candidate SQL captures the correct semantic meaning for the natural language prompt, whereas the golden SQL will return no results because there is no country named 'Oceania'. Therefore, the candidate SQL is functionally more accurate for the prompt, but technically the two queries' conditions are different and produce different outputs.\\n\\nGiven the reasoning above, the queries are not functionally similar because they filter on different country values, leading to different results.\"}, {'idx': 22, 'reason': 'mismatch', 'pred_sql': \"SELECT ABS(SUM(CASE WHEN equipment_type = 'Type A' THEN maintenance_cost ELSE 0 END) - SUM(CASE WHEN equipment_type = 'Type B' THEN maintenance_cost ELSE 0 END)) AS maintenance_cost_difference_2022 FROM equipment_maintenance WHERE equipment_type IN ('Type A', 'Type B') AND maintenance_date BETWEEN '2022-01-01' AND '2022-12-31';\", 'feedback': 'The golden SQL computes the difference in total maintenance costs between \\'Type A\\' and \\'Type B\\' aircraft for the year 2022, dividing the result by 10,000. It sums maintenance_cost by equipment_type within 2022 using EXTRACT(YEAR...), subtracts \\'Type B\\' sum from \\'Type A\\' sum, and then divides by 10,000, returning a possibly positive or negative difference.\\n\\nThe candidate SQL sums maintenance_cost likewise by equipment_type for \\'Type A\\' and \\'Type B\\' within 2022, using a date range filter instead of EXTRACT(YEAR...). It subtracts \\'Type B\\' sum from \\'Type A\\' sum but then applies ABS(), thus always returning a non-negative difference (absolute value). It also does not divide the difference by 10,000. Additionally, it filters equipment_type in the WHERE clause to reduce rows considered but functionally this is equivalent.\\n\\nMain differences:\\n- Candidate uses absolute value (ABS), golden does not (sign is preserved).\\n- Candidate does not divide by 10,000, golden does.\\n- Candidate uses BETWEEN date range, golden uses EXTRACT(YEAR...) - both are equivalent for 2022.\\n\\nAs the natural language prompt requests the \"difference\" in maintenance costs, which usually implies a signed difference (could be positive or negative depending on which cost is higher), the golden query matches that. The candidate query returns the absolute difference, which changes the meaning by removing sign information and also returns value 10,000 times larger because no division occurs.\\n\\nTherefore, the candidate SQL is not functionally equivalent to the golden SQL.'}, {'idx': 30, 'reason': 'mismatch', 'pred_sql': \"SELECT SUM(gs.revenue) AS total_revenue FROM garment_manufacturing gm JOIN garment_sales gs ON gm.garment_id = gs.garment_id WHERE gm.country = 'US';\", 'feedback': \"The golden SQL query correctly calculates the total revenue for garments manufactured in the United States and sold specifically in California by joining the garment_manufacturing and garment_sales tables on garment_id, filtering the manufacturing country as 'United States', and restricting sales to stores located within California using a subquery that filters stores by region.\\n\\nThe candidate SQL query sums the revenue for garments manufactured in 'US' but lacks the critical restriction on sales location (California). Also, the country condition uses 'US' instead of 'United States', which might not match the stored data depending on conventions. Most importantly, the candidate does not constrain sales to California stores, so it will include revenue from all sales regardless of location, making it functionally different.\\n\\nTherefore, the candidate SQL does not fully meet the prompt requirements and is not functionally similar to the golden SQL query.\"}, {'idx': 38, 'reason': 'mismatch', 'pred_sql': 'SELECT id, name, distance_last_3p AS average_distance_last_3p_shots FROM players;', 'feedback': 'The golden SQL query attempts to calculate the average distance of the last 5 three-point shots per player using a window function, which aligns with the prompt\\'s requirement. It uses AVG() as a window function partitioned by player name and ordered by name, specifying a frame to capture the last 5 shots. Although the frame specification (\"ROWS BETWEEN UNBOUNDED PRECEDING AND 4 PRECEDING\") seems unusual and may not correctly select the last 5 shots, it is at least a deliberate attempt at averaging over multiple rows.\\n\\nThe candidate SQL query simply selects the id, name, and distance_last_3p columns from the players table without any aggregation or window function. It does not attempt to compute any average and thus does not meet the prompt\\'s requirement to find the average distance for the last 5 three-point shots per player.\\n\\nTherefore, the candidate SQL is functionally not similar to the golden SQL. It does not calculate any average and hence fails to answer the prompt.'}, {'idx': 44, 'reason': 'mismatch', 'pred_sql': 'SELECT donor_id, COUNT(*) * 1.0 / (julianday(MAX(donation_date)) - julianday(MIN(donation_date)) + 30) * 30 AS avg_donations_per_month FROM donations GROUP BY donor_id;', 'feedback': 'The golden SQL query attempts to calculate the average number of donations per month for each donor by grouping donations both by donor_id and by month (using DATE_FORMAT on donation_date), then averaging the counts of donations per month. The WITH ROLLUP is used but likely unnecessary or incorrect here.\\n\\nThe candidate SQL query calculates the average donations per month by taking the total count of donations per donor, dividing it by the number of days between the first and last donation plus 30, then multiplying by 30. This is approximating the average donations per month based on a linear interpolation of days. \\n\\nThe main difference:\\n- Golden SQL groups by each month explicitly and averages the counts of donations per month.\\n- Candidate SQL estimates it via total donations and average number of months calculated from date difference.\\n\\nThe candidate query does not do a monthly grouping and does not average counts per month as the golden query does. Instead, it uses a time-span approach to approximate monthly average. Hence, these two queries are not functionally equivalent.\\n\\nAlso, the golden SQL is in MySQL syntax (DATE_FORMAT), and the candidate one uses julianday which is SQLite specific.\\n\\nTherefore, the candidate query is not functionally similar to the golden query in terms of how the average donations per month is calculated.'}, {'idx': 46, 'reason': 'mismatch', 'pred_sql': \"SELECT employment_rate FROM veteran_employment WHERE state = 'Texas' AND employment_date = '2021-12-01';\", 'feedback': 'The golden SQL query filters the employment rate for Texas in December 2021 by checking that the employment_date is any date in December 2021 using the MONTH() and YEAR() functions on the employment_date column. This means it encompasses all employment_dates within December 2021.\\n\\nThe candidate SQL query filters by an exact employment_date = \\'2021-12-01\\', i.e., only the employment rate recorded on December 1, 2021.\\n\\nIf there are multiple records within December 2021, the golden query would include all of them, whereas the candidate query only considers a single date in December. Therefore, the candidate query is more restrictive and may return a different (probably single) result than the golden query which includes all data for December 2021.\\n\\nThus, these two queries are not functionally equivalent in a general case, unless the data only has a single record for December 2021 corresponding to \\'2021-12-01\\'.\\n\\nBecause the prompt asks for the veteran employment rate \"as of December 2021\", which likely refers to the whole month or the last date of December, the golden SQL covers the entire month, while the candidate only covers one date.\\n\\nHence, the queries are not functionally similar.'}, {'idx': 49, 'reason': 'mismatch', 'pred_sql': 'SELECT t.name, tr.duration FROM treatments tr JOIN therapists t ON tr.therapist_id = t.id ORDER BY tr.duration DESC LIMIT 1;', 'feedback': \"The golden SQL query joins the treatments and therapists tables, groups the results by therapist name, and selects the maximum duration per therapist. The output will show each therapist's longest session duration. The candidate SQL query also joins the two tables but selects the single longest session overall by ordering by duration in descending order and limiting the output to one record. \\n\\nThe prompt asks for the therapist who conducted the longest single therapy session and how long it lasted. The golden SQL returns the longest session per therapist, which is not requested. The candidate SQL returns the single longest therapy session across all therapists, which matches the prompt better.\\n\\nTherefore, the candidate SQL is functionally different from the golden SQL in that it finds the maximum over all sessions whereas the golden SQL finds maximum per therapist. The candidate SQL is more aligned with the prompt's intent, while the golden SQL returns extraneous grouped results.\"}, {'idx': 56, 'reason': 'mismatch', 'pred_sql': 'SELECT news_channel, gender, AVG(followers) AS average_followers FROM news_anchors GROUP BY news_channel, gender;', 'feedback': 'The user query specifies calculating the average number of followers for news anchors \"in a specific news channel,\" which implies filtering by a particular news channel (e.g., \\'Channel1\\'). The golden SQL query explicitly filters the results using a WHERE clause to limit to \\'Channel1\\', and groups by gender.\\n\\nThe candidate SQL query instead returns the average followers grouped by both news_channel and gender, without filtering for a specific news channel. This means it produces average followers for all channels separately, which is broader than the user request.\\n\\nBecause the candidate SQL does not restrict the results to a specific channel as requested, it is not functionally equivalent to the golden SQL query.'}, {'idx': 58, 'reason': 'mismatch', 'pred_sql': '-- Cannot provide a valid query due to missing tables such as user play data and user region information.', 'feedback': \"The candidate SQL query does not provide an actual SQL query for the prompt; instead, it indicates the inability to produce the query due to missing relevant tables such as user play data and user region information. The golden SQL query assumes the existence of a table named 'user_actions' containing user_id, region, game_id, and action_date to answer the prompt. Since the candidate does not have data or a query that matches the prompt's intent, it fails to functionally replicate the golden SQL query. Therefore, the candidate SQL is not functionally similar to the golden SQL query.\"}, {'idx': 59, 'reason': 'mismatch', 'pred_sql': \"SELECT COUNT(*) AS PetitionCount FROM Petition WHERE StateSponsor IN ('California', 'Texas') AND Topic = 'Immigration Reform';\", 'feedback': \"The golden SQL query counts the distinct PetitionID values for petitions sponsored in either 'California' or 'Texas', with the topic 'Immigration Reform', and specifically for the fiscal year '2022'. The candidate SQL query counts all rows (without distinct) where StateSponsor is either 'California' or 'Texas' and the topic is 'Immigration Reform', but it does not restrict the results to the fiscal year '2022' and does not use DISTINCT for the PetitionID. This means the candidate could count duplicates if they exist and also could include petitions from other fiscal years. Because the candidate query is missing the FiscalYear filter and does not ensure unique petitions, it is not functionally equivalent to the golden query.\"}, {'idx': 62, 'reason': 'mismatch', 'pred_sql': \"SELECT STRFTIME('%Y-%m', date) AS year_month, MIN(amount) AS min_donation FROM donations GROUP BY year_month ORDER BY year_month;\", 'feedback': \"The golden SQL extracts just the month from the date and groups by it, thus it returns the minimum donation amount aggregated by month number across all years. The candidate SQL extracts the year and month together (in 'YYYY-MM' format) and groups by this value, which means it computes the minimum donation for each unique year-month combination, distinguishing between the same month in different years. Given the provided data is all from one year (2022), both queries will produce similar results in this case. However, conceptually, the candidate query is grouping by year and month combined, whereas the golden is grouping only by month without considering the year. Since the prompt asks for the minimum donation amount for each month (without specifying year), the golden query’s logic matches the natural language prompt more directly.\\n\\nFunctionally, for the current dataset, both will return the same minimum amount for each month, but if data across multiple years were present, their outputs would diverge. Therefore, the queries are not fully functionally equivalent regarding the prompt.\"}, {'idx': 65, 'reason': 'mismatch', 'pred_sql': \"SELECT DISTINCT s.id, s.name, s.country FROM suppliers s JOIN products p ON s.id = p.supplier_id WHERE p.is_organic = true AND p.product_country = 'Canada';\", 'feedback': 'The golden SQL query selects supplier names who have organic products (is_organic = true) and ensures that the supplier\\'s country matches the product\\'s country, and finally filters to suppliers located in Canada. The candidate SQL query selects distinct suppliers (including id, name, and country) that offer organic products from Canada (p.product_country = \\'Canada\\'), but it does not require that the supplier\\'s country matches the product\\'s country, and it filters on product_country only instead of supplier country. Because the prompt specifically asks for suppliers offering organic products from Canada, without explicitly requiring the supplier to be from Canada, the candidate’s filtering on product_country = \\'Canada\\' is correct. The golden SQL also filters s.country = \\'Canada\\', which aligns with \"suppliers offering organic products from Canada\" if interpreted as \"suppliers from Canada with organic products\". However, the candidate does not filter s.country, so it can include suppliers based in other countries but offering products from Canada. Therefore, the two queries have different filtering logic: golden requires supplier country = Canada and product country matching supplier country, candidate requires product country = Canada but no supplier country filter. \\n\\nThus, the candidate could list suppliers outside Canada who supply organic products from Canada, while golden only lists suppliers located in Canada supplying organic products from Canada. Since the prompt likely implies suppliers located in Canada offering organic products (regardless of product country), the golden SQL is more aligned, and the candidate less so as it ignores supplier country. Hence, they are not functionally equivalent.'}, {'idx': 67, 'reason': 'mismatch', 'pred_sql': '-- No update can be performed without specific data.', 'feedback': 'The golden SQL query performs an update to the \\'infection_rate\\' for the state \\'Texas\\' setting it to 5.7, which is directly executing an update as requested by the prompt \"Update the \\'infection_rates\\' table with new data.\" The candidate SQL query does not attempt an update and instead comments that no update can be performed without specific data. While the candidate SQL query aligns with a reasoning that updates require new data, it does not fulfill the requirement to perform an update if new data is implied by the prompt. The prompt does not provide explicit new data, but the golden query assumes the \"new data\" means updating Texas\\' infection rate to 5.7. The candidate query refuses to perform an update due to lack of data. Functionally, these two responses are not equivalent because the candidate produces no update, while the golden query updates the data. Therefore, they are not functionally similar.'}, {'idx': 69, 'reason': 'mismatch', 'pred_sql': 'SELECT ls.ContractorID, SUM(ls.EmployeeCount) AS TotalEmployees FROM LaborStatistics ls JOIN SustainablePractices sp ON ls.ContractorID = sp.ContractorID GROUP BY ls.ContractorID;', 'feedback': 'The golden SQL query first selects the ContractorIDs from the SustainablePractices table, then sums the EmployeeCount from the LaborStatistics table for only those contractors whose ContractorID is in the SustainablePractices table. This query returns a single aggregate value: the total number of employees for all contractors who implemented sustainable practices.\\n\\nThe candidate SQL query joins LaborStatistics with SustainablePractices on ContractorID and sums EmployeeCount grouped by ContractorID, thereby returning the total number of employees per contractor who implemented sustainable practices as multiple rows (one per contractor).\\n\\nFunctionally, both queries return information about the number of employees for contractors with sustainable practices. However, the golden SQL returns one aggregated sum for all such contractors, whereas the candidate SQL returns sums per contractor. Because the prompt only asks \"What is the number of employees for contractors who implemented sustainable practices?\"—a phrase that can be interpreted as the total number combined—the golden SQL returns a total sum, while the candidate SQL returns a breakdown per contractor.\\n\\nTherefore, the candidate SQL is not functionally equivalent to the golden SQL due to differing levels of aggregation and result format.'}, {'idx': 71, 'reason': 'mismatch', 'pred_sql': 'SELECT MAX(fare) AS max_fare FROM urban.trams WHERE fare >= 2;', 'feedback': 'The golden SQL query uses a condition \"fare > 2\" which excludes fares that are equal to 2 and those less than 2, meaning only fares strictly greater than 2 are considered. The candidate SQL query uses \"fare >= 2,\" which includes fares equal to 2 as well as those greater than 2. According to the prompt, fares less than $2 should be excluded, implying fares of $2 should be included, or at least the prompt is ambiguous about whether exactly 2 is included or not. The golden query excludes fares equal to 2, while the candidate includes them. Since the prompt is to exclude fares less than $2, which logically means fares equal to or greater than $2 are valid, the candidate SQL aligns better with the prompt\\'s wording. The difference comes down to an inclusive vs exclusive boundary at fare = $2. Functionally, the candidate SQL is more consistent with \"excluding fares less than $2\" because it includes fares of $2. The aliasing in the candidate SQL (AS max_fare) does not affect the function. Overall, the queries are not functionally equivalent because one excludes fares equal to 2 and the other includes them, which affects the result when fares of exactly 2 exist in the data.'}]}\n"
     ]
    }
   ],
   "source": [
    "og_metrics = evaluate_program(program, test_split, limit=500, max_workers=32)\n",
    "print(f\"Original Program: {og_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2fdea35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Program: {'accuracy': 0.736, 'correct': 368, 'total': 500, 'avg_latency_s': 0.1709705568339996, 'failures': [{'idx': 4, 'reason': 'mismatch', 'pred_sql': \"UPDATE peacekeeping_operations SET troops = 850 WHERE country = 'Afghanistan' AND year = 2005;\", 'feedback': \"The golden SQL query uses a common table expression (CTE) with an UPDATE statement and then inserts the affected rows back into the same table. This is an unusual and likely incorrect approach for updating a value, as it tries to insert rows back into the table after updating them, which may lead to errors or duplicate entries if the primary key is not changed or not handled properly.\\n\\nThe candidate SQL query is a straightforward UPDATE statement that sets the 'troops' value to 850 for the row where the country is 'Afghanistan' and the year is 2005. This is the standard and correct way to update a row in a table.\\n\\nFunctionally, both queries intend to update the troops value for the specific record. However, the golden query is overly complicated and possibly erroneous due to the unnecessary INSERT after the UPDATE. The candidate query correctly performs the intended update operation straightforwardly and effectively.\\n\\nSince the candidate query performs the functional update as intended, while the golden query attempts an UPDATE but then tries to insert the updated rows back into the table (which is generally incorrect or redundant), they differ in functional behavior and correctness.\"}, {'idx': 9, 'reason': 'mismatch', 'pred_sql': 'SELECT user_id, data_sharing, sharing_preferences FROM privacy_settings;', 'feedback': \"The golden SQL query retrieves users' names, their data sharing status, and specifically filters for users who have location sharing set to true within their sharing_preferences JSON field. It joins the privacy_settings table with a users table and filters on the specific JSON attribute for location sharing, effectively showing only those users who share their location data.\\n\\nThe candidate SQL query selects all columns from privacy_settings without filtering or joining to the users table, and does not extract or filter on any specific sharing preference. Therefore, the candidate query does not limit the results to users who are sharing location data nor provide user names.\\n\\nThese queries therefore serve different purposes: the golden SQL drills down specifically into users sharing location data with user names, while the candidate SQL provides a broad, unfiltered view of all privacy settings.\\n\\nHence, the candidate SQL query is not functionally similar to the golden SQL query.\"}, {'idx': 17, 'reason': 'mismatch', 'pred_sql': 'SELECT Category, SUM(Price) AS total_revenue FROM MenuItems GROUP BY Category;', 'feedback': 'The golden SQL query correctly categorizes menu items into two groups: \\'Vegetarian\\' and \\'Non-Vegetarian\\' by using a CASE statement. Items with category \\'Vegetarian\\' are grouped under \\'Vegetarian\\', and all other categories fall under \\'Non-Vegetarian\\'. It then sums the prices to get the total revenue for these two broad categories.\\n\\nThe candidate SQL query groups by the original \"Category\" column and sums the prices. This means it will have separate rows for all existing categories, including \\'Vegetarian\\' and \\'Meat\\' (or any other categories), rather than grouping all non-vegetarian items together as \"Non-Vegetarian\".\\n\\nSince the prompt asks for the total revenue for vegetarian and non-vegetarian items, the candidate query does not properly consolidate all non-vegetarian items into one group, unlike the golden query. Thus, the candidate query does not produce the same functional result as the golden query.'}, {'idx': 21, 'reason': 'mismatch', 'pred_sql': \"SELECT AVG(yearly_training) AS average_trainings_per_year FROM (SELECT year, SUM(training_count) AS yearly_training FROM DisasterPreparedness WHERE state = 'California' GROUP BY year) AS yearly_totals;\", 'feedback': \"The golden SQL query calculates the average number of trainings held per year in California by selecting the average of training_count values filtered by 'California' grouped by year. It averages the training_count per row for each year.\\n\\nThe candidate SQL query first calculates the total number of trainings per year in California by summing training_count grouped by year, then calculates the average of these yearly totals. This approach correctly aggregates training counts per year before averaging across years.\\n\\nThe key difference is that the golden SQL does not sum training_count per year before averaging, so it averages the training_count values directly per row within each year, which may not be correct if multiple rows per year exist. The candidate SQL properly sums per year first then averages, which is the appropriate way to find the average number of trainings per year.\\n\\nThus, the candidate SQL provides the correct functionality implied by the prompt, while the golden SQL's result may be incorrect if multiple rows exist per year.\\n\\nTherefore, they are not functionally similar.\"}, {'idx': 22, 'reason': 'mismatch', 'pred_sql': 'SELECT Region, COUNT(Service) AS service_count, RANK() OVER (ORDER BY COUNT(Service) DESC) AS availability_rank FROM HealthcareServices GROUP BY Region;', 'feedback': 'The golden SQL query returns each individual healthcare service within each region along with a rank based on the service name within that region (using ROW_NUMBER() partitioned by Region and ordered by Service). This effectively enumerates each service per region.\\n\\nThe candidate SQL query, on the other hand, aggregates the data by region, counting the number of services in each region (COUNT(Service) AS service_count), and then ranks the regions based on the service count using RANK() OVER (ORDER BY COUNT(Service) DESC). This aggregates to one row per region, showing total service count and regional ranking based on availability.\\n\\nThe user\\'s prompt requests: \"How many healthcare services are available in each region, and what is their ranking based on availability?\" The candidate query directly addresses this with total counts and ranking of regions by availability.\\n\\nThe golden query, while showing each service and a ranking within each region, does not produce per-region counts or a ranking of the regions by service availability.\\n\\nHence, the two queries answer related but different questions and produce different result formats.\\n\\nTherefore, they are not functionally similar.'}, {'idx': 26, 'reason': 'mismatch', 'pred_sql': \"SELECT location, SUM(num_beds) AS total_beds FROM hospitals GROUP BY location UNION ALL SELECT 'Total' AS location, SUM(num_beds) FROM hospitals;\", 'feedback': 'The golden SQL query groups the hospitals by location and calculates both the sum and the average number of beds for each location, providing a distribution breakdown. The candidate SQL groups by location and calculates the total beds per location but also adds an additional row with the overall total beds across all locations. The candidate does not provide the average number of beds per location, nor does it provide the distribution in the exact same format as the golden SQL. While both queries address the sum of beds by location, the golden SQL gives average beds as an additional metric, and the candidate gives a grand total row instead. Thus, the candidate query is not functionally similar because the outputs and metrics differ.'}, {'idx': 29, 'reason': 'mismatch', 'pred_sql': \"SELECT MAX(daily_total) AS max_observations FROM (SELECT observation_date, SUM(number_of_observations) AS daily_total FROM marine_species_observations WHERE ocean = 'Baltic Sea' GROUP BY observation_date) sub;\", 'feedback': 'The golden SQL query tries to find the maximum number of observations by selecting the maximum value of number_of_observations per day (grouped by ocean and day) between specific dates (2000-01-01 and 2022-12-31). However, the use of MAX(number_of_observations) with a GROUP BY ocean and DATE_TRUNC(\\'day\\', observation_date) does not produce the total number of species observed in a day but rather the maximum single observation count within that day for Baltic Sea.\\n\\nIn contrast, the candidate SQL query computes the total number of observations per day in the Baltic Sea by summing number_of_observations per observation_date and then selects the maximum daily total across all days. This approach aligns better with the natural language prompt — \"maximum number of marine species ever observed in a single day\" — by summing observations within the day rather than just taking the max single record.\\n\\nThere is a difference in date range filtering, as the golden SQL restricts to dates between 2000-01-01 and 2022-12-31, while the candidate does not apply any date restriction. This could affect the result if data outside that date range exists.\\n\\nFunctionally, the candidate is closer to the intent of the query by summing the daily observations to get total counts per day and then finding the maximum sum across days. The golden SQL only finds the maximum observation for a single record per day, which could underestimate the total daily count.\\n\\nDue to this significant difference in aggregation logic (MAX of single observations vs. MAX of sums per day), their results can differ, and they are not functionally equivalent.'}, {'idx': 35, 'reason': 'mismatch', 'pred_sql': 'SELECT AVG(serving_size) AS avg_serving_size FROM dishes WHERE protein >= 50;', 'feedback': 'The golden SQL query attempts to calculate the average serving size of dishes meeting some protein threshold, but it has multiple issues: it uses a correlated subquery \"(SELECT serving_size * 0.3)\" without referencing a table properly, and the grouping by protein with HAVING COUNT(*) > 0 is nonsensical and unnecessary for this purpose. The intention seems to be to filter dishes where protein is at least 30% of their serving size, but the query syntax and logic are flawed. \\n\\nThe candidate SQL query filters dishes with protein >= 50 and then calculates the average serving size of those dishes. This is simple, and explicitly matches a threshold for protein amount (>=50), while the golden query intends a dynamic threshold based on serving_size * 0.3 but does so incorrectly.\\n\\nFunctionally, the candidate and golden queries do not do the same:\\n- The candidate uses a fixed protein threshold (50).\\n- The golden query attempts to use a threshold of 30% of serving size (protein >= serving_size * 0.3), but does so incorrectly.\\n\\nTherefore, the candidate query is not functionally similar to the golden query. Moreover, the golden query is syntactically problematic and would not run successfully, but even interpreting the intended logic, the candidate query\\'s logic differs significantly.'}, {'idx': 40, 'reason': 'mismatch', 'pred_sql': 'SELECT port_id, port_name, crane_capacity FROM ports WHERE crane_capacity > 150;', 'feedback': 'The golden SQL query selects only the port_name from the ports table where the crane_capacity is greater than 150. The candidate SQL query selects port_id, port_name, and crane_capacity for the same condition. While both queries filter on the crane_capacity > 150 condition correctly, the result columns differ. The prompt asks to \"List all the ports,\" which can be interpreted as listing port names only, but the candidate query lists additional columns. Therefore, the candidate query returns more information than required, which means it is not functionally equivalent because the output format is different from the golden query. \\nHence, they are not functionally similar as per the prompt\\'s expected output.'}, {'idx': 46, 'reason': 'mismatch', 'pred_sql': 'CREATE TABLE HeritageSites (Site VARCHAR(50), CountryName VARCHAR(50));', 'feedback': \"The golden SQL provides a join operation between the HeritageSites and Countries tables on the country name, which is a crucial step if the prompt's intention is to retrieve combined information from both tables. The candidate SQL, however, only creates the HeritageSites table without performing any join or data retrieval operation. Since the candidate SQL does not include any join or implies any subsequent data extraction or manipulation that the golden SQL shows, the candidate does not fulfill the same functional purpose as the golden SQL.\"}, {'idx': 47, 'reason': 'mismatch', 'pred_sql': \"SELECT SUM(assets) AS total_assets FROM financial_institutions WHERE shariah_compliant = TRUE AND location IN ('Saudi Arabia', 'Kuwait', 'United Arab Emirates', 'Qatar', 'Bahrain', 'Oman', 'Jordan', 'Lebanon', 'Egypt', 'Morocco', 'Tunisia', 'Algeria', 'Libya', 'Sudan', 'Iraq', 'Palestine', 'Syria', 'Yemen')\", 'feedback': \"The golden SQL uses location LIKE checks for prefixes 'Middle East%' or 'North Africa%', assuming these are location values representing those regions. However, given the data context, locations are country names like 'Saudi Arabia' and 'Kuwait', not region names prefixed with 'Middle East' or 'North Africa'. Therefore, the golden SQL likely returns no rows or incorrect aggregation, because no location literally starts with those prefixes.\\n\\nThe candidate SQL lists specific countries from the Middle East and North Africa explicitly in the location IN clause and filters for shariah_compliant = TRUE. This approach aligns better with the data provided since locations are country names. It sums assets for those where the institution is Shariah-compliant and located in those countries.\\n\\nTherefore, functionally, the candidate SQL correctly captures Shariah-compliant institutions in specified Middle East and North African countries, whereas the golden SQL likely does not due to its filtering by prefixes which don’t match the actual location values.\\n\\nDespite the discrepancy in how the region is filtered, the intent is the same: sum assets of Shariah-compliant institutions in Middle East and North Africa. The candidate SQL more correctly implements the intent given the data.\\n\\nThus, the candidate SQL and golden SQL are *not* functionally equivalent because the golden SQL’s filtering condition is incorrect or ineffective for the given data, while the candidate SQL is accurate.\"}, {'idx': 55, 'reason': 'mismatch', 'pred_sql': \"SELECT AVG(tour_count) AS avg_eco_friendly_tours_per_hotel FROM (SELECT h.id, COUNT(t.id) AS tour_count FROM hotels h LEFT JOIN tours t ON h.id = t.hotel_id AND t.eco_friendly = true WHERE h.city = 'Paris' GROUP BY h.id) sub;\", 'feedback': \"The golden SQL query calculates the average of the eco_friendly boolean values directly from the tours table joined with hotels, filtering only those tours that are eco-friendly and associated with Paris hotels. Since eco_friendly is a boolean, aggregating AVG on eco_friendly effectively computes the fraction of tours that are eco-friendly (1 for true, 0 for false), so the result is the proportion of eco-friendly tours among all tours in Paris.\\n\\nHowever, the natural language prompt asks for the average number of eco-friendly tours offered per hotel in Paris. The candidate SQL query correctly counts the number of eco-friendly tours per Paris hotel (counting tours with eco_friendly = true per hotel), then averages this count across all Paris hotels. This matches the prompt better because it represents the mean count of eco-friendly tours per hotel, rather than the fraction of eco-friendly tours.\\n\\nThe golden query's use of AVG(t.eco_friendly) actually returns the fraction of eco-friendly tours among all tours (which may differ from the average number of eco-friendly tours per hotel). For example, a hotel with many tours and a hotel with none impact the average in different ways.\\n\\nTherefore, the candidate SQL matches the user prompt more accurately, while the golden SQL can be interpreted as a related but different metric. They are not functionally equivalent queries answering the same question.\"}, {'idx': 62, 'reason': 'mismatch', 'pred_sql': 'SELECT MAX(downtime) AS max_downtime_days FROM Equipment;', 'feedback': \"The golden SQL query attempts to filter the downtime values based on a date condition, specifically that the downtime is greater than or equal to a date 6 months ago from the current date. However, this logic is incorrect since the 'downtime' column is a decimal representing the number of downtime days, not a date or timestamp that can be compared to a date expression like DATE_SUB(CURDATE(), INTERVAL 6 MONTH). This suggests a conceptual mistake in the golden SQL's WHERE clause. The candidate SQL query simply selects the maximum downtime value from the Equipment table without any date filtering, which makes logical sense given the available data schema and the prompt asking for the maximum downtime experienced in the last 6 months. Since there is no date or time data in the table to filter on when the downtime occurred, the candidate SQL retrieves the max downtime over all the entries. Functionally, given the table structure, the candidate SQL is correct and the golden SQL is flawed. Therefore, functionally they differ, as the golden SQL's condition is invalid or nonsensical in this context and would result in an empty or erroneous result.\"}, {'idx': 65, 'reason': 'mismatch', 'pred_sql': 'SELECT Game_Name, Rating FROM Action_Games WHERE Rating = (SELECT MAX(Rating) FROM Action_Games);', 'feedback': 'The golden SQL query selects the maximum rating value from the Action_Games table along with the Game_Name, but it does not correctly associate the maximum rating specifically with the game name. In most SQL engines, this query would produce an arbitrary Game_Name along with the maximum rating, which might not be the correct highest-rated game. The candidate SQL query, however, correctly finds the game(s) with the maximum rating by filtering the rows where Rating equals the maximum rating, ensuring the correct game name and rating pair is returned. Functionally, the candidate query provides the correct answer as intended by the prompt, while the golden query might not. Hence, they are not functionally equivalent.'}, {'idx': 73, 'reason': 'mismatch', 'pred_sql': \"SELECT AVG(COALESCE(round_count,0)) AS avg_funding_rounds FROM (SELECT c.company_id, COUNT(DISTINCT fr.round_number) AS round_count FROM companies c LEFT JOIN funding_records fr ON c.company_id = fr.company_id WHERE c.industry = 'Fintech' AND c.founder_age > 40 GROUP BY c.company_id) sub;\", 'feedback': 'The golden SQL calculates the average of the round_number values directly from the funding_records table joined with companies filtered by industry \\'Fintech\\' and founder_age > 40. This approach averages the round_number associated with each funding record, which may not represent the true average number of funding rounds per company, especially if round_number is not incrementally continuous per company or if multiple records exist per round.\\n\\nThe candidate SQL first groups by company and counts the distinct number of funding rounds per company (using COUNT DISTINCT of round_number), then averages these counts across companies, including companies with zero funding rounds (using LEFT JOIN and COALESCE to handle nulls).\\n\\nThe candidate SQL better captures the intended meaning of \"average number of funding rounds for companies,\" because it counts how many distinct rounds each company has had, then averages that count. The golden SQL averages round_number values from all records, which may not accurately reflect the average count of rounds per company.\\n\\nHence, although both queries attempt to answer the prompt, their results can differ. The candidate SQL aligns more accurately with the natural language prompt\\'s intent. Therefore, they are not functionally equivalent.'}, {'idx': 76, 'reason': 'mismatch', 'pred_sql': \"INSERT INTO audience (visitor_id, name, country, visit_date) VALUES (1, 'Alex', 'USA', CURRENT_DATE);\", 'feedback': 'The golden SQL query dynamically generates the visitor_id by selecting the maximum visitor_id currently in the table and adding 1 to it, ensuring unique visitor_id values that increment sequentially. It uses CURDATE() to insert the current date. The candidate SQL query inserts a fixed visitor_id value of 1, which might lead to a primary key conflict if visitor_id 1 already exists in the table. It uses CURRENT_DATE, which is functionally equivalent to CURDATE() in terms of getting the current date. The main difference is in how visitor_id is assigned: the golden query is designed to avoid duplicate primary keys by incrementing the maximum existing id, while the candidate query simply inserts a constant value. Because of this, the candidate query could cause errors or data integrity issues, and therefore is not functionally the same as the golden query.'}, {'idx': 79, 'reason': 'mismatch', 'pred_sql': \"SELECT id, project_name, start_date, end_date FROM DefenseProjects WHERE end_date IS NULL OR end_date < '2021-01-01' OR end_date > '2021-12-31';\", 'feedback': 'The golden SQL query selects defense projects that were not completed in 2021 by filtering for projects whose end_date is after 2021-12-31 (i.e., completed after 2021). It implicitly excludes projects completed in the year 2021 or any projects with missing end_date.\\n\\nThe candidate SQL query retrieves projects with end_date NULL, or with end_date before 2021, or with end_date after 2021. This includes projects never completed (end_date NULL), completed before 2021, or completed after 2021. \\n\\nSince the prompt says \"Which defense projects were not completed in 2021?\", projects with end_date NULL (not completed at all), those completed before 2021, and those completed after 2021 all qualify as not completed in 2021. Therefore, the candidate query is more comprehensive.\\n\\nThe golden query only filters for projects completed after 2021, excluding those never completed or completed before 2021, which should also be counted as \"not completed in 2021.\"\\n\\nThus, the candidate SQL is functionally different from the golden SQL query because the candidate query includes a broader set of projects not completed in 2021, while the golden query only includes projects ending after 2021.\\n\\nHence, they are not functionally similar.'}, {'idx': 80, 'reason': 'mismatch', 'pred_sql': 'SELECT name, home_runs FROM baseball_stats WHERE home_runs = (SELECT MAX(home_runs) FROM baseball_stats);', 'feedback': 'The golden SQL query returns the name along with the maximum number of home runs by using an aggregate function directly: SELECT name, MAX(home_runs) FROM baseball_stats;. However, this query will not work correctly if there are multiple players since it aggregates home_runs but does not group by name; it actually returns one row with an arbitrary player name and the maximum home_runs value.\\n\\nThe candidate SQL query uses a subquery to first find the maximum number of home runs and then selects the name(s) and home_runs matching that maximum value, which correctly returns all players who have the highest home runs.\\n\\nFunctionally, the candidate query correctly returns the player(s) with the highest home run count, while the golden query is faulty as it combines an aggregate with a non-aggregated column without GROUP BY and thus potentially returns invalid results.\\n\\nTherefore, the candidate query is functionally correct and semantically aligns with the prompt, whereas the golden is flawed.\\n\\nThus, the candidate and golden queries are not functionally similar.'}, {'idx': 83, 'reason': 'mismatch', 'pred_sql': \"SELECT EXTRACT(YEAR FROM sale_date) AS year, EXTRACT(QUARTER FROM sale_date) AS quarter, region, SUM(sales) AS total_sales FROM sales WHERE region IN ('France', 'Germany') GROUP BY year, quarter, region ORDER BY year, quarter, region;\", 'feedback': \"The golden SQL query filters the sales data to include only products in the category 'Natural' by using the condition `product_category = 'Natural'`, focusing specifically on natural cosmetics. It groups the data by region and quarter to provide quarterly sales trends for natural cosmetics in France and Germany.\\n\\nThe candidate SQL query, however, does not include any filtering by `product_category = 'Natural'`. Therefore, it aggregates sales for all products in France and Germany, not just natural cosmetics. Additionally, the candidate query also extracts the year, which is not requested in the prompt or present in the golden SQL. Although it groups by year, quarter, and region, the extra year component is an extension rather than a direct contradiction, but the key difference is the lack of filtering by product category.\\n\\nBecause the candidate query does not filter to only natural cosmetics, it is not functionally equivalent and will produce different results than the golden SQL query.\"}, {'idx': 87, 'reason': 'mismatch', 'pred_sql': \"SELECT AVG(CASE WHEN gender = 'Male' THEN salary END) - AVG(CASE WHEN gender = 'Female' THEN salary END) AS average_salary_difference FROM employees;\", 'feedback': \"The golden SQL query calculates the difference in average salary by subtracting the average male salary from the average female salary. It does this by first computing the average salary for female employees in the main query, and then subtracting the average salary for male employees obtained via a subquery.\\n\\nThe candidate SQL query calculates the difference by computing the average salary of males and females using conditional aggregation inside a single query: it calculates the average salary of males (using CASE WHEN gender = 'Male' THEN salary END) and subtracts the average salary of females (CASE WHEN gender = 'Female' THEN salary END).\\n\\nFunctionally, both queries compute the difference between the average salary of males and females, though the candidate query computes it as (avg male - avg female), while the golden query does (avg female - avg male).\\n\\nBecause the candidate query subtracts average female salary from average male salary, but the golden query subtracts average male salary from average female salary, the sign of the result will be the opposite. Therefore, these queries are not functionally equivalent as they yield results with opposite signs.\\n\\nTherefore, the candidate query is not functionally similar to the golden query.\"}]}\n"
     ]
    }
   ],
   "source": [
    "opt_metrics = evaluate_program(optimized_program, test_split, limit=500, max_workers=32)\n",
    "print(f\"Original Program: {opt_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9937eb78",
   "metadata": {},
   "source": [
    "# Store Eval Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcc020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_metrics(metrics, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4, sort_keys=True) \n",
    "\n",
    "save_metrics(og_metrics, \"./dspy_program/4.1-mini.json\")\n",
    "save_metrics(opt_metrics,\"./optimized_program/4.1-mini.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
